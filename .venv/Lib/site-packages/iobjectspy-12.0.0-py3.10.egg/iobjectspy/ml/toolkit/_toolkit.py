# !/usr/bin/env python3
# coding=utf-8
import glob
import json
import os
import random
import shutil
import sys
from collections import OrderedDict
import numpy as np
import yaml
from PIL import Image
from dotmap import DotMap
import warnings
from ... import Dataset, Workspace, DatasourceConnectionInfo
from ..._logger import log_info


# 根据BIE标签合并相应字
# __all__ = ['find_element_in_list','getStr_InList_ByKey','check_module','checkpath','merge_number']

# def get_device():
#     """
#         返回一个允许的设备：cpu, cuda 或者 mlu
#     """
#     import torch
#     is_device_available = {
#         'cuda': torch.cuda.is_available()
#     }
#     device_list = [k for k, v in is_device_available.items() if v]
#     return device_list[0] if len(device_list) == 1 else 'cpu'


def find_element_in_list(element, list_element):
    """列表判断定位元素

    :param element: 需要定位的元素
    :param list_element: 被查找的列表
    :return: 是否能找到元素
    """
    try:
        list_element.index(element)
        return True
    except ValueError:
        return False


def getStr_InList_ByKey(key, list_i, list_v=None):
    """ 列表搜索字符串元素
    函数分为两种使用模式
        - list_v 为None时，直接根据key去查找list_i
        - list_v不为None时，根据key去查找list_v元素位置的值，作为搜索键值，查找list_i

    :param key: 待搜索的字符串键值
    :param list_i: 待搜索的列表
    :param list_v: 用于匹配搜索的列表，默认为None
    :return: 返回list_i中的字符元素
    """
    try:
        idx = list_i.index(key)
        if list_v is None:
            list_v = list_i
        return list_v[idx]
    except ValueError:
        return ""


def check_module(modulename=None):
    """检测模块是否存在

    :param modulename: 模块名称
    :return: 是否存在
    """
    if modulename is None:
        return (True)
    else:
        import importlib
        lib_spec = importlib.util.find_spec(modulename)
        found = lib_spec is not None
        return (found)


def checkpath(strpath, type=None):
    """ 检测路径文件是否有效

    :param strpath: 被检测的文件路径
    :param type: 是否指定文件后缀名
    :return: 文件是否存在; 当文件路径存在时，且type不为None，返回是否是指定文件类型
    """
    if not isinstance(strpath, str):
        # print('strpath is not a str!!')
        return (False)
    from pathlib import Path
    my_file = Path(strpath)
    if my_file.is_file():
        if type is None:
            return (True)
        elif type is not None and strpath.endswith(type):
            # print('file in strpath is not correct!')
            return (True)
    return (False)


def merge_number(rawss):
    """
    合并数字,CRFPP中需要将前后相邻的数字字符元素合并成一个元素
    比如路号，楼栋编号等等

    :param rawss: 原始待合并的字符串列表
    :return: 合并数字后的字符串列表
    """
    ss = list(rawss)
    isdigit = [y.isdigit() for y in [i for i in ss]]
    sy = [y for y in [i for i in ss]]
    i = 0
    while i < len(isdigit):
        if isdigit[i]:
            for j in range(i, len(isdigit)):
                if isdigit[j] is not True:
                    break
            if j == len(isdigit) - 1 and isdigit[j]:
                j = j + 1
            sy[i] = "".join(ss[i: j])
            for ii in range(i + 1, j):
                sy[ii] = ''
            i = j
        i += 1
    return (list(filter(None, sy)))


def stretch_n(bands, lower_percent=2, higher_percent=98):
    """
    将影像数据标准化到0到1之间


    :param bands:  输入影像
    :param lower_percent:   最小值比率
    :param higher_percent:  最大值比率
    :return:  标准化后的影像
    """
    out = np.zeros_like(bands, dtype=np.float)
    n = bands.shape[2]
    for i in range(n):
        a = 0  # np.min(band)
        b = 1  # np.max(band)
        c = np.percentile(bands[:, :, i], lower_percent)
        d = np.percentile(bands[:, :, i], higher_percent)
        t = a + (bands[:, :, i] - c) * (b - a) / (d - c)
        t[t < a] = a
        t[t > b] = b
        out[:, :, i] = t
    return out


# def stretch_min_max(bands, min, max):
#     """
#         将影像数据各个波段分别标准化到0到1之间
#
#
#         :param bands:  输入影像
#         :param min:   最小值数组
#         :param max:  最大值数组
#         :return:  标准化后的影像
#         """
#     out = np.zeros_like(bands, dtype=np.float)
#     n = bands.shape[2]
#     for i in range(n):
#         a = 0  # np.min(band)
#         b = 1  # np.max(band)
#         c = min[i]
#         d = max[i]
#         t = a + (bands[:, :, i] - c) * (b - a) / (d - c)
#         t[t < a] = a
#         t[t > b] = b
#         out[:, :, i] = t
#     return out


# def stretch_minmax(bands, max_value=256, min_value=0):
#     """
#     将影像数据标准化到0到1之间
#     :param bands: 输入影像
#     :param max_value: 最小值
#     :param min_value: 最大值
#     :return: 标准化后的影像
#     """
#     out = np.zeros_like(bands, dtype=np.float)
#     n = bands.shape[2]
#     for i in range(n):
#         a = 0  # np.min(band)
#         b = 1  # np.max(band)
#         c = min_value
#         d = max_value
#         t = a + (bands[:, :, i] - c) * (b - a) / (d - c)
#         t[t < a] = a
#         t[t > b] = b
#         out[:, :, i] = t
#     return out


# def get_percentclip_min_max(image_path, lower_percent=2, higher_percent=98):
#     import gdal
#     src_ds = gdal.Open(image_path)
#     band_xsize, band_ysize, band_count = src_ds.RasterXSize, src_ds.RasterYSize, src_ds.RasterCount
#     # band_pixel_count=band_xsize*band_ysize
#
#     band_percent_min, band_percent_max = [], []
#     for i in range(band_count):
#         band = src_ds.GetRasterBand(i + 1)
#         band_min, band_max = band.ComputeRasterMinMax()
#         hist = band.GetHistogram(min=-0.5 + band_min, max=band_max + 0.5, buckets=int(band_max - band_min + 1))
#         band_pixel_count = sum(hist)
#         band_pixel_min_count, band_pixel_max_count = band_pixel_count * lower_percent / 100, band_pixel_count * (
#                 100 - higher_percent) / 100
#         count_pixel = 0
#         for min_i in range(len(hist)):
#             count_pixel += hist[min_i]
#             if count_pixel > band_pixel_min_count:
#                 band_percent_min.append(band_min + min_i)
#                 break
#         count_pixel = 0
#         for max_i in range(len(hist) - 1, -1, -1):
#             count_pixel += hist[max_i]
#             if count_pixel > band_pixel_max_count:
#                 band_percent_max.append(band_max + max_i - len(hist))
#                 break
#     return band_percent_min, band_percent_max

def bounds_transform_coord(ds, infer_region=None, mode="general", to_raster=False, **kwargs):
    """
        计算影像和输入范围的交集
        :param ds: 输入影像
        :param infer_region: 输入范围
        :param mode: 单景影像模式或者变化检测多景影像
        :return: 返回交集的左上角坐标
    """
    import rasterio
    from iobjectspy import Rectangle
    bounds, region = None, None
    if infer_region != None:
        if isinstance(infer_region, Rectangle):
            bounds = infer_region
        elif isinstance(infer_region, Dataset):
            region = infer_region
        elif isinstance(infer_region, str):
            region = get_input_dataset(infer_region)
        else:
            raise TypeError('The type of inference region is not supported!')
    if bounds:
        # if (abs(bounds.left) > 180 and abs(ds.bounds.left) < 180) or (abs(bounds.left) < 180 and abs(ds.bounds.left) > 180):
        if (abs(bounds.right) > 180 and abs(ds.bounds.right) < 180) or (abs(bounds.right) < 180 and abs(ds.bounds.right) > 180):
            raise RuntimeError("The image coordinate system should be consistent with the inference region")

        if max(bounds.left, ds.bounds.left) > min(bounds.right, ds.bounds.right) and \
            max(bounds.top, ds.bounds.top) > min(bounds.bottom, ds.bounds.bottom):
            warnings.warn(" The inference region does not cover image.")
            return None

        elif (bounds.left < ds.bounds.left) or (bounds.bottom < ds.bounds.bottom) or \
                (bounds.right > ds.bounds.right) or (bounds.top > ds.bounds.top):
            warnings.warn("The inference region is beyond image.")

        if mode == "cd":
            try:
                ds_compare = kwargs.get("ds_compare")
            except:
                raise ValueError("Compare dataset is required")

            if (bounds.left < ds_compare.bounds.left) or (bounds.bottom < ds_compare.bounds.bottom) or \
                    (bounds.right > ds_compare.bounds.right) or (bounds.top > ds_compare.bounds.top):
                warnings.warn("The inference region is beyond image.")

            bounds = Rectangle(max(bounds.left, ds.bounds.left, ds_compare.bounds.left),
                                  max(bounds.bottom, ds.bounds.bottom, ds_compare.bounds.bottom),
                                  min(bounds.right, ds.bounds.right, ds_compare.bounds.right),
                                  min(bounds.top, ds.bounds.top, ds_compare.bounds.top))

            rectangle_ymin_compare, rectangle_xmin_compare = rasterio.transform.rowcol(ds_compare.transform,
                                                                                       bounds.left,
                                                                                       bounds.top)
        else:
            bounds = Rectangle(max(bounds.left, ds.bounds.left),
                                  max(bounds.bottom, ds.bounds.bottom),
                                  min(bounds.right, ds.bounds.right),
                                  min(bounds.top, ds.bounds.top))
    elif region:
        if mode == "general":
            bounds = Rectangle(max(region.bounds.left, ds.bounds.left),
                              max(region.bounds.bottom, ds.bounds.bottom),
                              min(region.bounds.right, ds.bounds.right),
                              min(region.bounds.top, ds.bounds.top))
        else:
            try:
                ds_compare = kwargs.get("ds_compare")
            except:
                raise ValueError("Compare dataset is required")
            bounds = Rectangle(max(region.bounds.left, ds.bounds.left, ds_compare.bounds.left),
                                  max(region.bounds.bottom, ds.bounds.bottom, ds_compare.bounds.bottom),
                                  min(region.bounds.right, ds.bounds.right, ds_compare.bounds.right),
                                  min(region.bounds.top, ds.bounds.top, ds_compare.bounds.top))
            rectangle_ymin_compare, rectangle_xmin_compare = rasterio.transform.rowcol(ds_compare.transform,
                                                                                       bounds.left,
                                                                                       bounds.top)
            if (region.bounds.left < ds_compare.bounds.left) or (region.bounds.bottom < ds_compare.bounds.bottom) or \
                    (region.bounds.right > ds_compare.bounds.right) or (region.bounds.top > ds_compare.bounds.top):
                warnings.warn("The inference region is beyond image.")

        #if (abs(region.bounds.left) > 180 and abs(ds.bounds.left) < 180) or (abs(region.bounds.left) < 180 and abs(ds.bounds.left) > 180):
        if (abs(region.bounds.right) > 180 and abs(ds.bounds.right) < 180) or (abs(region.bounds.right) < 180 and abs(ds.bounds.right) > 180):
            raise RuntimeError("The image coordinate system should be consistent with the inference region")
        if max(region.bounds.left, ds.bounds.left) > min(region.bounds.right, ds.bounds.right) and \
            max(region.bounds.top, ds.bounds.top) > min(region.bounds.bottom, ds.bounds.bottom):
            warnings.warn("The inference region does not cover image.")
            return None
        elif (region.bounds.left < ds.bounds.left) or (region.bounds.bottom < ds.bounds.bottom) or \
                (region.bounds.right > ds.bounds.right) or (region.bounds.top > ds.bounds.top):
            warnings.warn("The inference region is beyond image.")

    else:
        if mode == "cd":
            try:
                ds_compare = kwargs.get("ds_compare")
            except:
                raise ValueError("Compare dataset is required")
            bounds = Rectangle(max(ds.bounds.left, ds_compare.bounds.left),
                                  max(ds.bounds.bottom, ds_compare.bounds.bottom),
                                  min(ds.bounds.right, ds_compare.bounds.right),
                                  min(ds.bounds.top, ds_compare.bounds.top))
            rectangle_ymin_compare, rectangle_xmin_compare = rasterio.transform.rowcol(ds_compare.transform,
                                                                                       bounds.left,
                                                                                       bounds.top)

    rectangle_ymin, rectangle_xmin = rasterio.transform.rowcol(ds.transform, bounds.left,
                                                                         bounds.top)
    rectangle_ymax, rectangle_xmax = rasterio.transform.rowcol(ds.transform, bounds.right,
                                                                         bounds.bottom)
    bounds_rectangle = (bounds.left, bounds.bottom, bounds.right, bounds.top)
    width, height = rectangle_xmax - rectangle_xmin, rectangle_ymax - rectangle_ymin

    transform = rasterio.transform.from_bounds(bounds.left, bounds.bottom,
                                                 bounds.right, bounds.top,
                                                 width, height)
    out_path_mask = None
    if region is not None:
        if to_raster:
            try:
                out_path = kwargs.get('out_path')
                dir_name = os.path.split(out_path)
                file_name = os.path.splitext(dir_name[1])
                out_path_mask = os.path.join(dir_name[0], file_name[0] + '_mask.tif')
            except:
                raise RuntimeError('Missing rasterized save path')
            vector2raster(region, bounds, transform, height, width, out_path_mask)

    if mode == "general":
        return rectangle_ymin, rectangle_xmin, bounds_rectangle, width, height, transform, out_path_mask
    else:
        return rectangle_ymin, rectangle_xmin, rectangle_ymin_compare, rectangle_xmin_compare, bounds_rectangle, width, height, transform, out_path_mask

def vector2raster(region, bounds, transform, height, width, out_path):
    """
    根据输入范围和转换矩阵将矢量栅格化
    :param region: 矢量面
    :type region: Dataset
    :param bounds: 输入范围
    :type bounds: Rectangle
    :param transform: 转换矩阵
    :type transform: array
    :param height: 栅格高度
    :type height: int
    :param width: 栅格宽度
    :type width: int
    :param out_path: 栅格的保存路径
    :type: str
    :return:
    """
    import rasterio
    import math
    from rasterio.windows import Window
    from iobjectspy import clip
    recordset = region.query_with_bounds(bounds, cursor_type='STATIC')
    feature_list = []
    for feature in recordset.get_features():
        clip_geometry = clip(feature.geometry, bounds)
        if clip_geometry is not None:
            feature_geojson = json.loads(clip_geometry.to_geojson())
            feature_list.append((feature_geojson, 1))

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        dst = rasterio.open(out_path, 'w', driver='GTiff', width=width, height=height,
                            transform=transform, count=1, dtype=np.uint8)

    default_patch = 10240  # default
    num_row_patches = math.ceil(height / default_patch)
    num_col_patches = math.ceil(width / default_patch)
    for j in range(num_row_patches):
        for i in range(num_col_patches):
            if i == num_col_patches - 1 and j == num_row_patches - 1:
                height_patch, width_patch = height - j * default_patch, width - i * default_patch
            elif i == num_col_patches - 1:
                height_patch, width_patch = default_patch, width - i * default_patch
            elif j == num_row_patches - 1:
                height_patch, width_patch = height - j * default_patch, default_patch
            else:
                height_patch, width_patch = default_patch, default_patch
            coord_min = rasterio.transform.xy(transform, j*default_patch, i*default_patch)
            coord_max = rasterio.transform.xy(transform, j*default_patch + height_patch, i*default_patch + width_patch)
            transf_tile = rasterio.transform.from_bounds(coord_min[0], coord_max[1],
                                                         coord_max[0], coord_min[1], width_patch, height_patch)

            image = rasterio.features.rasterize(((g, 1) for g, v in feature_list), out_shape=(height_patch, width_patch),
                                        transform=transf_tile)

            image = image[np.newaxis, :, :].astype('uint8')
            dst.write(image, window=Window(i * default_patch, j * default_patch, width_patch, height_patch))
    dst.close()

# def read_short_json_file(file_path, encoding='utf8'):
#     """
#     读取小的json文件，转换为dict
#
#
#     :param file_path:  文件路径
#     :param encoding:  文件编码
#     :return:  json字符对应的dict对象
#     """
#     with open(file_path, 'r', encoding=encoding) as json_f:
#         json_str = ''
#         for line in json_f:
#             json_str += line
#
#     return json.loads(json_str)


# def list_xy_file_fromtxt(txt_path):
#     """
#     通过txt文件列出image，mask所有文件完整路径
#     :param txt_path: 文件名字txt路径
#     :return:
#     """
#     x_filenames = []
#     y_filenames = []
#     with open(txt_path, 'r') as f:
#         for line in f:
#             files = line.strip().split(',')
#             x_filenames.append(files[0])
#             y_filenames.append(files[1])
#     return x_filenames, y_filenames


# def get_image_from_csv(csv_path, is_aug=False, input_bands=3, output_bands=1, image_size=None, generate=False,
#                        batch_size=None):
#     """
#     从csv文件中的路径读取训练数据
#     :param csv_path: csv未见路径
#     :return: 读取的数据 （X,Y）
#     """
#     import rasterio
#     from rasterio._base import NotGeoreferencedWarning
#     from warnings import filterwarnings
#     filterwarnings('ignore', category=NotGeoreferencedWarning)
#     from albumentations import Compose, VerticalFlip, RandomRotate90, OneOf, ElasticTransform, GridDistortion, \
#         CLAHE, OpticalDistortion, RandomBrightnessContrast, RandomGamma, RandomCrop, Resize, \
#         HueSaturationValue, IAAAdditiveGaussianNoise, IAAPerspective, IAASharpen, Blur, \
#         MotionBlur, ImageCompression
#     log_info('ignore  NotGeoreferencedWarning')
#     x_files, y_files = list_xy_file_fromtxt(csv_path)
#     if len(x_files) < 1:
#         raise Exception('Data load error,Image file is not enough')
#     if len(y_files) < 1:
#         raise Exception('Data load error,Mask file is not enough')
#
#     def read_xy(x_file, y_file):
#         x_image, y = rasterio.open(x_file).read()[:input_bands, ...], np.array(Image.open(y_file)) \
#             if y_file.strip().endswith('png') else rasterio.open(y_file).read(1)
#
#         x_image = np.transpose(x_image, (1, 2, 0))
#
#         if is_aug:
#             aug_list = [
#                 # ToFloat(),
#
#                 VerticalFlip(p=0.5),
#                 RandomRotate90(p=0.5),
#                 OneOf([
#                     ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),
#                     GridDistortion(p=0.5),
#                     OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)
#                 ], p=0.1),
#                 ImageCompression(quality_lower=50, quality_upper=90, p=0.5),
#                 IAAAdditiveGaussianNoise(p=0.2),
#                 IAAPerspective(p=0.5),
#
#                 OneOf(
#                     [
#                         CLAHE(p=1),
#                         RandomBrightnessContrast(p=1),
#                         RandomGamma(p=1),
#                     ],
#                     p=0.9,
#                 ),
#
#                 OneOf(
#                     [
#                         IAASharpen(p=1),
#                         Blur(blur_limit=3, p=1),
#                         MotionBlur(blur_limit=3, p=1),
#                     ],
#                     p=0.9,
#                 ),
#
#                 OneOf(
#                     [
#                         RandomBrightnessContrast(p=1),
#                         HueSaturationValue(p=1),
#                     ],
#                     p=0.9,
#                 )]
#             if image_size is not None:
#                 if image_size < x_image.shape[1]:
#                     aug_list.append(Resize(image_size, image_size))
#                 else:
#                     aug_list.append(RandomCrop(image_size, image_size, p=1))
#             aug = Compose(aug_list)
#             augmented = aug(image=x_image, mask=y)
#             x_image = augmented['image']
#             y = augmented['mask']
#         elif image_size is not None:
#             aug = Compose([
#                 RandomCrop(image_size, image_size, p=1)]
#             )
#             augmented = aug(image=x_image, mask=y)
#             x_image = augmented['image']
#             y = augmented['mask']
#         if output_bands > 1:
#             y = to_onehot(y, [num for num in range(output_bands)])
#         return x_image, y
#
#     if generate:
#         def gen_f():
#             X = []
#             Y = []
#             count = 0
#             while True:
#                 for x_file, y_file in zip(x_files, y_files):
#                     x_image, y = read_xy(x_file, y_file)
#                     X.append(x_image)
#                     if len(y.shape) < 3:
#                         Y.append(y[:, :, np.newaxis])
#                     else:
#                         Y.append(y)
#                     count += 1
#                     if count >= batch_size:
#                         yield np.array(X), np.array(Y)
#                         X = []
#                         Y = []
#                         count = 0
#
#         return gen_f
#     else:
#         X = []
#         Y = []
#         for x_file, y_file in zip(x_files, y_files):
#             x_image, y = read_xy(x_file, y_file)
#             X.append(x_image)
#             if len(y.shape) < 3:
#                 Y.append(y[:, :, np.newaxis])
#             else:
#                 Y.append(y)
#         return np.array(X), np.array(Y)


# def get_changedet_image_from_csv(csv_path, is_aug=False, band_num=1, image_size=None):
#     """
#     从csv文件中的路径读取训练数据
#     :param csv_path: csv未见路径
#     :return: 读取的数据 （X,Y）
#     """
#     import rasterio
#     pre_x_files, next_x_files, y_files = [], [], []
#     with open(csv_path, 'r', encoding='utf8') as f:
#         for line in f:
#             pre_x_file, next_x_file, y_file = line.strip().split(',')
#             pre_x_files.append(pre_x_file)
#             next_x_files.append(next_x_file)
#             y_files.append(y_file)
#
#     PX = []
#     NX = []
#     Y = []
#     for x_file, next_x_file, y_file in zip(pre_x_files, next_x_files, y_files):
#         x_image, latest_x_image, y = rasterio.open(x_file).read()[:3, ...], rasterio.open(next_x_file).read()[:3,
#                                                                             ...], np.array(Image.open(y_file)) \
#                                          if os.path.splitext(y_file)[-1] is 'png' else rasterio.open(y_file).read(1)
#         x_image = np.transpose(x_image, (1, 2, 0))
#         latest_x_image = np.transpose(latest_x_image, (1, 2, 0))
#
#         # if is_aug:
#         #     aug_list = [
#         #         # ToFloat(),
#         #         VerticalFlip(p=0.5),
#         #         RandomRotate90(p=0.5),
#         #         OneOf([
#         #             ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),
#         #             GridDistortion(p=0.5),
#         #             OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)
#         #         ], p=0.1),
#         #         JpegCompression(quality_lower=50, quality_upper=80, p=0.5),
#         #         CLAHE(p=0.8),
#         #         RandomBrightnessContrast(p=0.8),
#         #         RandomGamma(p=0.8)]
#         #     if image_size is not None:
#         #         aug_list.append(RandomCrop(image_size, image_size, p=1))
#         #     aug = Compose(aug_list)
#         #     augmented = aug(image=x_image, mask=y)
#         #     x_image = augmented['image']
#         #     y = augmented['mask']
#         # elif image_size is not None:
#         #     aug = Compose([
#         #         RandomCrop(image_size, image_size, p=1)]
#         #     )
#         #     augmented = aug(image=x_image, mask=y)
#         #     x_image = augmented['image']
#         #     y = augmented['mask']
#         if band_num > 1:
#             y = to_onehot(y, [num for num in range(band_num)])
#         PX.append(x_image)
#         NX.append(latest_x_image)
#         if len(y.shape) < 3:
#             Y.append(y[:, :, np.newaxis])
#         else:
#             Y.append(y)
#
#     return np.array(PX), np.array(NX), np.array(Y)


def preprocess_input(x, int8=True):
    MEAN_RGB = [0.485, 0.456, 0.406] * 60
    STDDEV_RGB = [0.229, 0.224, 0.225] * 60
    assert x.ndim in (3, 4)
    band_num = x.shape[-1]
    MEAN_RGB = MEAN_RGB[0:band_num]
    STDDEV_RGB = STDDEV_RGB[0:band_num]
    value_scale = 255
    if int8:
        x = x - np.array([item * value_scale for item in MEAN_RGB])
        x = x / np.array([item * value_scale for item in STDDEV_RGB])
    else:
        x = x - np.array(MEAN_RGB)
        x = x / np.array(STDDEV_RGB)
    return x


# def to_onehot(y, classes):
#     """
#     最多支持256类
#     :param y:
#     :param classes: 最多支持256类
#     :return:
#     """
#     y_shape = list(y.shape)
#     y_shape.append(len(classes))
#     y_shape = tuple(y_shape)
#     y_out = np.zeros(y_shape, dtype=np.uint8)
#     for i in range(len(classes)):
#         y_out[..., i][y == classes[i]] = 1
#     return y_out
#
#
# def to_onehot_image_cls(y, classes):
#     """
#     最多支持256类
#     :param y:
#     :param classes: 最多支持256类
#     :return:
#     """
#     y_shape = list(y.shape)
#     y_shape.append(len(classes))
#     y_shape = tuple(y_shape)
#     y_out = np.zeros(y_shape, dtype=np.uint8)
#     for i in range(len(classes)):
#         y_out[..., i][y == classes[i]] = 1
#     return np.squeeze(y_out)


def list_file(folder, pattern='*', ext='tif'):
    """
    列出指定文件夹下的文件目录
    :param folder: 指定的文件夹
    :param pattern: 文件名过滤
    :param ext: 后缀名过滤
    :return: 符合要求的文件路径list
    """
    folder_dirs = [x[0] for x in os.walk(folder)]
    files = [x[2] for x in os.walk(folder)]
    filenames = []
    new_filenames = []
    for folderdir in folder_dirs:
        filenames.extend(sorted(glob.glob(folderdir + '/' + pattern + '.' + ext)))
    for filename in filenames:
        new_filenames.append(filename.replace('\\', '/'))
    if len(new_filenames) == 0 and len(files[0]) != 0:
        if files[0][0].split('.')[-1] != ext:
            warnings.warn('x_ext in train_data.sda should be the same with file in data folder')
            # raise Exception('x_ext in train_data.sda should be the same with file in data folder')
    return new_filenames


def split_train_val_withdirs(image_dirs, mask_dirs, train_txt_path, val_scale=0.3, x_ext='tif', y_ext='tif'):
    """
    多份分割数据放在一起划分训练集和验证集,shuffle
    :param image_dirs: 多个影像目录
    :type image_dirs: list
    :param mask_dirs:  多个mask目录，与影像目录顺序对应
    :type mask_dirs: list
    :param train_txt_path: 训练文件记录的txt文件路径
    :type train_txt_path: str
    :param val_scale: 验证集比例
    :type val_scale: float
    :param x_ext: 影像后缀名
    :type x_ext: str
    :param y_ext: mask后缀名
    :type y_ext: str
    :return: train_num,val_num,train_val_num
    """

    if os.path.exists(train_txt_path) is not True:
        os.makedirs(train_txt_path)

    image_files = []
    mask_files = []
    for image_dir, mask_dir in zip(image_dirs, mask_dirs):
        files = list_file(image_dir, '*', x_ext)
        for file in files:
            filename = os.path.splitext(os.path.basename(file))[0]
            mask_file = os.path.join(mask_dir, filename + '.' + y_ext)
            if os.path.exists(mask_file):
                mask_files.append(mask_file)
                image_files.append(file)

    merge_files = list(zip(image_files, mask_files))
    random.shuffle(merge_files)
    train_val_num = len(merge_files)
    val_num = int(train_val_num * val_scale)
    train_num = train_val_num - val_num

    # 使用with语句安全地写入文件
    with open(os.path.join(train_txt_path, 'train.csv'), 'w', encoding='utf8', buffering=1) as train_file, \
            open(os.path.join(train_txt_path, 'trainval.csv'), 'w', encoding='utf8', buffering=1) as trainval_file, \
            open(os.path.join(train_txt_path, 'val.csv'), 'w', encoding='utf8', buffering=1) as val_file:

        for i, (image_file, mask_file) in enumerate(merge_files):
            line = f"{image_file},{mask_file}\n"

            # 写入到相应的文件
            if i < train_num:
                train_file.write(line)
                train_file.flush()
                trainval_file.write(line)
                trainval_file.flush()
            elif i < train_val_num:
                val_file.write(line)
                val_file.flush()
                trainval_file.write(line)
                trainval_file.flush()

    log_info(f"Number of samples in the TrainSet：{train_num}")
    log_info(f"Number of samples in the ValSet：{val_num}")
    log_info(f"Number of samples：{train_val_num}")

    return train_num, val_num, train_val_num


def split_train_val_change_det(pre_image_dirs, next_image_dirs, mask_dirs, train_txt_path, val_scale=0.3, x_ext='tif',
                               y_ext='tif'):
    """
    多份分割数据放在一起划分训练集和验证集
    :param pre_image_dirs: 前一时相影像目录
    :type pre_image_dirs: list
    :param next_image_dirs: 后一时相影像目录
    :type next_image_dirs: list
    :param mask_dirs:  多个mask目录，与影像目录顺序对应
    :type mask_dirs: list
    :param train_txt_path: 训练文件记录的txt文件路径
    :type train_txt_path: str
    :param val_scale: 验证集比例
    :type val_scale: float
    :param x_ext: 影像后缀名
    :type x_ext: str
    :param y_ext: mask后缀名
    :type y_ext: str
    :return: None
    """

    if os.path.exists(train_txt_path) is not True:
        os.makedirs(train_txt_path)

    pre_image_files = []
    next_image_files = []
    mask_files = []
    for image_dir, latest_image_dir, mask_dir in zip(pre_image_dirs, next_image_dirs, mask_dirs):
        files = list_file(image_dir, '*', x_ext)
        for file in files:
            filename = os.path.splitext(os.path.basename(file))[0]
            mask_file = os.path.join(mask_dir, filename + '.' + y_ext)
            latest_image_file = os.path.join(latest_image_dir, filename + '.' + x_ext)
            if os.path.exists(mask_file) and os.path.exists(latest_image_file):
                mask_files.append(mask_file)
                pre_image_files.append(file)
                next_image_files.append(latest_image_file)

    merge_files = list(zip(pre_image_files, next_image_files, mask_files))
    random.shuffle(merge_files)
    train_val_num = len(merge_files)
    val_num = int(train_val_num * val_scale)
    train_num = train_val_num - val_num

    # 使用with语句安全地写入文件
    with open(os.path.join(train_txt_path, 'train.csv'), 'w', encoding='utf8', buffering=1) as train_file, \
         open(os.path.join(train_txt_path, 'trainval.csv'), 'w', encoding='utf8', buffering=1) as trainval_file, \
         open(os.path.join(train_txt_path, 'val.csv'), 'w', encoding='utf8', buffering=1) as val_file:

        for i, (image_file, latest_image_file, mask_file) in enumerate(merge_files):
            line = f"{image_file},{latest_image_file},{mask_file}\n"

            # 写入到相应的文件
            if i < train_num:
                train_file.write(line)
                train_file.flush()
                trainval_file.write(line)
                trainval_file.flush()
            elif i < train_val_num:
                val_file.write(line)
                val_file.flush()
                trainval_file.write(line)
                trainval_file.flush()

    log_info(f"Number of samples in the TrainSet：{train_num}")
    log_info(f"Number of samples in the ValSet：{val_num}")
    log_info(f"Number of samples：{train_val_num}")


def split_train_val_scene_classification(image_dirs, train_txt_path, val_scale=0.3):
    """
    场景分类数据集划分验证测试集
    :param image_dirs:
    :param train_txt_path:
    :param val_scale:
    :return:
    """
    if os.path.exists(train_txt_path) is not True:
        os.makedirs(train_txt_path)
    train_file = open(os.path.join(train_txt_path, 'train.csv'), 'w', encoding='utf8')
    trainval_file = open(os.path.join(train_txt_path, 'trainval.csv'), 'w', encoding='utf8')
    val_file = open(os.path.join(train_txt_path, 'val.csv'), 'w', encoding='utf8')
    all_files = []
    for image_dir in image_dirs:
        with open(os.path.join(image_dir, 'scene_classification.csv'), 'r', encoding='utf8') as f:
            for line in f:
                line = line.strip().split(',')
                line[0] = os.path.join(image_dir, line[0])
                all_files.append(line)
    random.shuffle(all_files)
    train_val_num = len(all_files)
    val_num = int(train_val_num * val_scale)
    train_num = train_val_num - val_num
    i = 0
    for file in all_files:
        if i < train_num:
            train_file.write(','.join(file) + '\n')
            trainval_file.write(','.join(file) + '\n')
        elif i < train_val_num:
            val_file.write(','.join(file) + '\n')
            trainval_file.write(','.join(file) + '\n')
        i += 1
    train_file.close()
    val_file.close()
    trainval_file.close()
    return train_num, val_num, train_val_num


def split_train_val_image_classification(image_dirs, train_txt_path, val_scale=0.3):
    """
    影像分类数据集划分验证测试集
    :param image_dirs:
    :param train_txt_path:
    :param val_scale:
    :return:
    """
    if os.path.exists(train_txt_path) is not True:
        os.makedirs(train_txt_path)
    train_file = open(os.path.join(train_txt_path, 'train.csv'), 'w', encoding='utf8')
    trainval_file = open(os.path.join(train_txt_path, 'trainval.csv'), 'w', encoding='utf8')
    val_file = open(os.path.join(train_txt_path, 'val.csv'), 'w', encoding='utf8')
    all_files = []
    for image_dir in image_dirs:
        with open(os.path.join(image_dir, 'image_classification.csv'), 'r', encoding='utf8') as f:
            for line in f:
                line = line.strip().split(',')
                line[0] = os.path.join(image_dir, line[0])
                all_files.append(line)
    random.shuffle(all_files)
    train_val_num = len(all_files)
    val_num = int(train_val_num * val_scale)
    train_num = train_val_num - val_num
    i = 0
    for file in all_files:
        if i < train_num:
            train_file.write(','.join(file) + '\n')
            trainval_file.write(','.join(file) + '\n')
        elif i < train_val_num:
            val_file.write(','.join(file) + '\n')
            trainval_file.write(','.join(file) + '\n')
        i += 1
    train_file.close()
    val_file.close()
    trainval_file.close()
    return train_num, val_num, train_val_num


def get_input_dataset(value):
    if value is None:
        return None
    if isinstance(value, str):
        _value = value.replace('\\', '/')
        try:
            _index = _value.rindex('|')
        except:
            try:
                _index = _value.rindex('/')
            except:
                _index = -1
        if _index < 0:
            return value
        else:
            _ds_info = _value[:_index]
            if _ds_info.find('\\') >= 0 or _ds_info.find('/') >= 0 or _ds_info.find(':') >= 0:
                ds = Workspace().open_datasource(_ds_info, True)
                if ds is not None:
                    return ds[_value[_index + 1:]]
                else:
                    return None
            else:
                alias = _value[:_index]
                ds = Workspace().get_datasource(alias)
                if ds is not None:
                    dt_name = value[_index + 1:]
                    return ds[dt_name]
                else:
                    return None
    elif isinstance(value, Dataset):
        return value
    else:
        return value


def _get_dataset_readonly(value):
    if value is None:
        return None
    if isinstance(value, str):
        _value = value.replace('\\', '/')
        try:
            _index = _value.rindex('|')
        except:
            try:
                _index = _value.rindex('/')
            except:
                _index = -1
        if _index < 0:
            return value
        else:
            _ds_info = _value[:_index]
            if _ds_info.find('\\') >= 0 or _ds_info.find('/') >= 0 or _ds_info.find(':') >= 0:
                conn = DatasourceConnectionInfo(_ds_info)
                conn.set_readonly(True)
                ds = Workspace().open_datasource(conn, True)
                if ds is not None:
                    return ds[_value[_index + 1:]]
                else:
                    return None
            else:
                # TODO not change this  case
                alias = _value[:_index]
                ds = Workspace().get_datasource(alias)
                if ds is not None:
                    dt_name = value[_index + 1:]
                    return ds[dt_name]
                else:
                    return None
    elif isinstance(value, Dataset):
        return value
    else:
        return value


def view_bar(num, total):
    """
    进度条
    :param num: 当前进度
    :param total: 任务总量
    :return:
    """
    rate = float(num) / float(total)
    rate_num = int(rate * 100)
    r = '\r[%s%s]%d%%,%d' % (">" * rate_num, "-" * (100 - rate_num), rate_num, num)
    sys.stdout.write(r)
    sys.stdout.flush()


def ordered_yaml_dump(data, stream=None, Dumper=yaml.SafeDumper, **kwds):
    class OrderedDumper(Dumper):
        pass

    def _dict_representer(dumper, data):
        return dumper.represent_mapping(
            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,
            data.items())

    OrderedDumper.add_representer(OrderedDict, _dict_representer)
    return yaml.dump(data, stream, OrderedDumper, **kwds)


def save_config_to_yaml(config: OrderedDict, yaml_file: str, encoding='utf8') -> None:
    """
    save the config to a yaml format file
    :param config:
    :param yaml_file:
    :param encoding:
    :return:
    """
    with open(yaml_file, 'w', encoding=encoding) as f:
        ordered_yaml_dump(config, f, encoding='utf8', allow_unicode=True)


def get_config_from_yaml(yaml_file, encoding='utf8'):
    """
    Get the config from a yml or yaml file
    :param yaml_file: 文件路径
    :param encoding: encoding default: utf8
    :return: config(namespace) or config(dictionary)
    """
    with open(yaml_file, encoding=encoding) as f:
        config_dict = yaml.load(f, Loader=yaml.FullLoader)
    config = DotMap(config_dict)
    return config


def _is_image_file(input_data):
    """
    输入数据是否为影像文件
    通过后缀名判断
    """
    import rasterio
    try:
        with rasterio.open(input_data) as ds:
            data_is_image = True
    except Exception as e:
        data_is_image = False

    return data_is_image


def save_pattle_png(image, color_codes, out_file):
    if out_file.endswith('png'):
        r = sorted(color_codes.items(), key=lambda d: d[1])
        palette = [color_value for class_color in r for color_value in class_color[0]]
        out = Image.fromarray(np.squeeze(image), mode="P")
        out.putpalette(palette)
        out.save(out_file, optimize=True)
    else:
        raise Exception('out_file should end with png')


def del_dir(path, is_del_dir=True):
    """
       删除文件夹
    """
    if is_del_dir is True:
        if os.path.exists(path):
            shutil.rmtree(path)


# def compute_bbox_iou(box, boxes, box_area, boxes_area):
#     """Calculates IoU of the given box with the array of the given boxes.
#     # 使用给定框的数组计算给定框的IoU。即计算一个bbox和一组bbox的iou
#     box: 1D vector [y1, x1, y2, x2], equal to [ymin,xmin,ymax,xmax]
#     boxes: [boxes_count, (y1, x1, y2, x2)]
#     box_area: float. the area of 'box'  'box'的面积
#     boxes_area: array of length boxes_count.  长度数组box_count
#     Note: the areas are passed in rather than calculated here for
#           efficency. Calculate once in the caller to avoid duplicate work.
#     :return: 一维数组
#     """
#     # Calculate intersection areas
#     y1 = np.maximum(box[0], boxes[:, 0])  # 数值对应元素比较取最大值 a=[[3,4],[2,5]],b=[[1,5],[3,7]] np.maximum(a,b) [[3,5],[3,7]]
#     y2 = np.minimum(box[2], boxes[:, 2])  # 数值对应元素比较取最小值
#     x1 = np.maximum(box[1], boxes[:, 1])
#     x2 = np.minimum(box[3], boxes[:, 3])
#     intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)
#     union = box_area + boxes_area[:] - intersection[:]
#     iou = intersection / union
#     return iou


# def non_max_suppression(boxes, scores, threshold):
#     """Performs non-maximum supression and returns indicies of kept boxes.
#     执行非最大抑制并返回保留框的索引。
#     boxes: [N, (y1, x1, y2, x2)]. Notice that (y2, x2) lays outside the box. 注意（y2，x2）在框外。
#     scores: 1-D array of box scores.
#     threshold: Float. IoU threshold to use for filtering. 用于过滤的IoU阈值
#     """
#     assert boxes.shape[0] > 0
#     if boxes.dtype.kind != "f":  # not float
#         boxes = boxes.astype(np.float32)
#
#     # Compute box areas
#     y1 = boxes[:, 0]
#     x1 = boxes[:, 1]
#     y2 = boxes[:, 2]
#     x2 = boxes[:, 3]
#     area = (y2 - y1) * (x2 - x1)
#
#     # Get indicies of boxes sorted by scores (highest first)
#     # 获取按分数排序的盒子索引（最高的第一个）
#     ixs = scores.argsort()[::-1]  # 从小到大排序
#
#     pick = []
#     while len(ixs) > 0:
#         # Pick top box and add its index to the list
#         # 选择顶部框并将其索引添加到列表中
#         i = ixs[0]
#         pick.append(i)
#         # Compute IoU of the picked box with the rest
#         iou = compute_bbox_iou(boxes[i], boxes[ixs[1:]], area[i], area[ixs[1:]])
#         # Identify boxes with IoU over the threshold. This
#         # returns indicies into ixs[1:], so add 1 to get
#         # indicies into ixs.
#         # 识别超过阈值的IoU的盒子。 这将索引返回到ixs [1：]，所以加1以使索引变成ixs。
#         remove_ixs = np.where(iou > threshold)[0] + 1
#         # Remove indicies of the picked and overlapped boxes.
#         # 删除挑选和重叠框的索引。
#         ixs = np.delete(ixs, remove_ixs)
#         ixs = np.delete(ixs, 0)
#     return np.array(pick, dtype=np.int32)


def get_pic_path_from_dir(input_dir, get_all_dir, suffix):
    """
    从给定的文件目录下获取指定后缀图片的完整路径
    :param input_dir: 输入文件目录的路径
    :type input_dir: str
    :param get_all_dir: 是否获取输入路径下所有子目录内图片
    :type get_all_dir: bool
    :param suffix: 指定获取图片的后缀
    :type suffix: list

    :return image_path_list: 所获图片的完整路径
    :type image_path_list: list [image_path1,image_path2,...]
    """
    image_path_list = []
    # 获取图片的完整路径
    if get_all_dir:
        for root, dir, files in os.walk(input_dir):
            for file in files:
                # 初步以后缀过滤掉非图片文件
                file_suffix = os.path.splitext(file)[-1]
                if file_suffix in suffix:
                    image_path = os.path.abspath(os.path.join(root, file))
                    image_path_list.append(image_path)
    else:
        files = os.listdir(input_dir)
        for file in files:
            file_suffix = os.path.splitext(file)[-1]
            if file_suffix in suffix:
                image_path = os.path.abspath(os.path.join(input_dir, file))
                image_path_list.append(image_path)

    return image_path_list


def mkdir_not_exist(path_list):
    """
    创建单个或者多个不存在的文件路径
    path_list[path1, path2, path3]
    :param path1:某个需要被创建的文件路径
    type path1: str
    """
    for _path in path_list:
        if not os.path.exists(_path):
            os.makedirs(_path)


def get_available_filename(dir_path, file_name):
    """
    根据 file_name 获取 dir_path 下可用的文件名

    :param dir_path: 要保存数据的所在文件路径
    :param file_name: 用户输入的想要保存的文件名
    :return: available_filename: 可用的文件名
            available_file_path: 可用的文件保存路径
    """
    file_num = 1
    file_path, ext = os.path.splitext(os.path.join(dir_path, file_name))
    available_file_path = os.path.join(dir_path, file_name)
    while os.path.exists(available_file_path):
        available_file_path = file_path + ('_' + str(file_num)) + ext
        file_num += 1
    available_filename = os.path.basename(available_file_path)
    return available_filename, available_file_path


def get_out_data_dir(out_data):
    """
    根据输入的out_data获取文件夹路径；
    输入datasource数据源则返回数据源所在文件夹，输入数据源字符串路径同理；
    输入文件夹字符串路径则返回文件夹路径
    :param out_data: string or datasource
    :return: 文件夹路径，string
    """
    from iobjectspy import Datasource
    if isinstance(out_data,Datasource):
        if out_data.connection_info.server == ':memory:':
            out_data = None
        else:
            out_data = os.path.dirname(out_data.connection_info.server)
    elif isinstance(out_data,str):
        if out_data.endswith('.udbx') or out_data.endswith('.udb'):
            out_data = os.path.dirname(out_data)
        elif out_data.startswith("--server"):
            connection_info = {param.split('=')[0]: param.split('=')[1].strip() for param in
                               [i for i in out_data.split('--') if i != '']}
            out_data = os.path.dirname(connection_info.get("server"))
        else:
            out_data = None
    else:
        out_data = None
    return out_data
