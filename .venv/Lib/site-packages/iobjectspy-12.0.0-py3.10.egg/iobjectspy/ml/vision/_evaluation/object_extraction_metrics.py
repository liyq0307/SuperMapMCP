# !/usr/bin/env python3
# coding=utf-8

import os
import re

from iobjectspy._jsuperpy._utils import check_lic
from iobjectspy.ml.vision._evaluation._base_utils import save_metric_dict_to_attribute_table
from iobjectspy.ml.vision._evaluation._obj_metrics import _obj_map, _build_numpy_array_dataset, _obj_prec_rec, _obj_f1
    # _get_classe_names_from_sda, _get_classe_names_from_model, _build_numpy_array_xml


def object_extraction_all(inference_data, ground_truth_data,
                         inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                         iou_thr=0.5,
                         nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的mAP,prec_recall,f1

    """
    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)
    mAP, AP, _ = _obj_map(inference_array,
                          ground_truth_array,
                          categorys_dict,
                          scale_ranges=None,
                          iou_thr=iou_thr,
                          dataset=None,
                          logger=None,
                          nproc=nproc)

    Recall, Precision, _ = _obj_prec_rec(inference_array,
                                         ground_truth_array,
                                         categorys_dict,
                                         scale_ranges=None,
                                         iou_thr=iou_thr,
                                         dataset=None,
                                         logger=None,
                                         nproc=nproc)

    F1, _ = _obj_f1(inference_array,
                    ground_truth_array,
                    categorys_dict,
                    scale_ranges=None,
                    iou_thr=iou_thr,
                    dataset=None,
                    logger=None,
                    nproc=nproc)

    metric_dict = {'F1': F1, 'Recall': Recall, 'Precision_': Precision,'AP':AP, 'mAP': mAP}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)

    return metric_dict

def object_extraction_map(inference_data, ground_truth_data,
                         inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                         iou_thr=0.5,
                         nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的mAP

        通过11点插值法计算出来的mAP评估指标（首先设定一组阈值，[0, 0.1, 0.2, …, 1]。
        然后对于recall大于每一个阈值（比如recall>0.3），我们都会得到一个对应的最大precision。
        这样，我们就计算出了11个precision。mAP即为这11个precision的平均值

    """

    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)

    mAP, _, _ = _obj_map(inference_array,
                         ground_truth_array,
                         categorys_dict,
                         scale_ranges=None,
                         iou_thr=iou_thr,
                         dataset=None,
                         logger=None,
                         nproc=nproc)
    metric_dict = {"mAP": mAP}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)

    return metric_dict

def object_extraction_ap(inference_data, ground_truth_data,
                         inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                         iou_thr=0.5,
                         nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的mAP

        通过11点插值法计算出来的mAP评估指标（首先设定一组阈值，[0, 0.1, 0.2, …, 1]。
        然后对于recall大于每一个阈值（比如recall>0.3），我们都会得到一个对应的最大precision。
        这样，我们就计算出了11个precision。mAP即为这11个precision的平均值

    """

    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)

    _, AP, _ = _obj_map(inference_array,
                          ground_truth_array,
                          categorys_dict,
                          scale_ranges=None,
                          iou_thr=iou_thr,
                          dataset=None,
                          logger=None,
                          nproc=nproc)
    metric_dict = {"AP": AP}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)

    return metric_dict

def object_extraction_precision(inference_data, ground_truth_data,
                             inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                             iou_thr=0.5,
                             nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的召回率与精确率

        TP,TN,FP和FN分别表示真阳性，真阴性,假阳性和假阴性计数。
        召回率(recalls)是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了;并且由以下等式定义：recalls=TP/(TP+FN)
        精确率(precision)是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本;并且由以下等式定义：precision=TP/(TP+FP)

    """



    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)

    _, Precision, _ = _obj_prec_rec(inference_array,
                                         ground_truth_array,
                                         categorys_dict,
                                         scale_ranges=None,
                                         iou_thr=iou_thr,
                                         dataset=None,
                                         logger=None,
                                         nproc=nproc)
    metric_dict = { 'Precision_': Precision}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)


    return metric_dict

def object_extraction_recall(inference_data, ground_truth_data,
                             inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                             iou_thr=0.5,
                             nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的召回率与精确率

        TP,TN,FP和FN分别表示真阳性，真阴性,假阳性和假阴性计数。
        召回率(recalls)是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了;并且由以下等式定义：recalls=TP/(TP+FN)
        精确率(precision)是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本;并且由以下等式定义：precision=TP/(TP+FP)

    """

    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)

    Recall, _, _ = _obj_prec_rec(inference_array,
                                 ground_truth_array,
                                 categorys_dict,
                                 scale_ranges=None,
                                 iou_thr=iou_thr,
                                 dataset=None,
                                 logger=None,
                                 nproc=nproc)
    metric_dict = {'Recall': Recall}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)

    return metric_dict

def object_extraction_f1(inference_data, ground_truth_data,
                         inference_class_value_field='category', ground_truth_class_value_field=None, out_data='',out_data_name='',categorys_dict=None,
                         iou_thr=0.5,
                         nproc=4):
    """
        基于输入的真实标签数据集和预测标签数据集计算结果的召回率与精确率

        F1分数(F1 Score)是统计学中用来衡量二分类模型精确度的一种指标,它同时兼顾了分类模型的精确率和召回率,F1分数可以看作是模型精确率和召回率的一种加权平均:并且由以下等式定义:2*(precision*recall)/(precision+recall)

    """

    inference_array, ground_truth_array = _build_numpy_array_dataset(inference_data,
                                                                     ground_truth_data,
                                                                     categorys_dict,
                                                                     ground_truth_class_value_field,
                                                                     inference_class_value_field)

    F1, _ = _obj_f1(inference_array,
                    ground_truth_array,
                    categorys_dict,
                    scale_ranges=None,
                    iou_thr=iou_thr,
                    dataset=None,
                    logger=None,
                    nproc=nproc)
    metric_dict = {'F1': F1}
    if out_data is not None:
        save_metric_dict_to_attribute_table(metric_dict, out_data, out_data_name)

    return metric_dict


