from functools import partial

import torch
import torch.nn as nn
from torch.nn import functional as F

from ._base import EncoderMixin
from ....prompt_segmentation.segment_anything.modeling import ImageEncoderViT
from ....prompt_segmentation.segment_anything.modeling import PromptEncoder


class SamEncoder(nn.Module, EncoderMixin):
    def __init__(self,
                 encoder_embed_dim: int,
                 encoder_depth: int,
                 encoder_num_heads: int,
                 encoder_global_attn_indexes: list,
                 **kwargs):
        super().__init__()
        prompt_embed_dim = 256
        image_size = kwargs.get('image_size', 1024)
        vit_patch_size = 16
        image_embedding_size = image_size // vit_patch_size
        self.image_encoder = ImageEncoderViT(
            depth=encoder_depth,
            embed_dim=encoder_embed_dim,
            img_size=image_size,
            mlp_ratio=4,
            norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),
            num_heads=encoder_num_heads,
            patch_size=vit_patch_size,
            qkv_bias=True,
            use_rel_pos=True,
            global_attn_indexes=encoder_global_attn_indexes,
            window_size=14,
            out_chans=prompt_embed_dim,
        )
        self.prompt_encoder = PromptEncoder(
            embed_dim=prompt_embed_dim,
            image_embedding_size=(image_embedding_size, image_embedding_size),
            input_image_size=(image_size, image_size),
            mask_in_chans=16,
        )

    def forward(self, x):
        input_images = self.preprocess(x)
        image_embeddings = self.image_encoder(input_images)
        sparse_embeddings, dense_embeddings = self.prompt_encoder(
            points=None, boxes=None, masks=None
        )
        return image_embeddings, sparse_embeddings, dense_embeddings

    def preprocess(self, x: torch.Tensor) -> torch.Tensor:
        """pad to a square input."""
        # Pad
        h, w = x.shape[-2:]
        padh = self.image_encoder.img_size - h
        padw = self.image_encoder.img_size - w
        x = F.pad(x, (0, padw, 0, padh))
        return x

    def load_state_dict(self, state_dict):
        # 创建新的状态字典
        image_encoder_state_dict = {}
        prompt_encoder_state_dict = {}

        # 分配权重
        for key, value in state_dict.items():
            if key.startswith('image_encoder'):
                new_key = key.replace('image_encoder.', '')
                image_encoder_state_dict[new_key] = value
            elif key.startswith('prompt_encoder'):
                new_key = key.replace('prompt_encoder.', '')
                prompt_encoder_state_dict[new_key] = value

        self.image_encoder.load_state_dict(image_encoder_state_dict)
        self.prompt_encoder.load_state_dict(prompt_encoder_state_dict)
        return {}, {}

    def get_stages(self):
        pass


sam_vit_encoders = {
    "sam-vit-b": {
        "encoder": SamEncoder,
        "pretrained_settings": {
            "sa-1b": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            },
            "imagenet": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            }
        },
        "params": {
            "encoder_embed_dim": 768,
            "encoder_depth": 12,
            "encoder_num_heads": 12,
            "encoder_global_attn_indexes": [2, 5, 8, 11],
            "image_size": 512,
        }
    },
    "sam-vit-l": {
        "encoder": SamEncoder,
        "pretrained_settings": {
            "sa-1b": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            },
            "imagenet": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            }
        },
        "params": {
            "encoder_embed_dim": 1024,
            "encoder_depth": 24,
            "encoder_num_heads": 16,
            "encoder_global_attn_indexes": [5, 11, 17, 23],
            "image_size": 512,
        }
    },
    "sam-vit-h": {
        "encoder": SamEncoder,
        "pretrained_settings": {
            "sa-1b": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            },
            "imagenet": {
                "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth",
                "input_space": "RGB",
                "input_size": [3, 1024, 1024],
                "input_range": [0, 1],
                "mean": [0.485, 0.456, 0.406],
                "std": [0.229, 0.224, 0.225],
                "num_classes": None,
            }
        },
        "params": {
            "encoder_embed_dim": 1024,
            "encoder_depth": 24,
            "encoder_num_heads": 16,
            "encoder_global_attn_indexes": [5, 11, 17, 23],
            "image_size": 512,
        }
    }
}
