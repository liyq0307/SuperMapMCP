import torch.nn as nn
from .decoder import Mask2FormerHead
from typing import Optional
from ..encoders import get_encoder
from ..base import SemanticInferenceHead, SegmentationModel
from addict import Dict
from ..encoders import encoders


class Mask2Former(SegmentationModel):
    """
    Main class for mask classification semantic segmentation architectures.
    Args:
        encoder_name: name of classification model (without last dense layers) used as feature
            extractor to build segmentation model.
        encoder_depth: number of stages used in decoder, larger depth - more features are generated.
            e.g. for depth=3 encoder will generate list of features with following spatial shapes
            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have
            spatial resolution (H/(2^depth), W/(2^depth)]
        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).
        decoder_pyramid_channels: a number of convolution filters in decoder of segformer_.
        decoder_segmentation_channels: a number of convolution filters in segmentation head of mask2former.
        decoder_dropout: spatial dropout rate in range (0, 1).
        in_channels: number of input channels for model, default is 3.
        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).
        activation (str, callable): activation function used in ``.predict(x)`` method for inference.
            One of [``sigmoid``, ``softmax2d``, callable, None]
        upsampling: optional, final upsampling factor
            (default is 4 to preserve input -> output spatial shape identity)
        aux_params: if specified model will have additional classification auxiliary output
            build on top of encoder, supported params:
                - classes (int): number of classes
                - pooling (str): one of 'max', 'avg'. Default is 'avg'.
                - dropout (float): dropout factor in [0, 1)
                - activation (str): activation function to apply "sigmoid"/"softmax" (could be None to return logits)

    Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation.
    """

    def __init__(
            self,
            encoder_name: str = "mit-b3",
            encoder_depth: int = 5,
            encoder_weights: Optional[str] = "imagenet",
            in_channels: int = 3,
            classes: int = 1,
    ):
        super().__init__()
        self.siame_encoder = False
        self.encoder_name = encoder_name
        self.encoder = get_encoder(
            encoder_name,
            in_channels=in_channels,
            depth=encoder_depth,
            weights=encoder_weights,
        )
        self.decoder = Mask2FormerHead(classes, self.output_shape())

        self.segmentation_head = SemanticInferenceHead()
        self.classification_head = None

    def output_shape(self):
        self._out_features = ["res2", "res3", "res4", "res5"]
        self._out_feature_strides = {
            "res2": 4,
            "res3": 8,
            "res4": 16,
            "res5": 32,
        }
        encoder_out_channels = encoders[self.encoder_name]["params"]["out_channels"]
        self._out_feature_channels = {
            "res2": encoder_out_channels[-4],
            "res3": encoder_out_channels[-3],
            "res4": encoder_out_channels[-2],
            "res5": encoder_out_channels[-1],
        }

        backbone_feature_shape = dict()
        for name in self._out_features:
            backbone_feature_shape[name] = Dict(
                {'channel': self._out_feature_channels[name], 'stride': self._out_feature_strides[name]})

        return backbone_feature_shape
