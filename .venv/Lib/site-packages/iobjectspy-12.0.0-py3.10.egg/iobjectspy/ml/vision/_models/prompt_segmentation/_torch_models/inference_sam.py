# !/usr/bin/env python3
# coding=utf-8
import os
import time

import numpy as np
import rasterio
import torch
import torch.nn as nn
import tqdm
from torch.utils.data import DataLoader

from iobjectspy import (open_datasource, create_datasource, vector_to_raster, PromptSampleType, Datasource,
                        datasetraster_to_numpy_array)
from iobjectspy._jsuperpy.data._util import FieldType
from ..segment_anything import sam_model_registry
from ..segment_anything.dataset import (SamBboxDataset, SamPointDataset)
from ..segment_anything.sample import LocalSimilaritySample, EqualDistanceSample
from ..segment_anything.utils.utils import (
    sam_filt_masks,
    mask2bbox_nms,
    mask_seg_mask,
    sam_bbox_postprocess,
)
from ..segment_anything.utils.amg import BatchMaskData
from ..utils import (
    np_to_tensor,
    coor2xyxy,
    get_coordinate,
    rle_masks2ori_mask,
    read_raster_data,
    resize_matrix,
    resize_np_by_bounds,
    encode_mask_results
)
from ...device_utils import Device_type

sample_functions = {
    PromptSampleType.LOCALSIMILARITY: LocalSimilaritySample,
    PromptSampleType.EQUALDISTANCE: EqualDistanceSample,
}


# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'


class SamVectorEstimation:
    """
    SAM推理类：通过提示类型的不同来用不同的方法进行推理
    """

    def __init__(self, config, sam_model_type, model_path, gpu, batch_size, **kwargs):
        self.sam_model_type = sam_model_type
        # pth文件路径
        self.sam_model_pth = model_path
        self.gpu = gpu
        self.ori_image_size = None
        self.transform = None
        self.rectangle_size = None
        self.mask_label = None
        self.original_mask = None
        self.original_mask_temp = None
        self.bs = batch_size
        self.devices = 'cpu'
        # 创建sam模型实例,并加载权重文件,并指定推理设备cuda或cpu
        self.masks_process = []
        self.points_process = []
        self.model = None
        self.sam = self.build_sam()

    def build_sam(self):
        # 创建sam模型实例，并加载权重文件Instantiate a SAM model
        sam = None
        print(f"---------Instantiate a SAM_{self.sam_model_type} model---------")
        start_time = time.time()
        sam_model_registrar = sam_model_registry[self.sam_model_type]
        Device_type.setup()
        # sam_model_registrar = sam_model_fast_registry[self.sam_model_type] # 不支持Windows
        # 包装模型以支持数据并行
        try:
            if Device_type.device_count() > 1 and len(self.gpu) != 1:
                sam_device = min(self.gpu)
                sam = sam_model_registrar(checkpoint=self.sam_model_pth, device=f'{Device_type._device}:{sam_device}')
                self.devices = Device_type._device
                print("Multiple GPU")
            elif len(self.gpu) == 1:
                if self.gpu[0] != -1:
                    sam_device = self.gpu[0]
                    sam = sam_model_registrar(checkpoint=self.sam_model_pth, device=f'{Device_type._device}:{sam_device}')
                    self.devices = Device_type._device
                    print("Single GPU")
                else:
                    self.devices = 'cpu'
                    Device_type.setup('cpu')
                    sam = sam_model_registrar(checkpoint=self.sam_model_pth, device=self.devices)
                    print("CPU")
            if sam is None:
                raise RuntimeError("SAM Model was not instantiated! "
                                   "Please check if the GPU is available and if the GPU number is entered correctly.")
        except Exception as e:
            raise e

        print(f"---------The SAM model has been successfully instantiated, "
              f"taking {int(time.time() - start_time)}s---------")
        return sam

    def _set_sam_parameters(self, sample_method, **kwargs):
        self.sample_func = sample_functions[sample_method](self)
        self.sample_func.set_up(**kwargs)

    def predict(self, input_data, prompt_data, outpath, outname, prompt_type, tilesize, offset, sample_method,
                **kwargs):
        """
        通过提示的模式判断使用哪些推理方式
        :param input_data:      输入的tif影像图片路径(str)
        :param prompt_data:     输入的矢量提示数据(DatasetVector)
        :param outpath:         输出的数据源Udbx路径(str)
        :param outname:         输出结果保存的数据集名称(str)
        :param prompt_type:     提示的模式(str)
        :param tilesize:        用户指定的block大小
        :param offset:          用户指定的tile重叠量
        :param sample_method:   采样点策略
        :return:                输出结果保存的数据集名称(str)
        """
        self.tilesize = tilesize
        self.offset = offset

        if prompt_type == 'point':
            from iobjectspy.ml.vision._models.prompt_segmentation.segment_anything.inference import SamPointInference
            self.model = SamPointInference(self.sam, self.bs)
            result = self._infer_point_prompt(input_data, prompt_data, outpath, outname, **kwargs)
        elif prompt_type == 'bbox':
            from iobjectspy.ml.vision._models.prompt_segmentation.segment_anything.inference import SamInference
            self.model = SamInference(self.sam, self.bs)
            result = self._bbox_prompt_process(input_data, prompt_data, outpath, outname, **kwargs)

        elif prompt_type == 'polygon':
            from iobjectspy.ml.vision._models.prompt_segmentation.segment_anything.inference import SamInference
            if Device_type.device_count() > 1 and len(self.gpu) != 1:
                self.model = SamInference(self.sam, self.bs // len(self.gpu))
                self.model = nn.DataParallel(self.model, device_ids=self.gpu)
                self.model.to(self.devices)
            else:
                self.model = SamInference(self.sam, self.bs)
            if sample_method is None:
                sample_method = PromptSampleType.LOCALSIMILARITY
            result = self._polygon_prompt_process(input_data, prompt_data, outpath, outname,
                                                  sample_method=sample_method, **kwargs)

        elif prompt_type == 'noPrompt':
            from iobjectspy.ml.vision._models.prompt_segmentation.segment_anything.inference import SamInference
            if Device_type.device_count() > 1 and len(self.gpu) != 1:
                self.model = SamInference(self.sam, self.bs // len(self.gpu))
                self.model = nn.DataParallel(self.model, device_ids=self.gpu)
                self.model.to(self.devices)
            else:
                self.model = SamInference(self.sam, self.bs)
            if sample_method is None:
                sample_method = PromptSampleType.EQUALDISTANCE
            result = self._noprompt_process(input_data, outpath, outname,
                                            sample_method=sample_method, **kwargs)
        else:
            # 如果prompt_type不是预期的值，抛出异常
            raise ValueError("Unknown prompt type! Please specify a valid type：'point', 'bbox', 'polygon', 'noPrompt'.")

        return result

    # def _infer_point_prompt(self, input_data, prompt_data, outpath, outname, **kwargs):
    #     """
    #     输入矢量点数据作为提示
    #     :param input_data:
    #     :param prompt_data:
    #     :param outpath:
    #     :param outname:
    #     :param kwargs:
    #     :return:
    #     """
    #     result = os.path.exists(outpath)
    #     if not result:
    #         create_datasource(outpath)
    #
    #     tif_path = input_data
    #     with rasterio.open(tif_path) as src:
    #         src_bounds = src.bounds
    #     datasetname = self._prompt_predict(src_bounds, tif_path, prompt_data, outpath, outname)
    #     return datasetname

    def _bbox_prompt_process(self, input_data, prompt_data, outpath, outname, **kwargs):
        """
        输入矢量点数据作为提示
        :param input_data:
        :param prompt_data:
        :param outpath:
        :param outname:
        :param kwargs:
        :return:
        """
        # 设置超参数
        self.sam_iou_thresh = kwargs.get('sam_iou_thresh', 0.75)
        self.mask_area_thresh = 0

        if isinstance(outpath, str):
            result = os.path.exists(outpath)
            if not result:
                create_datasource(outpath)

        tif_path = input_data
        # 打开图像
        with rasterio.open(tif_path, encoding='utf-8') as ds:
            img_width, img_height, rectangle_ymin, rectangle_xmin, transform = read_raster_data(ds, infer_region=None)
            self.ori_image_size = (img_width, img_height)
            self.affine_transform = transform
            features_list = prompt_data.get_features()  # dt是一个DatasetVector对象
            # 将数据还原到原图中对应的经纬度格式,并保存分数和类别信息
            coordinate_boxes_list = get_coordinate(features_list, bbox=True)
            # 将coordinate_box全部转换为基于像素坐标的(x1,y1,x2,y2)
            boxes_list = coor2xyxy(coordinate_boxes_list, self.affine_transform, bbox=True)
            temp_list = [boxe_[:4] for boxe_ in boxes_list]
            boxes_list_np = np.array(temp_list)

        dataset = SamBboxDataset(self.sam, tif_path, prompt_data, boxes_list_np, self.tilesize, self.offset)
        datasetname = self._bbox_prompt_infer(dataset, outpath, outname)
        return datasetname

    def _polygon_prompt_process(self, input_data, prompt_data, outpath, outname, sample_method, **kwargs):
        """
        提供多边形提示，在多边形内生成采样点进行推理
        :param input_data:      输入的tif影像图片路径(str)
        :param prompt_data:     输入的矢量提示数据(DatasetVector)
        :param outpath:         输出的数据源Udbx路径(str)
        :param outname:         输出结果保存的数据集名称(str)
        :return:                输出结果保存的数据集名称(str)
        """
        # 设置超参数
        self._set_sam_parameters(sample_method, **kwargs)
        save_intermediate_datasets = kwargs.get('save_intermediate_datasets', False)

        if isinstance(outpath, str):
            result = os.path.exists(outpath)
            if not result:
                ds_temp = create_datasource(outpath)
            else:
                ds_temp = open_datasource(outpath)
        if isinstance(outpath, Datasource):
            ds_temp = outpath

        tif_path = input_data
        with rasterio.open(tif_path) as src:
            img_width, img_height, rectangle_ymin, rectangle_xmin, transform = read_raster_data(src, infer_region=None)
            src_bounds = src.bounds
            self.res = abs(src.res[0])

        raster_name = prompt_data.name + '_raster'
        prompt_data_raster = ds_temp.get_dataset(raster_name)
        if not prompt_data_raster:
            prompt_data_raster = vector_to_raster(prompt_data, 'SmUserID',
                                                  cell_size=self.res, pixel_format=FieldType.INT32,
                                                  out_data=ds_temp)
        prompt_bounds = prompt_data_raster.bounds
        prompt_np = datasetraster_to_numpy_array(prompt_data_raster).astype(np.uint8)
        ds_temp.delete(prompt_data_raster.name)
        prompt_np[prompt_np > 0] = 1

        if src_bounds != prompt_bounds:
            prompt_np = resize_np_by_bounds(src_bounds, prompt_bounds, self.res, prompt_np)
        target_shape = (img_height, img_width)
        self.mask_label = resize_matrix(prompt_np, target_shape)

        dataset = SamPointDataset(self.sam, tif_path, prompt_np, self.tilesize, self.offset, self.point_num,
                                  sample_method=self.sample_func,
                                  save_intermediate_datasets=save_intermediate_datasets)
        datasetname = self._region_prompt_infer(dataset, src_bounds, outpath, outname)

        return datasetname

    def _noprompt_process(self, input_data, outpath, outname, sample_method, **kwargs):
        """
        提供多边形提示，在多边形内生成采样点进行推理
        :param input_data:      输入的tif影像图片路径(str)
        :param prompt_data:     输入的矢量提示数据(DatasetVector)
        :param outpath:         输出的数据源Udbx路径(str)
        :param outname:         输出结果保存的数据集名称(str)
        :return:                输出结果保存的数据集名称(str)
        """
        # 设置超参数
        self._set_sam_parameters(sample_method, **kwargs)
        save_intermediate_datasets = kwargs.get('save_intermediate_datasets', False)

        if isinstance(outpath, str):
            result = os.path.exists(outpath)
            if not result:
                create_datasource(outpath)

        tif_path = input_data
        with rasterio.open(tif_path) as src:
            img_width, img_height, rectangle_ymin, rectangle_xmin, transform = read_raster_data(src, infer_region=None)
            src_bounds = src.bounds

        prompt_np = np.zeros((img_height, img_width), dtype=np.uint8)
        self.mask_label = prompt_np
        dataset = SamPointDataset(self.sam, tif_path, prompt_np, self.tilesize, self.offset, self.point_num,
                                  sample_method=self.sample_func,
                                  save_intermediate_datasets=save_intermediate_datasets)
        datasetname = self._region_prompt_infer(dataset, src_bounds, outpath, outname)

        return datasetname

    def _bbox_prompt_infer(self, dataset, outpath, outname):
        self.original_mask = np.zeros((dataset.ori_image_size[1], dataset.ori_image_size[0]), dtype=np.bool_)
        profile = dataset.profile
        profile.update(dtype=rasterio.uint8, count=1)

        assert self.bs == 1, "The batch size for bbox prompt inference is only supported at 1!"
        # 创建 DataLoader 实例
        dataloader = DataLoader(dataset, batch_size=self.bs)
        for batch, idx in tqdm.tqdm(dataloader, desc='Inferencing...'):
            if "bbox" not in batch:
                continue
            self.model.reset_image()
            tile_corner = batch["tile_corner"]
            image = batch["tile_rgb"].to(self.devices)
            image_size = batch["tile_size"].to(self.devices)
            transformed_boxes = batch["bbox"].to(self.devices)
            masks, iou_preds = self.model(image, image_size, tile_corner,
                                          transformed_boxes=transformed_boxes, return_logits=False)
            # sam后处理过滤
            mask_data = sam_bbox_postprocess(masks, iou_preds, self.sam_iou_thresh)

            if isinstance(mask_data["segmentations"], list):
                mask_list_temp_ = mask_data
            elif isinstance(mask_data["segmentations"], dict):
                mask_list_temp_ = []
                for key, masks_value in mask_data["segmentations"].items():
                    mask_list_temp_.append(masks_value)

            mask_list = {k: [np_to_tensor(i) for i in mask_list_] for k, mask_list_ in enumerate(mask_list_temp_)}

            # 将数据转化为原图对应的mask然后加到original_mask中
            tile_leups = [np.array([[int(tile_corner[int(k)][0]), int(tile_corner[int(k)][1])]])
                          for k, v in mask_list.items() for _ in range(len(v))]
            if len(tile_leups) == 0:
                continue
            tile_leups_np = np.concatenate(tile_leups)

            mask_all = [value for sublist in mask_list.values() for value in sublist]
            masks_list_tensor = torch.cat(mask_all, dim=1).cpu().squeeze(0)
            masks_list_rle = encode_mask_results(masks_list_tensor)
            rle_masks2ori_mask(self.original_mask, masks_list_rle, tile_leups_np)

        # 内存释放
        Device_type.empty_cache()
        del self.model

        eroded_mask = self.original_mask.astype(np.uint8)
        affine_matrix = dataset.affine_transform
        if affine_matrix.e > 0:
            eroded_mask = np.flipud(eroded_mask)
        outname = dataset.write2dataset(eroded_mask, outpath, outname, profile)

        return outname

    def _region_prompt_infer(self, dataset, src_bounds, outpath, outname):
        """
        这个类主要适配范围提示推理，通过给定的范围生成采样点来进行推理
        :param dataset:      SAMDataset类
        :param src_bounds:   影像范围
        :param outpath:      输出的udbx路径
        :param outname:      输出数据集名称
        :return:
        """
        self.original_mask = np.zeros((dataset.ori_image_size[1], dataset.ori_image_size[0]), dtype=np.bool_)
        self.transform = dataset.affine_transform
        profile = dataset.profile
        profile.update(dtype=rasterio.uint8, count=1)

        sample_points_list = []
        # 创建 DataLoader 实例
        dataloader = DataLoader(dataset, batch_size=self.bs, pin_memory=True)
        for batch, idx in tqdm.tqdm(dataloader, desc='Inferencing...'):
            tile_corner = batch["tile_corner"].to(self.devices)
            image = batch["tile_rgb"].to(self.devices)
            image_size = batch["tile_size"].to(self.devices)
            transf_points = batch["sample_pos_points"]
            labels = batch["sample_pos_labels"]
            crop_box = [0, 0, self.tilesize, self.tilesize]
            ori_box = [0, 0, self.mask_label.shape[1], self.mask_label.shape[0]]
            if transf_points.numel() == 0:
                continue

            n_batches = labels.shape[1] // 64 + int(labels.shape[1] % 64 != 0)
            data = BatchMaskData()
            for b in range(n_batches):
                transf_points_batch = transf_points[:, b * 64: (b + 1) * 64].to(self.devices)
                labels_batch = labels[:, b * 64: (b + 1) * 64].to(self.devices)
                masks_batch, iou_preds_batch = self.model(image, image_size, tile_corner,
                                                          transf_points=transf_points_batch, labels=labels_batch)
                # sam后处理过滤
                data_batch = sam_filt_masks(crop_box, ori_box, masks_batch, iou_preds_batch,
                                            self.sam_iou_thresh, self.sam_mask_threshold,
                                            self.sam_filter_edge)
                data.cat(data_batch)
                del data_batch

            if isinstance(self.model, nn.DataParallel):
                self.model.module.reset_image()
            else:
                self.model.reset_image()
            Device_type.empty_cache()
            # sam后处理过滤
            mask_data = mask2bbox_nms(data, self.sam_bbox_nms_thresh)

            # mask segmentation mask
            mask_list_temp_ = mask_seg_mask(mask_data, self.mask_area_thresh, self.tile_open_kernel,
                                            self.bias, self.tile_open_iterations)

            mask_list = {k: [np_to_tensor(i) for i in mask_list_]
                         for k, mask_list_ in enumerate(mask_list_temp_)}

            # 将数据转化为原图对应的mask然后加到original_mask中
            tile_leups = [np.array([[int(tile_corner[int(k)][0]), int(tile_corner[int(k)][1])]])
                          for k, v in mask_list.items() for _ in range(len(v))]
            if len(tile_leups) == 0:
                continue
            tile_leups_np = np.concatenate(tile_leups)

            mask_all = [value for sublist in mask_list.values() for value in sublist]
            masks_list_tensor = torch.cat(mask_all, dim=1).cpu().squeeze(0)
            masks_list_rle = encode_mask_results(masks_list_tensor)

            rle_masks2ori_mask(self.original_mask, masks_list_rle, tile_leups_np)

            # 保存采样点
            sample_pos_points = batch['ori_sample_points']
            sample_points_list.append(sample_pos_points)
            del batch, mask_data

        # 内存释放
        del self.model, self.sam
        Device_type.empty_cache()

        # 用原来的提示滤掉提示区域外的结果
        label = 1 - self.mask_label
        try:
            self.original_mask = np.logical_and(self.original_mask, label).astype(np.uint8)
        except Exception as e:
            self.original_mask = self._logical_and_blockwise(self.original_mask, label)
        eroded_mask = self.original_mask.astype(np.uint8)
        affine_matrix = dataset.affine_transform
        if affine_matrix.e > 0:
            eroded_mask = np.flipud(eroded_mask)

        outname = dataset.write2dataset(sample_points_list, eroded_mask, outpath, outname, src_bounds, profile,
                                        self.post_process, self.morph_process_kernel, self.morph_process_iterations)

        return outname

    def _logical_and_blockwise(self, array1, array2, block_size=1000):
        """分块处理逻辑与运算

        Args:
            array1: 第一个数组
            array2: 第二个数组
            block_size: 分块大小

        Returns:
            result: 处理后的数组
        """
        # 方案1：使用分块处理
        h, w = array1.shape
        result = np.empty_like(array1, dtype=np.uint8)

        for i in range(0, h, block_size):
            for j in range(0, w, block_size):
                i_end = min(i + block_size, h)
                j_end = min(j + block_size, w)

                block_result = np.logical_and(
                    array1[i:i_end, j:j_end],
                    array2[i:i_end, j:j_end]
                ).astype(np.uint8)

                result[i:i_end, j:j_end] = block_result

        return result
