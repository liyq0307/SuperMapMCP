# -*- coding:utf8 -*-

"""
Mask R-CNN
Common utility functions and classes.
Copyright (c) 2017 Matterport, Inc.
Licensed under the MIT License (see LICENSE for details)
Written by Waleed Abdulla
"""
# !/usr/bin/env python3
# coding=utf-8
import numpy as np
import json
import scipy.misc
import skimage.color
import skimage.io
import PIL
from PIL import Image, ImageDraw
import os
import colorsys
import cv2 as cv
import pycocotools.mask as mask_util
from iobjectspy.ml.toolkit._toolkit import save_pattle_png, save_config_to_yaml
from collections import OrderedDict
import copy
import random
from matplotlib.colors import rgb_to_hsv, hsv_to_rgb
from albumentations import ChannelShuffle
from iobjectspy._jsuperpy.data._util import get_output_datasource, check_output_datasource, DatasetType, FieldType
from rasterio import transform as rio_transform
from rasterio import features
from iobjectspy import DatasetType, FieldInfo, FieldType, \
    Feature, Geometry, GeoRegion

# URL from which to download the latest COCO trained weights
# 从哪个URL下载最新的COCO训练的权重
# COCO_MODEL_URL = "https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5"


############################################################
#  Bounding Boxes 边界框
############################################################

# def extract_bboxes(mask):
#     """Compute bounding boxes from masks.
#     计算mask的边界框。
#     mask: [height, width, num_instances]. Mask pixels are either 1 or 0.
#     Returns: bbox array [num_instances, (y1, x1, y2, x2)].
#     """
#     boxes = np.zeros([mask.shape[-1], 4], dtype=np.int32)  # [num_instances, (y1, x1, y2, x2)] y对应行数 x对应列数
#     delete_inds = []
#     for i in range(mask.shape[-1]):
#         m = mask[:, :, i]  # [height, width]
#         # Bounding box.
#         horizontal_indicies = np.where(np.any(m, axis=0))[0]
#         vertical_indicies = np.where(np.any(m, axis=1))[0]
#         # np.any 测试给定轴上的任何数组元素是否评估为True。axis=0 按列评估，如果这一列都为0则返回False，否则返回True
#
#         if horizontal_indicies.shape[0]:
#             x1, x2 = horizontal_indicies[[0, -1]]
#             y1, y2 = vertical_indicies[[0, -1]]
#             # x2 and y2 should not be part of the box. Increment by 1.
#             x2 += 1
#             y2 += 1
#             boxes[i] = np.array([y1, x1, y2, x2])
#         else:
#             # No mask for this instance. Might happen due to
#             # resizing or cropping. Set bbox to zeros,不能构成矩行的框要删除
#             delete_inds.append(i)
#     return boxes.astype(np.int32), delete_inds


def compute_iou(box, boxes, box_area, boxes_area):
    """Calculates IoU of the given box with the array of the given boxes.
    # 使用给定框的数组计算给定框的IoU。
    box: 1D vector [y1, x1, y2, x2]
    boxes: [boxes_count, (y1, x1, y2, x2)]
    box_area: float. the area of 'box'  'box'的面积
    boxes_area: array of length boxes_count.  长度数组box_count
    Note: the areas are passed in rather than calculated here for
          efficency. Calculate once in the caller to avoid duplicate work.
    """
    # Calculate intersection areas
    y1 = np.maximum(box[0], boxes[:, 0])  # 数值对应元素比较取最大值 a=[[3,4],[2,5]],b=[[1,5],[3,7]] np.maximum(a,b) [[3,5],[3,7]]
    y2 = np.minimum(box[2], boxes[:, 2])  # 数值对应元素比较取最小值
    x1 = np.maximum(box[1], boxes[:, 1])
    x2 = np.minimum(box[3], boxes[:, 3])
    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)
    union = box_area + boxes_area[:] - intersection[:]
    iou = intersection / union
    return iou


# def compute_overlaps(boxes1, boxes2):
#     """Computes IoU overlaps between two sets of boxes.
#     计算两组框之间的IoU重叠。
#     boxes1, boxes2: [N, (y1, x1, y2, x2)].
#     For better performance, pass the largest set first and the smaller second.
#     为了获得更好的性能，请先传递最大集合，再传递更小的第二个。
#     """
#     # Areas of anchors and GT boxes
#     area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
#     area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
#
#     # Compute overlaps to generate matrix [boxes1 count, boxes2 count] 计算重叠以生成矩阵
#     # Each cell contains the IoU value. 每个单元格都包含IoU值。
#     overlaps = np.zeros((boxes1.shape[0], boxes2.shape[0]))
#     for i in range(overlaps.shape[1]):
#         box2 = boxes2[i]
#         overlaps[:, i] = compute_iou(box2, boxes1, area2[i], area1)
#     return overlaps


# def compute_overlaps_masks(masks1, masks2):
#     '''Computes IoU overlaps between two sets of masks.
#     计算两组掩码之间的IoU重叠。
#     masks1, masks2: [Height, Width, instances]
#     '''
#     # flatten masks
#     masks1 = np.reshape(masks1 > .5, (-1, masks1.shape[-1])).astype(np.float32)  # [Height*Width, instances]
#     masks2 = np.reshape(masks2 > .5, (-1, masks2.shape[-1])).astype(np.float32)
#     area1 = np.sum(masks1, axis=0)
#     area2 = np.sum(masks2, axis=0)
#
#     # intersections and union
#     intersections = np.dot(masks1.T, masks2)
#     union = area1[:, None] + area2[None, :] - intersections
#     overlaps = intersections / union
#
#     return overlaps


def non_max_suppression(boxes, scores, threshold):
    """Performs non-maximum supression and returns indicies of kept boxes.
    执行非最大抑制并返回保留框的索引
    boxes: [N, (y1, x1, y2, x2)]. Notice that (y2, x2) lays outside the box. 注意（y2，x2）在框外。
    scores: 1-D array of box scores.
    threshold: Float. IoU threshold to use for filtering. 用于过滤的IoU阈值
    """
    assert boxes.shape[0] > 0
    if boxes.dtype.kind != "f":  # not float
        boxes = boxes.astype(np.float32)

    # Compute box areas
    y1 = boxes[:, 0]
    x1 = boxes[:, 1]
    y2 = boxes[:, 2]
    x2 = boxes[:, 3]
    area = (y2 - y1) * (x2 - x1)

    # Get indicies of boxes sorted by scores (highest first)
    # 获取按分数排序的盒子索引（最高的第一个）
    ixs = scores.argsort()[::-1]  # 从小到大排序

    pick = []
    while len(ixs) > 0:
        # Pick top box and add its index to the list
        # 选择顶部框并将其索引添加到列表中
        i = ixs[0]
        pick.append(i)
        # Compute IoU of the picked box with the rest
        iou = compute_iou(boxes[i], boxes[ixs[1:]], area[i], area[ixs[1:]])
        # Identify boxes with IoU over the threshold. This
        # returns indicies into ixs[1:], so add 1 to get
        # indicies into ixs.
        # 识别超过阈值的IoU的盒子。 这将索引返回到ixs [1：]，所以加1以使索引变成ixs。
        remove_ixs = np.where(iou > threshold)[0] + 1
        # Remove indicies of the picked and overlapped boxes.
        # 删除挑选和重叠框的索引。
        ixs = np.delete(ixs, remove_ixs)
        ixs = np.delete(ixs, 0)
    return np.array(pick, dtype=np.int32)


# def apply_box_deltas(boxes, deltas):
#     """Applies the given deltas to the given boxes.
#     将给定的deltas应用于给定的框。
#     boxes: [N, (y1, x1, y2, x2)]. Note that (y2, x2) is outside the box.
#     deltas: [N, (dy, dx, log(dh), log(dw))]
#     """
#     boxes = boxes.astype(np.float32)
#     # Convert to y, x, h, w
#     height = boxes[:, 2] - boxes[:, 0]
#     width = boxes[:, 3] - boxes[:, 1]
#     center_y = boxes[:, 0] + 0.5 * height
#     center_x = boxes[:, 1] + 0.5 * width
#     # Apply deltas
#     center_y += deltas[:, 0] * height
#     center_x += deltas[:, 1] * width
#     height *= np.exp(deltas[:, 2])
#     width *= np.exp(deltas[:, 3])
#     # Convert back to y1, x1, y2, x2
#     y1 = center_y - 0.5 * height
#     x1 = center_x - 0.5 * width
#     y2 = y1 + height
#     x2 = x1 + width
#     return np.stack([y1, x1, y2, x2], axis=1)


# def box_refinement_graph(box, gt_box):
#     """Compute refinement needed to transform box to gt_box.
#     将框转换为gt_box所需的计算细化。
#     box and gt_box are [N, (y1, x1, y2, x2)]
#     """
#     box = tf.cast(box, tf.float32)
#     gt_box = tf.cast(gt_box, tf.float32)
#
#     height = box[:, 2] - box[:, 0]
#     width = box[:, 3] - box[:, 1]
#     center_y = box[:, 0] + 0.5 * height
#     center_x = box[:, 1] + 0.5 * width
#
#     gt_height = gt_box[:, 2] - gt_box[:, 0]
#     gt_width = gt_box[:, 3] - gt_box[:, 1]
#     gt_center_y = gt_box[:, 0] + 0.5 * gt_height
#     gt_center_x = gt_box[:, 1] + 0.5 * gt_width
#
#     dy = (gt_center_y - center_y) / height
#     dx = (gt_center_x - center_x) / width
#     dh = tf.math.log(gt_height / height)
#     dw = tf.math.log(gt_width / width)
#
#     result = tf.stack([dy, dx, dh, dw], axis=1)
#     return result


# def box_refinement(box, gt_box):
#     """Compute refinement needed to transform box to gt_box.
#     将框转换为gt_box所需的计算细化。
#     box and gt_box are [N, (y1, x1, y2, x2)]. (y2, x2) is
#     assumed to be outside the box.
#     """
#     box = box.astype(np.float32)
#     gt_box = gt_box.astype(np.float32)
#
#     height = box[:, 2] - box[:, 0]
#     width = box[:, 3] - box[:, 1]
#     center_y = box[:, 0] + 0.5 * height
#     center_x = box[:, 1] + 0.5 * width
#
#     gt_height = gt_box[:, 2] - gt_box[:, 0]
#     gt_width = gt_box[:, 3] - gt_box[:, 1]
#     gt_center_y = gt_box[:, 0] + 0.5 * gt_height
#     gt_center_x = gt_box[:, 1] + 0.5 * gt_width
#
#     dy = (gt_center_y - center_y) / height
#     dx = (gt_center_x - center_x) / width
#     dh = np.log(gt_height / height)
#     dw = np.log(gt_width / width)
#
#     return np.stack([dy, dx, dh, dw], axis=1)


############################################################
#  Object_Extract_Dataset
############################################################
# class Object_Extract_Dataset(object):
#     """The base class for dataset classes.
#     To use it, create a new class that adds functions specific to the dataset
#     you want to use. For example:
#     数据集类的基类。
#     要使用它，请创建一个新类，添加特定于数据集的函数
#     你想用。例如
#     class CatsAndDogsDataset(Dataset):
#         def load_cats_and_dogs(self):
#             ...
#         def load_mask(self, image_id):
#             ...
#         def image_reference(self, image_id):
#             ...
#     See COCODataset and ShapesDataset as examples.
#     """
#
#     def __init__(self, class_map=None):
#         self._image_ids = []
#         self.image_info = []
#         # Background is always the first class
#         self.class_info = [{"source": "", "id": 0, "name": "BG"}]
#         self.source_class_ids = {}
#
#     def add_class(self, source, class_id, class_name):
#         assert "." not in source, "Source name cannot contain a dot"  # 源名称不能包含点
#         # Does the class exist already?  class是否已经存在？
#         for info in self.class_info:
#             if info['source'] == source and info["id"] == class_id:
#                 # source.class_id combination already available, skip
#                 return
#         # Add the class
#         self.class_info.append({
#             "source": source,
#             "id": class_id,
#             "name": class_name,
#         })
#
#     def add_image(self, source, image_id, path, **kwargs):
#         image_info = {
#             "id": image_id,
#             "source": source,
#             "path": path,
#         }
#         image_info.update(kwargs)
#         self.image_info.append(image_info)
#
#     def image_reference(self, image_id):
#         """Return a link to the image in its source Website or details about
#         the image that help looking it up or debugging it.
#         Override for your dataset, but pass to this function
#         if you encounter images not in your dataset.
#         返回源网站中图片的链接或有关详情
#          帮助查看或调试它的图像。
#          覆盖您的数据集，但传递给此函数
#          如果您遇到不在数据集中的图像。
#         """
#         return ""
#
#     def prepare(self, class_map=None):
#         """Prepares the Dataset class for use.
#         TODO: class map is not supported yet. When done, it should handle mapping
#               classes from different datasets to the same class ID.
#
#         准备Dataset类以供使用。
#          TODO：class map尚未得到支持。 完成后，它应该处理映射
#                从不同数据集到同一个类ID的类。
#         """
#
#         def clean_name(name):
#             """Returns a shorter version of object names for cleaner display.
#             返回更简洁的对象名称，以便更清晰地显示"""
#             return ",".join(name.split(",")[:1])
#
#         # Build (or rebuild) everything else from the info dicts.
#         # 从info dicts构建（或重建）其他的一切
#         self.num_classes = len(self.class_info)
#         self.class_ids = np.arange(self.num_classes)
#         self.class_names = [clean_name(c["name"]) for c in self.class_info]
#         self.num_images = len(self.image_info)
#         self._image_ids = np.arange(self.num_images)
#         ##source.id:id
#         self.class_from_source_map = {"{}.{}".format(info['source'], info['id']): id
#                                       for info, id in zip(self.class_info, self.class_ids)}
#
#         # Map sources to class_ids they support 将资源映射到它们支持的class_ids
#         self.sources = list(set([i['source'] for i in self.class_info]))
#         self.source_class_ids = {}
#         # Loop over datasets
#         for source in self.sources:
#             self.source_class_ids[source] = []
#             # Find classes that belong to this dataset 查找属于此数据集的类
#             for i, info in enumerate(self.class_info):
#                 # Include BG class in all datasets
#                 if i == 0 or source == info['source']:
#                     self.source_class_ids[source].append(i)
#
#     def map_source_class_id(self, source_class_id):
#         """Takes a source class ID and returns the int class ID assigned to it.
#         For example:
#         dataset.map_source_class_id("coco.12") -> 23
#         """
#         return self.class_from_source_map[source_class_id]
#
#     def get_source_class_id(self, class_id, source):
#         """Map an internal class ID to the corresponding class ID in the source dataset."""
#         info = self.class_info[class_id]
#         assert info['source'] == source
#         return info['id']
#
#     def append_data(self, class_info, image_info):
#         self.external_to_class_id = {}
#         for i, c in enumerate(self.class_info):
#             for ds, id in c["map"]:
#                 self.external_to_class_id[ds + str(id)] = i
#
#         # Map external image IDs to internal ones.
#         self.external_to_image_id = {}
#         for i, info in enumerate(self.image_info):
#             self.external_to_image_id[info["ds"] + str(info["id"])] = i
#
#     @property
#     def image_ids(self):
#         return self._image_ids
#
#     def source_image_link(self, image_id):
#         """Returns the path or URL to the image.
#         Override this to return a URL to the image if it's availble online for easy
#         debugging.
#         """
#         return self.image_info[image_id]["path"]
#
#     def load_image(self, image_id):
#         """Load the specified image and return a [H,W,3] Numpy array.
#         """
#         # Load image
#         image = skimage.io.imread(self.image_info[image_id]['path'])
#         # If grayscale. Convert to RGB for consistency.
#         if image.ndim != 3:
#             image = skimage.color.gray2rgb(image)
#         return image
#
#     def load_mask(self, image_id):
#         """Load instance masks for the given image.
#         Different datasets use different ways to store masks. Override this
#         method to load instance masks and return them in the form of am
#         array of binary masks of shape [height, width, instances].
#         Returns:
#             masks: A bool array of shape [height, width, instance count] with
#                 a binary mask per instance.
#             class_ids: a 1D array of class IDs of the instance masks.
#         """
#         # Override this function to load a mask from your dataset.
#         # Otherwise, it returns an empty mask.
#         mask = np.empty([0, 0, 0])
#         class_ids = np.empty([0], np.int32)
#         return mask, class_ids


# def resize_image(image, min_dim=None, max_dim=None, padding=False):
#     """
#     Resizes an image keeping the aspect ratio.
#     min_dim: if provided, resizes the image such that it's smaller
#         dimension == min_dim
#     max_dim: if provided, ensures that the image longest side doesn't
#         exceed this value.
#     padding: If true, pads image with zeros so it's size is max_dim x max_dim
#     Returns:
#     image: the resized image
#     window: (y1, x1, y2, x2). If max_dim is provided, padding might
#         be inserted in the returned image. If so, this window is the
#         coordinates of the image part of the full image (excluding
#         the padding). The x2, y2 pixels are not included.
#     scale: The scale factor used to resize the image
#     padding: Padding added to the image [(top, bottom), (left, right), (0, 0)]
#     """
#     # Default window (y1, x1, y2, x2) and default scale == 1.
#     h, w = image.shape[:2]
#     window = (0, 0, h, w)
#     scale = 1
#
#     # Scale?
#     if min_dim:
#         # Scale up but not down
#         scale = max(1, min_dim / min(h, w))
#     # Does it exceed max dim?
#     if max_dim:
#         image_max = max(h, w)
#         if round(image_max * scale) > max_dim:
#             scale = max_dim / image_max
#     # Resize image and mask
#     if scale != 1:
#         # 这个方法专门reize图片,如果需要resize并且将像素值映射到256上,需要*255
#         image = np.array(Image.fromarray(image).resize(size=(round(h * scale), round(w * scale))))
#         # image = scipy.misc.imresize(
#         #     image, (round(h * scale), round(w * scale)))
#     # Need padding?
#     if padding:
#         # Get new height and width
#         h, w = image.shape[:2]
#         top_pad = (max_dim - h) // 2
#         bottom_pad = max_dim - h - top_pad
#         left_pad = (max_dim - w) // 2
#         right_pad = max_dim - w - left_pad
#         padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]
#         image = np.pad(image, padding, mode='constant', constant_values=0)
#         window = (top_pad, left_pad, h + top_pad, w + left_pad)
#     return image, window, scale, padding


# def resize_mask(mask, scale, padding):
#     """Resizes a mask using the given scale and padding.
#     Typically, you get the scale and padding from resize_image() to
#     ensure both, the image and the mask, are resized consistently.
#     scale: mask scaling factor
#     padding: Padding to add to the mask in the form
#             [(top, bottom), (left, right), (0, 0)]
#     """
#     h, w = mask.shape[:2]
#     mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)
#     mask = np.pad(mask, padding, mode='constant', constant_values=0)
#     return mask
#
#
# def minimize_mask(bbox, mask, mini_shape):
#     """Resize masks to a smaller version to cut memory load.
#     Mini-masks can then resized back to image scale using expand_masks()
#     See inspect_data.ipynb notebook for more details.
#     """
#     mini_mask = np.zeros(mini_shape + (mask.shape[-1],), dtype=bool)
#     for i in range(mask.shape[-1]):
#         m = mask[:, :, i]
#         y1, x1, y2, x2 = bbox[i][:4]
#         m = m[y1:y2, x1:x2]
#         if m.size == 0:
#             raise Exception("Invalid bounding box with area of zero")
#         # 现在的方法就是只起到resize的作用,astype只是将数据输入的类型改变,并不改变原来数据类型
#         m = np.array(Image.fromarray(m.astype(float)).resize(size=mini_shape, resample=PIL.Image.BILINEAR)) * 255
#         # 原来的方法会根据极值之差归一化,最大最小都是1,会全归一化为0
#         # m = scipy.misc.imresize(m.astype(float), mini_shape, interp='bilinear')
#         mini_mask[:, :, i] = np.where(m >= 128, 1, 0)
#     return mini_mask
#
#
# def expand_mask(bbox, mini_mask, image_shape):
#     """Resizes mini masks back to image size. Reverses the change
#     of minimize_mask().
#     See inspect_data.ipynb notebook for more details.
#     """
#     mask = np.zeros(image_shape[:2] + (mini_mask.shape[-1],), dtype=bool)
#     for i in range(mask.shape[-1]):
#         m = mini_mask[:, :, i]
#         y1, x1, y2, x2 = bbox[i][:4]
#         h = y2 - y1
#         w = x2 - x1
#         m = scipy.misc.imresize(m.astype(float), (h, w), interp='bilinear')
#         mask[y1:y2, x1:x2, i] = np.where(m >= 128, 1, 0)
#     return mask
#
#
# def mold_mask(mask, config):
#     pass
#
#
# def unmold_mask(mask, bbox, image_shape):
#     """Converts a mask generated by the neural network into a format similar
#     to it's original shape.
#     mask: [height, width] of type float. A small, typically 28x28 mask.
#     bbox: [y1, x1, y2, x2]. The box to fit the mask in.
#     Returns a binary mask with the same size as the original image.
#     """
#     threshold = 0.5
#     y1, x1, y2, x2 = bbox
#     mask = scipy.misc.imresize(
#         mask, (y2 - y1, x2 - x1), interp='bilinear').astype(np.float32) / 255.0
#     mask = np.where(mask >= threshold, 1, 0).astype(np.uint8)
#
#     # Put the mask in the right location.
#     full_mask = np.zeros(image_shape[:2], dtype=np.uint8)
#     full_mask[y1:y2, x1:x2] = mask
#     return full_mask
#
#
# ############################################################
# #  Anchors
# ############################################################
#
# def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):
#     """
#     scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]
#     ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]
#     shape: [height, width] spatial shape of the feature map over which
#             to generate anchors.
#     feature_stride: Stride of the feature map relative to the image in pixels.
#     anchor_stride: Stride of anchors on the feature map. For example, if the
#         value is 2 then generate anchors for every other feature map pixel.
#     """
#     # Get all combinations of scales and ratios
#     scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))
#     scales = scales.flatten()
#     ratios = ratios.flatten()
#
#     # Enumerate heights and widths from scales and ratios
#     heights = scales / np.sqrt(ratios)
#     widths = scales * np.sqrt(ratios)
#
#     # Enumerate shifts in feature space
#     shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride
#     shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride
#     shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)
#
#     # Enumerate combinations of shifts, widths, and heights
#     box_widths, box_centers_x = np.meshgrid(widths, shifts_x)
#     box_heights, box_centers_y = np.meshgrid(heights, shifts_y)
#
#     # Reshape to get a list of (y, x) and a list of (h, w)
#     box_centers = np.stack(
#         [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])
#     box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])
#
#     # Convert to corner coordinates (y1, x1, y2, x2)
#     boxes = np.concatenate([box_centers - 0.5 * box_sizes,
#                             box_centers + 0.5 * box_sizes], axis=1)
#     return boxes
#
#
# def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,
#                              anchor_stride):
#     """Generate anchors at different levels of a feature pyramid. Each scale
#     is associated with a level of the pyramid, but each ratio is used in
#     all levels of the pyramid.
#     Returns:
#     anchors: [N, (y1, x1, y2, x2)]. All generated anchors in one array. Sorted
#         with the same order of the given scales. So, anchors of scale[0] come
#         first, then anchors of scale[1], and so on.
#     """
#     # Anchors
#     # [anchor_count, (y1, x1, y2, x2)]
#     anchors = []
#     for i in range(len(scales)):
#         anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],
#                                         feature_strides[i], anchor_stride))
#     return np.concatenate(anchors, axis=0)
#
#
# ############################################################
# #  Miscellaneous
# ############################################################
#
# def trim_zeros(x):
#     """It's common to have tensors larger than the available data and
#     pad with zeros. This function removes rows that are all zeros.
#     x: [rows, columns].
#     """
#     assert len(x.shape) == 2
#     return x[~np.all(x == 0, axis=1)]
#
#
# def compute_matches(gt_boxes, gt_class_ids, gt_masks,
#                     pred_boxes, pred_class_ids, pred_scores, pred_masks,
#                     iou_threshold=0.5, score_threshold=0.0):
#     """Finds matches between prediction and ground truth instances.
#
#     Returns:
#         gt_match: 1-D array. For each GT box it has the index of the matched
#                   predicted box.
#         pred_match: 1-D array. For each predicted box, it has the index of
#                     the matched ground truth box.
#         overlaps: [pred_boxes, gt_boxes] IoU overlaps.
#     """
#     # Trim zero padding
#     # TODO: cleaner to do zero unpadding upstream
#     gt_boxes = trim_zeros(gt_boxes)
#     gt_masks = gt_masks[..., :gt_boxes.shape[0]]
#     pred_boxes = trim_zeros(pred_boxes)
#     pred_scores = pred_scores[:pred_boxes.shape[0]]
#     # Sort predictions by score from high to low
#     indices = np.argsort(pred_scores)[::-1]
#     pred_boxes = pred_boxes[indices]
#     pred_class_ids = pred_class_ids[indices]
#     pred_scores = pred_scores[indices]
#     pred_masks = pred_masks[..., indices]
#
#     # Compute IoU overlaps [pred_masks, gt_masks]
#     overlaps = compute_overlaps_masks(pred_masks, gt_masks)
#
#     # Loop through predictions and find matching ground truth boxes
#     match_count = 0
#     pred_match = -1 * np.ones([pred_boxes.shape[0]])
#     gt_match = -1 * np.ones([gt_boxes.shape[0]])
#     for i in range(len(pred_boxes)):
#         # Find best matching ground truth box
#         # 1. Sort matches by score
#         sorted_ixs = np.argsort(overlaps[i])[::-1]
#         # 2. Remove low scores
#         low_score_idx = np.where(overlaps[i, sorted_ixs] < score_threshold)[0]
#         if low_score_idx.size > 0:
#             sorted_ixs = sorted_ixs[:low_score_idx[0]]
#         # 3. Find the match
#         for j in sorted_ixs:
#             # If ground truth box is already matched, go to next one
#             if gt_match[j] > -1:
#                 continue
#             # If we reach IoU smaller than the threshold, end the loop
#             iou = overlaps[i, j]
#             if iou < iou_threshold:
#                 break
#             # Do we have a match?
#             if pred_class_ids[i] == gt_class_ids[j]:
#                 match_count += 1
#                 gt_match[j] = i
#                 pred_match[i] = j
#                 break
#
#     return gt_match, pred_match, overlaps
#
#
# def compute_ap1(gt_boxes, gt_class_ids, gt_masks,
#                 pred_boxes, pred_class_ids, pred_scores, pred_masks,
#                 iou_threshold=0.5):
#     """Compute Average Precision at a set IoU threshold (default 0.5).
#     Returns:
#     mAP: Mean Average Precision
#     precisions: List of precisions at different class score thresholds.
#     recalls: List of recall values at different class score thresholds.
#     overlaps: [pred_boxes, gt_boxes] IoU overlaps.
#     """
#     # Trim zero padding and sort predictions by score from high to low
#     # TODO: cleaner to do zero unpadding upstream
#     gt_boxes = trim_zeros(gt_boxes)
#     gt_masks = gt_masks[..., :gt_boxes.shape[0]]
#     pred_boxes = trim_zeros(pred_boxes)
#     pred_scores = pred_scores[:pred_boxes.shape[0]]
#     indices = np.argsort(pred_scores)[::-1]
#     pred_boxes = pred_boxes[indices]
#     pred_class_ids = pred_class_ids[indices]
#     pred_scores = pred_scores[indices]
#     pred_masks = pred_masks[..., indices]
#
#     # Compute IoU overlaps [pred_masks, gt_masks]
#     overlaps = compute_overlaps_masks(pred_masks, gt_masks)
#
#     # Loop through ground truth boxes and find matching predictions
#     match_count = 0
#     pred_match = np.zeros([pred_boxes.shape[0]])
#     gt_match = np.zeros([gt_boxes.shape[0]])
#     for i in range(len(pred_boxes)):
#         # Find best matching ground truth box
#         sorted_ixs = np.argsort(overlaps[i])[::-1]
#         for j in sorted_ixs:
#             # If ground truth box is already matched, go to next one
#             if gt_match[j] == 1:
#                 continue
#             # If we reach IoU smaller than the threshold, end the loop
#             iou = overlaps[i, j]
#             if iou < iou_threshold:
#                 break
#             # Do we have a match?
#             if pred_class_ids[i] == gt_class_ids[j]:
#                 match_count += 1
#                 gt_match[j] = 1
#                 pred_match[i] = 1
#                 break
#
#     # Compute precision and recall at each prediction box step
#     precisions = np.cumsum(pred_match) / (np.arange(len(pred_match)) + 1)
#     recalls = np.cumsum(pred_match).astype(np.float32) / len(gt_match)
#
#     # Pad with start and end values to simplify the math
#     precisions = np.concatenate([[0], precisions, [0]])
#     recalls = np.concatenate([[0], recalls, [1]])
#
#     # Ensure precision values decrease but don't increase. This way, the
#     # precision value at each recall threshold is the maximum it can be
#     # for all following recall thresholds, as specified by the VOC paper.
#     for i in range(len(precisions) - 2, -1, -1):
#         precisions[i] = np.maximum(precisions[i], precisions[i + 1])
#
#     # Compute mean AP over recall range
#     indices = np.where(recalls[:-1] != recalls[1:])[0] + 1
#     mAP = np.sum((recalls[indices] - recalls[indices - 1]) *
#                  precisions[indices])
#
#     return mAP, precisions, recalls, overlaps
#
#
# def compute_ap(gt_boxes, gt_class_ids, gt_masks,
#                pred_boxes, pred_class_ids, pred_scores, pred_masks,
#                iou_threshold=0.5):
#     """Compute Average Precision at a set IoU threshold (default 0.5).
#
#     Returns:
#     mAP: Mean Average Precision
#     precisions: List of precisions at different class score thresholds.
#     recalls: List of recall values at different class score thresholds.
#     overlaps: [pred_boxes, gt_boxes] IoU overlaps.
#     """
#     # Get matches and overlaps
#     gt_match, pred_match, overlaps = compute_matches(
#         gt_boxes, gt_class_ids, gt_masks,
#         pred_boxes, pred_class_ids, pred_scores, pred_masks,
#         iou_threshold)
#
#     # Compute precision and recall at each prediction box step
#     precisions = np.cumsum(pred_match > -1) / (np.arange(len(pred_match)) + 1)
#     recalls = np.cumsum(pred_match > -1).astype(np.float32) / len(gt_match)
#
#     # Pad with start and end values to simplify the math
#     precisions = np.concatenate([[0], precisions, [0]])
#     recalls = np.concatenate([[0], recalls, [1]])
#
#     # Ensure precision values decrease but don't increase. This way, the
#     # precision value at each recall threshold is the maximum it can be
#     # for all following recall thresholds, as specified by the VOC paper.
#     for i in range(len(precisions) - 2, -1, -1):
#         precisions[i] = np.maximum(precisions[i], precisions[i + 1])
#
#     # Compute mean AP over recall range
#     indices = np.where(recalls[:-1] != recalls[1:])[0] + 1
#     mAP = np.sum((recalls[indices] - recalls[indices - 1]) *
#                  precisions[indices])
#
#     return mAP, precisions, recalls, overlaps
#
#
# def compute_recall(pred_boxes, gt_boxes, iou):
#     """Compute the recall at the given IoU threshold. It's an indication
#     of how many GT boxes were found by the given prediction boxes.
#     pred_boxes: [N, (y1, x1, y2, x2)] in image coordinates
#     gt_boxes: [N, (y1, x1, y2, x2)] in image coordinates
#     """
#     # Measure overlaps
#     overlaps = compute_overlaps(pred_boxes, gt_boxes)
#     iou_max = np.max(overlaps, axis=1)
#     iou_argmax = np.argmax(overlaps, axis=1)
#     positive_ids = np.where(iou_max >= iou)[0]
#     matched_gt_boxes = iou_argmax[positive_ids]
#
#     recall = len(set(matched_gt_boxes)) / gt_boxes.shape[0]
#     return recall, positive_ids
#
#
# # ## Batch Slicing
# # Some custom layers support a batch size of 1 only, and require a lot of work
# # to support batches greater than 1. This function slices an input tensor
# # across the batch dimension and feeds batches of size 1. Effectively,
# # an easy way to support batches > 1 quickly with little code modification.
# # In the long run, it's more efficient to modify the code to support large
# # batches and getting rid of this function. Consider this a temporary solution
# def batch_slice(inputs, graph_fn, batch_size, names=None):
#     """Splits inputs into slices and feeds each slice to a copy of the given
#     computation graph and then combines the results. It allows you to run a
#     graph on a batch of inputs even if the graph is written to support one
#     instance only.
#     inputs: list of tensors. All must have the same first dimension length
#     graph_fn: A function that returns a TF tensor that's part of a graph.
#     batch_size: number of slices to divide the data into.
#     names: If provided, assigns names to the resulting tensors.
#     """
#     if not isinstance(inputs, list):
#         inputs = [inputs]
#
#     outputs = []
#     for i in range(batch_size):
#         inputs_slice = [x[i] for x in inputs]
#         output_slice = graph_fn(*inputs_slice)
#         if not isinstance(output_slice, (tuple, list)):
#             output_slice = [output_slice]
#         outputs.append(output_slice)
#     # Change outputs from a list of slices where each is
#     # a list of outputs to a list of outputs and each has
#     # a list of slices
#     outputs = list(zip(*outputs))
#
#     if names is None:
#         names = [None] * len(outputs)
#
#     result = [tf.stack(o, axis=0, name=n)
#               for o, n in zip(outputs, names)]
#     if len(result) == 1:
#         result = result[0]
#
#     return result
#
#
# # 保存Annotations的xml文件
# def _save_xml(output_path_label, lists, width, height, pic_name, depth, class_names):
#     """
#         传入lists包含bbox，category，difficult信息，将其转换为xml格式
#
#         :param output_path_label: 输入标签文件存储路径
#         :type output_path_label: str
#         :param lists: 包含bbox，category，difficult信息
#         :type lists: list
#         :param width: 图像宽度
#         :type width: Long
#         :param height: 图像高度
#         :type height: Long
#         :param pic_name: 对应标签文件的图片名称
#         :type pic_name: str
#         """
#     # tile_format: 切片的图像格式:TIFF,PNG,JPG,从推理图像的名称中获得
#     tile_format = pic_name.split(".")[-1],
#     if tile_format == 'jpg' or tile_format == 'png':
#         depth = 3
#     from lxml.etree import Element, SubElement, tostring
#
#     node_root = Element('annotation')
#
#     node_folder = SubElement(node_root, 'folder')
#     node_folder.text = 'VOC'
#
#     node_filename = SubElement(node_root, 'filename')
#     node_filename.text = pic_name
#
#     node_size = SubElement(node_root, 'size')
#     node_width = SubElement(node_size, 'width')
#     node_width.text = '%s' % width
#
#     node_height = SubElement(node_size, 'height')
#     node_height.text = '%s' % height
#
#     node_depth = SubElement(node_size, 'depth')
#     node_depth.text = '%s' % depth
#
#     # segmented
#     node_segmented = SubElement(node_root, 'segmented')
#     node_segmented.text = '%s' % 0
#
#     for list in lists:
#         node_object = SubElement(node_root, 'object')
#         node_name = SubElement(node_object, 'name')
#         node_name.text = class_names[int(list[4])]
#         node_difficult = SubElement(node_object, 'difficult')
#         node_difficult.text = str(list[5])
#         node_bndbox = SubElement(node_object, 'bndbox')
#         node_xmin = SubElement(node_bndbox, 'xmin')
#         node_xmin.text = '%s' % list[1]
#         node_ymin = SubElement(node_bndbox, 'ymin')
#         node_ymin.text = '%s' % list[0]
#         node_xmax = SubElement(node_bndbox, 'xmax')
#         node_xmax.text = '%s' % list[3]
#         node_ymax = SubElement(node_bndbox, 'ymax')
#         node_ymax.text = '%s' % list[2]
#
#     del lists[:]
#     xml = tostring(node_root, pretty_print=True)
#
#     save_xml = os.path.join(output_path_label, pic_name + '.xml')
#
#     # # 如果xml文件已经存在，那么在命名后加_1
#     # while True:
#     #     if os.path.exists(save_xml):
#     #         base_xml = os.path.basename(save_xml)
#     #         base_xml_new = base_xml.split(".")[0] + "_1" + ".xml"
#     #         save_xml = save_xml.replace(base_xml, base_xml_new)
#     #     else:
#     #         break
#     with open(save_xml, 'wb') as f:
#         f.write(xml)
#
#
# def _get_continuous_codes_list(num_color=256):
#     """
#     动态获取颜色表列表
#
#     :param int num_color: 颜色表数量
#     :return: color_continuous_codes_list 颜色表列表
#     :type: list[tuple]
#
#     """
#
#     r, g, b = [v / 255 for v in get_rgb("#ffffff")]
#     h, s, v = colorsys.rgb_to_hsv(r, g, b)
#
#     color_continuous_codes_list = []
#
#     for i in range(num_color):
#         ns = (1 / num_color) * (i + 1)
#         color_continuous_codes_list.append(tuple([int(v * 255) for v in colorsys.hsv_to_rgb(h, ns, v)]))
#
#     return list(set(color_continuous_codes_list))
#
#
# def get_rgb(v):
#     """
#     获取RGB颜色
#    :param v: 十六进制颜色码
#    :return: RGB颜色值
#        """
#     r, g, b = v[1:3], v[3:5], v[5:7]
#     return int(r, 16), int(g, 16), int(b, 16)
#
#
# def _save_segobject(image, output_segobject, num_color_id):
#     """
#     保存按照object分割的mask标签
#     """
#
#     # 构建目标对象颜色表
#     color_continuous_codes_list = _get_continuous_codes_list(num_color_id)
#     color_codes_segobject = {}
#     color_codes_segobject[(0, 0, 0)] = 0
#     for color_codes_id in range(num_color_id):
#         color_codes_segobject[color_continuous_codes_list[color_codes_id]] = color_codes_id
#
#     image = image.astype(np.uint8)
#     save_pattle_png(image, color_codes_segobject, output_segobject)
#
#
# def show_two_image(image1, image2):
#     # 同时可视化两个RGB或者mask
#     from matplotlib import pyplot as plt
#     fig = plt.figure(figsize=(10, 10))
#     ax1 = plt.subplot(1, 2, 1)
#     ax2 = plt.subplot(1, 2, 2)
#     plt.sca(ax1)
#     plt.imshow(image1)
#     plt.sca(ax2)
#     plt.imshow(image2)
#     plt.show()
#
#
# def create_sda_voc_mask(categorys, image_count, tile_size_x, tile_size_y, image_mean, tile_format, output_path):
#     # 创建VOC数据描述配置文件
#     dic_voc_yml = OrderedDict({
#         'dataset': OrderedDict({"name": "example_voc",
#                                 'classes': categorys,
#                                 'image_count': image_count,
#                                 "data_type": "voc_mask",
#                                 "tile_size_x": tile_size_x,
#                                 "tile_size_y": tile_size_y,
#                                 "image_mean": image_mean,
#                                 "suffix": tile_format})
#
#     })
#     save_config_to_yaml(dic_voc_yml, output_path)
#
#
# def rand(a=0, b=1):
#     # 总区间的比例再加上区间下限，生成随机数
#     return np.random.rand() * (b - a) + a
#
#
# def merge_bboxes(bboxes, cutx, cuty, h, w, save_proportion):
#     """
#     对于合成到新图像中的bbox，单个部分超出cutx,y以及新图片宽高的进行删除或者修改，保留不低于原来面积指定比例的bbox
#     """
#     merge_bbox = []
#     for i in range(len(bboxes)):
#         for box in bboxes[i]:
#             tmp_box = []
#             x1, y1, x2, y2 = box[0], box[1], box[2], box[3]
#             raw_x1, raw_y1, raw_x2, raw_y2 = box[0], box[1], box[2], box[3]
#             clipped = False
#
#             if i == 0:
#                 # 合成后的图被cutx,cuty分成四个部分，单个图超出界限的将被覆盖，这里y1,x1就是判断是否超出该部分界限，以下i==之后的第一个判断都是这个意图
#                 if y1 > cuty or x1 > cutx:
#                     continue
#                 # ***处理x和y方向上，跨越界限的y2或者x2,如果这个bbox这样处理之后在x,y任一方向上小于5个像素，那就舍弃改bbox,而我也是在这里判断保留比例
#                 if y2 >= cuty and y1 <= cuty:
#                     clipped = True
#                     y2 = cuty
#
#                 if x2 >= cutx and x1 <= cutx:
#                     clipped = True
#                     x2 = cutx
#
#                 if clipped:
#                     cut_area = (x2 - x1) * (y2 - y1)
#                     raw_area = (raw_x2 - raw_x1) * (raw_y2 - raw_y1)
#                     if not (cut_area / raw_area >= save_proportion):
#                         continue
#
#             elif i == 1:
#                 if y2 < cuty or x1 > cutx or y1 > h:
#                     continue
#
#                 if y2 >= cuty and y1 <= cuty:
#                     clipped = True
#                     y1 = cuty
#
#                 if x2 >= cutx and x1 <= cutx:
#                     clipped = True
#                     x2 = cutx
#
#                 if clipped:
#                     cut_area = (x2 - x1) * (y2 - y1)
#                     raw_area = (raw_x2 - raw_x1) * (raw_y2 - raw_y1)
#                     if not (cut_area / raw_area >= save_proportion):
#                         continue
#
#             elif i == 2:
#                 if y2 < cuty or x2 < cutx or x1 > w or y1 > h:
#                     continue
#
#                 if y2 >= cuty and y1 <= cuty:
#                     clipped = True
#                     y1 = cuty
#
#                 if x2 >= cutx and x1 <= cutx:
#                     clipped = True
#                     x1 = cutx
#
#                 if clipped:
#                     cut_area = (x2 - x1) * (y2 - y1)
#                     raw_area = (raw_x2 - raw_x1) * (raw_y2 - raw_y1)
#                     if not (cut_area / raw_area >= save_proportion):
#                         continue
#
#             elif i == 3:
#                 if y1 > cuty or x2 < cutx or x1 > w:
#                     continue
#
#                 if y2 >= cuty and y1 <= cuty:
#                     clipped = True
#                     y2 = cuty
#
#                 if x2 >= cutx and x1 <= cutx:
#                     clipped = True
#                     x1 = cutx
#
#                 if clipped:
#                     cut_area = (x2 - x1) * (y2 - y1)
#                     raw_area = (raw_x2 - raw_x1) * (raw_y2 - raw_y1)
#                     if not (cut_area / raw_area >= save_proportion):
#                         continue
#
#             tmp_box.append(x1)
#             tmp_box.append(y1)
#             tmp_box.append(x2)
#             tmp_box.append(y2)
#             tmp_box.append(box[-1])  # bbox有5列,最后一列是类别编号
#             merge_bbox.append(tmp_box)
#     return merge_bbox
#
#
# def analyze_xml(file_name, shape=None):
#     '''
#     从xml文件中解析class，对象位置
#     :param file_name: xml文件位置
#     :return: class，每个类别的矩形位置
#     '''
#     fp = open(file_name)
#     class_name = []
#     rectangle_position = []
#     for p in fp:
#         if '<object>' in p:
#             # print(next(fp))
#             class_name.append(next(fp).split('>')[1].split('<')[0])
#
#         if '<bndbox>' in p:
#             rectangle = []
#             [rectangle.append(round(eval(next(fp).split('>')[1].split('<')[0]))) for _ in range(4)]
#             rectangle_position.append(rectangle)
#     fp.close()
#     if shape:
#         h, w = shape[0], shape[1]
#         if len(rectangle_position) > 0:
#             _class_name = []
#             rectangle_position = np.array(rectangle_position)
#             # [xmin:xmax,ymin:ymax]没取到xmax,ymax
#             rectangle_position[:, 2] = rectangle_position[:, 2] + 1
#             rectangle_position[:, 3] = rectangle_position[:, 3] + 1
#
#             rectangle_position[:, 2][rectangle_position[:, 2] > w - 1] = w - 1
#             rectangle_position[:, 3][rectangle_position[:, 3] > h - 1] = h - 1
#
#             rectangle_position[:, 0:3][rectangle_position[:, 0:3] < 0] = 0
#
#             rectangle_position_w = rectangle_position[:, 2] - rectangle_position[:, 0]
#             rectangle_position_h = rectangle_position[:, 3] - rectangle_position[:, 1]
#             # 需要rectangle_position的宽高都大于1的rectangle_position才能进入下一步，and表示为与操作，两者都正确则结果正确
#             right_index = np.logical_and(rectangle_position_w > 1, rectangle_position_h > 1)
#             rectangle_position = (rectangle_position[right_index]).tolist()
#
#             for id in range(len(right_index)):
#                 if right_index[id]:
#                     _class_name.append(class_name[id])
#             class_name = _class_name
#     "没有标签"
#     if len(rectangle_position) > 0:
#         return class_name, rectangle_position
#     else:
#         return [], []
#
#
# def get_mosaic_data(images_dir, xmls_dir, files, class_dict, input_shape, save_proportion=0.9,
#                     hue=.1, sat=1.5, val=1.5, proc_img=True, color_p=(3, 6), color_aug=False):
#     '''random preprocessing for real-time data augmentation'''
#     h, w = input_shape
#     # bask_num=class_dict["basketball_court"]
#     # trace_num=class_dict["Ground_Track_Field"]
#     min_offset_x = 0.4
#     min_offset_y = 0.4
#     scale_low = 1 - min(min_offset_x, min_offset_y)
#     scale_high = scale_low + 0.2
#
#     image_datas = []
#     mask_datas = []
#     bbox_datas = []
#     index = 0
#     # place_x,y组合起来就是这张图复制到新图上，所在的左上角坐标
#     place_x = [0, 0, int(w * min_offset_x), int(w * min_offset_x)]
#     place_y = [0, int(h * min_offset_y), int(w * min_offset_y), 0]
#     img_suffix = os.listdir(images_dir)
#     img_suffix = img_suffix[-1].split(".")[-1]
#     for mask_path in files:
#         # 每一行进行分割，一行对应一个图片的名称和图片里的bboxes
#         mask_file = os.path.basename(mask_path)
#         img_file = mask_file.replace("png", img_suffix)
#         img_path = os.path.join(images_dir, img_file)
#
#         xml_file = mask_file.replace("png", "xml")
#         xml_path = os.path.join(xmls_dir, xml_file)
#         # 打开图片
#         image = Image.open(img_path)
#         image = image.convert("RGB")
#         # 打开图片的大小
#         iw, ih = image.size
#         mask_raw = cv.imdecode(np.fromfile(mask_path, dtype=np.uint8), cv.IMREAD_COLOR)
#
#         # 解析出的bbox与要求的一致
#         class_name, bbox = analyze_xml(xml_path)
#         if len(bbox) < 1:
#             "没有标签,那后续的操作无法进行"
#             return [], [], []
#         class_id = np.array([class_dict[x] for x in class_name])
#         bbox = np.array(bbox)
#         bbox = np.insert(bbox, 4, class_id, axis=1)
#
#         # image.save(str(index)+".jpg")
#         # 是否翻转图片
#         flip = rand() < .5
#         if flip and len(bbox) > 0:
#             image = image.transpose(Image.FLIP_LEFT_RIGHT)
#             cv.flip(mask_raw, flipCode=1, dst=mask_raw)
#             bbox[:, [0, 2]] = iw - bbox[:, [2, 0]]
#
#         # 对输入进来的图片进行缩放，缩放的比例跟合并后的图像宽高相关，与图片本身的宽高无关
#         new_ar = w / h
#         scale = rand(scale_low, scale_high)
#         # 对长度更多的那一边先缩放，在利用宽高比例来得到短边
#         if new_ar < 1:
#             nh = int(scale * h)
#             # 保持宽高比例，用缩放后的高求宽
#             nw = int(nh * new_ar)
#         else:
#             nw = int(scale * w)
#             nh = int(nw / new_ar)
#         # 图片resize到指定尺寸
#         image = image.resize((nw, nh), Image.BICUBIC)
#         mask = cv.resize(mask_raw, dsize=(nh, nw), interpolation=cv.INTER_NEAREST)
#
#         # 进行色域变换，用现成的
#         hue = rand(-hue, hue)
#         sat = rand(1, sat) if rand() < .5 else 1 / rand(1, sat)
#         val = rand(1, val) if rand() < .5 else 1 / rand(1, val)
#         x = rgb_to_hsv(np.array(image) / 255.)
#         x[..., 0] += hue
#         x[..., 0][x[..., 0] > 1] -= 1
#         x[..., 0][x[..., 0] < 0] += 1
#         x[..., 1] *= sat
#         x[..., 2] *= val
#         x[x > 1] = 1
#         x[x < 0] = 0
#         image = hsv_to_rgb(x)
#
#         image = Image.fromarray((image * 255).astype(np.uint8))
#         # **image.show()
#         # 将图片进行放置，分别对应四张分割图片的位置
#         dx = place_x[index]
#         dy = place_y[index]
#         # w h 是设定的合成后图片的宽高，128设定的是color
#         new_image = Image.new('RGB', (w, h), (128, 128, 128))
#         new_mask = Image.new('RGB', (w, h), (128, 128, 128))
#         # dx, dy是左上角的坐标点,paste会有损失吗？有的，只指定了左上角，溢出的就浪费了
#         new_image.paste(image, (dx, dy))
#         mask = Image.fromarray(mask)
#         new_mask.paste(mask, (dx, dy))
#         image_data = np.array(new_image)
#
#         if color_aug:
#             "在拼图中随机使用颜色增强"
#             apply_p = random.randint(color_p[0], color_p[1]) / 10
#             if random.randint(0, 10) / 10 > apply_p:
#                 color_change = random.randint(0, 1)
#                 if color_change == 0:
#                     image_data = ChannelShuffle(p=1)(image=image_data)["image"]
#                 elif color_change == 1:
#                     image_gray = cv.cvtColor(image_data, cv.COLOR_RGB2GRAY)
#                     image_data = cv.cvtColor(image_gray, cv.COLOR_GRAY2RGB)
#
#         image_data = image_data / 255
#         mask_data = np.array(new_mask)
#
#         index = index + 1
#         bbox_data = []
#         # 对bbox进行重新处理
#         if len(bbox) > 0:
#             # 根据新的宽高与原来宽高的比例得到缩放后的坐标，再加上这张图片在新的大图上的左上角坐标
#             bbox[:, [0, 2]] = bbox[:, [0, 2]] * nw / iw + dx
#             bbox[:, [1, 3]] = bbox[:, [1, 3]] * nh / ih + dy
#
#             raw_bbox = copy.deepcopy(bbox)
#
#             # 小于0的都变成0，超过w,h的bbox值都变成w,h
#             bbox[:, 0:2][bbox[:, 0:2] < 0] = 0
#
#             ##超过像素索引,255之后会从0开始,-1已经到像素的最大索引了
#             bbox[:, 2][bbox[:, 2] > (w - 1)] = w - 1
#             bbox[:, 3][bbox[:, 3] > (h - 1)] = h - 1
#
#             # 计算前后的面积比例,保留大于一定比例的标注
#             now_areas = (bbox[:, 2] - bbox[:, 0]) * (bbox[:, 3] - bbox[:, 1])
#             raw_areas = (raw_bbox[:, 2] - raw_bbox[:, 0]) * (raw_bbox[:, 3] - raw_bbox[:, 1])
#
#             save_index = np.where(now_areas / raw_areas > 0.95)[0]
#             bbox = bbox[save_index]
#             # ** "用面积过滤指定类别对象"
#             # both_save_inds=np.array([])
#             # bask_inds=np.where(bbox[:,4]==bask_num)[0]
#             # bask_save_inds=np.where(now_areas[bask_inds]>3200)[0]
#             # both_save_inds=both_save_inds+bask_inds[bask_save_inds]
#             #
#             # trace_inds = np.where(bbox[:, 4] == trace_num)[0]
#             # trace_save_inds = np.where(now_areas[trace_inds] > 43000)[0]
#             # both_save_inds = both_save_inds + trace_inds[trace_save_inds]
#             #
#             # bbox=bbox[both_save_inds]
#
#             if len(bbox) > 0:
#                 bbox_w = bbox[:, 2] - bbox[:, 0]
#                 bbox_h = bbox[:, 3] - bbox[:, 1]
#                 # 需要bbox的宽高都大于1的bbox才能进入下一步，and表示为与操作，两者都正确则结果正确,bbox[True,True,True...]
#                 bbox = bbox[np.logical_and(bbox_w > 1, bbox_h > 1)]
#
#                 bbox_data = np.zeros((len(bbox), 5))
#                 bbox_data[:len(bbox)] = bbox
#
#         image_datas.append(image_data)
#         mask_datas.append(mask_data)
#         bbox_datas.append(bbox_data)
#
#         # **mask_show = Image.fromarray((mask_data * 255).astype(np.uint8))
#         # for j in range(len(bbox_data)):
#         #
#         #     left, top, right, bottom = bbox_data[j][0:4]
#         #     draw = ImageDraw.Draw(mask_show)
#         #     draw.rectangle([left , top , right, bottom ], outline="green",width=2)
#         #     # #thickness这里控制了i的值，会使得bbox相对于真实值往内收缩
#         #     # thickness = 3
#         #     # for i in range(thickness):
#         #     #     draw.rectangle([left + i, top + i, right - i, bottom - i], outline=(255, 255, 255))
#         # mask_show.show()
#         # **print("fsdff")
#
#         # **img = Image.fromarray((image_data * 255).astype(np.uint8))
#
#         # **img.show()
#         # 经过以上处理就是把一张图片处理完了
#
#     # 将图片分割，放在一起
#     cutx = np.random.randint(int(w * min_offset_x), int(w * (1 - min_offset_x)))
#     cuty = np.random.randint(int(h * min_offset_y), int(h * (1 - min_offset_y)))
#
#     new_image = np.zeros([h, w, 3])
#     new_image[:cuty, :cutx, :] = image_datas[0][:cuty, :cutx, :]
#     new_image[cuty:, :cutx, :] = image_datas[1][cuty:, :cutx, :]
#     new_image[cuty:, cutx:, :] = image_datas[2][cuty:, cutx:, :]
#     new_image[:cuty, cutx:, :] = image_datas[3][:cuty, cutx:, :]
#
#     new_image = (new_image * 255).astype(np.uint8)
#
#     # **#检验第四个顺序的图bbox对应标签
#     # mmask=Image.fromarray((mask_datas[3]*255).astype(np.uint8))
#     # for num in range(len(bbox_datas[3])):
#     #     left, top, right, bottom = bbox_datas[3][num]
#     #     draw = ImageDraw.Draw(mmask)
#     #     draw.rectangle([left , top , right, bottom ], outline="green",width=2)
#     #
#     # **mmask.show()
#
#     new_mask = np.zeros([h, w, 3])
#     new_mask[:cuty, :cutx, :] = mask_datas[0][:cuty, :cutx, :]
#     new_mask[cuty:, :cutx, :] = mask_datas[1][cuty:, :cutx, :]
#     new_mask[cuty:, cutx:, :] = mask_datas[2][cuty:, cutx:, :]
#     new_mask[:cuty, cutx:, :] = mask_datas[3][:cuty, cutx:, :]
#     new_mask = new_mask.astype(np.uint8)
#
#     new_bbox = merge_bboxes(bbox_datas, cutx, cuty, h, w, save_proportion=save_proportion)
#     # if len(new_bbox)>0:
#     #     **"合并bbox后,再次用面积过滤指定类别对象"
#     #     new_bbox=np.array(new_bbox)
#     #     now_areas = (new_bbox[:, 2] - new_bbox[:, 0]) * (new_bbox[:, 3] - new_bbox[:, 1])
#     #
#     #     both_save_inds = np.array([])
#     #     bask_inds = np.where(new_bbox[:, 4] == bask_num)[0]
#     #     bask_save_inds = np.where(now_areas[bask_inds] > 2000)[0]
#     #     # bask_save_inds = np.where(now_areas[bask_inds] > 3200)[0]
#     #     both_save_inds = np.concatenate((both_save_inds , bask_inds[bask_save_inds]),axis=0)
#     #
#     #     trace_inds = np.where(new_bbox[:, 4] == trace_num)[0]
#     #     trace_save_inds = np.where(now_areas[trace_inds] > 16000)[0]
#     #     both_save_inds = np.concatenate((both_save_inds , trace_inds[trace_save_inds]),axis=0)
#     #     both_save_inds=both_save_inds.astype(np.uint8)
#     #     if len(both_save_inds)>0:
#     #          new_bbox = list(new_bbox[both_save_inds])
#     #     else:
#     #         return  [],[],[]
#     if len(new_bbox) > 0:
#         new_bbox = np.array(new_bbox).astype(np.uint16)
#         # [xmin:xmax,ymin:ymax]没取到xmax,ymax
#         new_bbox[:, 2] = new_bbox[:, 2] + 1
#         new_bbox[:, 3] = new_bbox[:, 3] + 1
#
#         new_bbox[:, 2][new_bbox[:, 2] > w - 1] = w - 1
#         new_bbox[:, 3][new_bbox[:, 3] > h - 1] = h - 1
#
#         # **for j in range(len(new_bbox)):
#         #     left, top, right, bottom = new_bbox[j][0:4]
#         #     draw = ImageDraw.Draw(show_mask)
#         #     draw.rectangle([left, top, right, bottom], outline="red", width=1)
#         #
#         # **show_mask.show()
#
#         return new_image, new_mask, new_bbox
#     else:
#         return [], [], []

def nms_all_output(all_rois, all_masks, all_scores, all_class_ids, all_leups, transform, class_names, out_data_location,
                   out_dataset_name, nms_thresh, profile, return_bbox=False):
    """
    对预测结果进行nms后转为georegion对象存入udbx中
            all_rois：一维列表，列表内每个单元存储每个block预测结果的最小外接矩形
            all_masks：一维列表，列表内每个单元存储单个对象的mask，与all_rois内bbox的顺序一致
            all_scores：一维列表，列表内每个单元存储每个block预测结果的所有分数值
            all_class_ids：一维列表，列表内每个单元存储每个block预测结果的所有类别代码
            all_leups：一维列表，列表内每个单元存储所预测block在原图上的左上角点像素坐标（多个相同的点）
    """

    extra_rois, extra_masks, extra_class_ids, extra_scores, extra_leups \
        = nms_large_output(all_rois, all_masks,all_scores, all_class_ids, all_leups,nms_thresh)

    extra_rois, extra_scores, extra_class_ids, masks_processed = geo_process_masks_smooth_no_pinjie(
        extra_rois, extra_masks, extra_scores, extra_class_ids, extra_leups, transform)

    output_dataset_name = save_udb(transform, extra_rois, masks_processed, extra_scores, extra_class_ids, class_names,
                                   out_data_location, out_dataset_name, profile, return_bbox=return_bbox)

    return output_dataset_name


def nms_large_output(all_rois, all_masks, all_scores, all_class_ids,all_leups, nms_thresh):
    """
        对预测结果进行nms,相关的结果除mask是列表中顺序存储多个对象mask，其他都与模型的直接输出形式无异
                all_rois：一维列表，列表内每个单元存储每个block预测结果的最小外接矩形
                all_masks：一维列表，列表内每个单元存储单个对象的mask，与all_rois内bbox的顺序一致
                all_scores：一维列表，列表内每个单元存储每个block预测结果的所有分数值
                all_class_ids：一维列表，列表内每个单元存储每个block预测结果的所有类别代码
                all_leups：一维列表，列表内每个单元存储所预测block在原图上的左上角点像素坐标（多个相同的点）
    """

    rois = np.concatenate([arr for arr in all_rois if arr.size > 0])
    class_ids = np.concatenate([arr for arr in all_class_ids if arr.size > 0])
    scores = np.concatenate([arr for arr in all_scores if arr.size > 0])
    leups = np.concatenate([arr for arr in all_leups if arr.size > 0])

    # 按类别分别进行nms， 这一步返回一个列表，表示有几类
    class_num = list(set(class_ids.flatten()))
    saved_inds = np.array([], dtype=np.uint8)

    # 提取第几类
    for _num in range(len(class_num)):
        _cls = class_num[_num]
        _cls_inds = np.where(class_ids == _cls)[0]
        # 提取当前类的rois和scores
        cls_rois = rois[_cls_inds]
        cls_scores = scores[_cls_inds]
        # 执行非最大抑制并返回保留框的索引
        pick_inds = non_max_suppression(cls_rois, cls_scores, threshold=nms_thresh)
        pick_inds = _cls_inds[pick_inds]
        if len(pick_inds) > 0:
            saved_inds = np.append(saved_inds, pick_inds, axis=0)
    saved_inds.sort()

    # 以nms结果更新模型推理结果
    extra_rois = rois[saved_inds]
    extra_rois = extra_rois.astype(np.float64)
    extra_scores = scores[saved_inds]
    extra_class_ids = class_ids[saved_inds]
    extra_leups = leups[saved_inds]
    if isinstance(all_masks[0], list):
        masks = np.concatenate([arr for arr in all_masks if arr.size > 0])
        extra_masks = masks[saved_inds]
    else:
        extra_masks = [all_masks[i_m] for i_m in saved_inds]

    return extra_rois, extra_masks, extra_class_ids, extra_scores,extra_leups


def geo_process_masks_smooth_no_pinjie(rois, masks, scores, class_ids, leups, transform, return_geometry=True):
    # leups ,masks 存的内容都是对于单一的mask
    #return_geometry：进行实例分割的返回结果是否为geometry。如果是true，返回geometry;如果是false，会返回geojson

    # mask_num = len(leups)
    mask_num = len(rois)
    masks_process = [None] * mask_num
    problem_mk = []

    for mnum in range(mask_num):
        mask_c = masks[mnum]
        leup = leups[mnum]
        x, y = rio_transform.xy(transform, leup[1], leup[0])
        # input is the transform of the satelite img, the height and width of the mask left_up point corresponding to satelite img
        tile_transform = rio_transform.from_origin(x, y, transform[0], -transform[4])
        if isinstance(mask_c, dict):
            # mask_c = rle2mask(mask_c)
            mask_c = np.array(mask_util.decode(mask_c), dtype=np.uint8)
        # else:
        #     mask_c = np.array(mask_util.decode(mask_c),dtype=np.uint8)
        con_ck = features.shapes(mask_c, mask=mask_c.astype(np.bool_), transform=tile_transform)
        point_num = 0
        con_c = None
        select_ind = 0
        try:
            _con_c = next(con_ck)[0]
            ind_cal = 0
            while True:
                if len(_con_c["coordinates"][0]) > point_num:
                    point_num = len(_con_c["coordinates"][0])
                    con_c = copy.deepcopy(_con_c)
                    select_ind = ind_cal

                else:
                    try:
                        _con_c = next(con_ck)[0]
                        ind_cal += 1
                    except Exception as e:  # iter over
                        # it will still use the con_c,that points<=100
                        # print("StopIteration")
                        break
            if con_c == None:
                raise ValueError("there is no closed polygon in the mask")
            # print("len is {}  ,select is {}".format(ind_cal,select_ind))
            json_shp = json.dumps(con_c)
            # generate shp of the mask
            if return_geometry:
                region_shp = Geometry.from_geojson(json_shp)
                masks_process[mnum] = region_shp
            else:
                masks_process[mnum] = json_shp
        except Exception as e:
            continue
            # 只要最大像素值不为0,还抛出异常,那就说明这个对象有问题,很可能就是没有闭合的面
            print(e)
            # raise ValueError(e)
    all_inds = np.array([x for x in range(len(scores))])
    all_inds = np.delete(all_inds, problem_mk, axis=0)
    rois = rois[all_inds]
    scores = scores[all_inds]
    class_ids = class_ids[all_inds]
    masks_process = [masks_process[i_m] for i_m in range(len(masks_process)) if i_m in all_inds]
    return rois, scores, class_ids, masks_process


def save_udb(transform, rois, masks, scores, class_ids, class_name, out_dir, out_name, profile, return_bbox=False):
    """

    :param rois: [xmin,ymin,xmax,ymax]
    :param masks:
    :param scores: scores of result
    :param class_ids: class names correspond ids
    :param class_name: class_ids corresponding class names
    :param out_dir: existing or creating udb location
    :param out_name: dataset name,just for one
    :return:
    """
    # save_udb(estimate_out,class_names,out_dir,out_name):

    # input_data=r"E:\push_branch\resources_ml\example_data\inference\pv_panels_infer.tif"
    # from PIL import Image, ImageDraw
    # show_img = Image.open(input_data)
    # draw = ImageDraw.Draw(show_img)

    # just for predict an image,open udb
    ds = get_output_datasource(out_dir)
    check_output_datasource(ds)
    if out_name is None:
        out_name = 'NewDataset'
    # ##########create vector dataset,set their names

    mask_dataset = ds.create_vector_dataset(out_name, DatasetType.REGION, adjust_name=True)
    if profile.get("crs"):
        mask_dataset.set_prj_coordsys(profile.get("crs").to_wkt())
    ################create field
    type_class = FieldInfo("class_type", FieldType.TEXT, max_length=50)
    type_score = FieldInfo("scores", FieldType.DOUBLE, max_length=50)
    type_object = FieldInfo("object_num", FieldType.INT64, max_length=50)

    #############each vector dataset create field
    mask_dataset.create_field(type_class)
    mask_dataset.create_field(type_score)
    mask_dataset.create_field(type_object)

    feature_mask_list = []
    if len(scores) != 0:
        for ie in range(len(scores)):
            _mask = masks[ie]
            _score = scores[ie]
            _class = class_name[class_ids[ie]]

            # masks have been converted
            feature_mask = Feature(_mask, {"class_type": _class, "scores": _score, "object_num": ie},
                                   field_infos=[type_class, type_score, type_object])
            feature_mask_list.append(feature_mask)
        mask_dataset.append(feature_mask_list)
        mask_dataset.close()

        if return_bbox:
            bbox_name = out_name + str('_bbox')

            bbox_dataset = ds.create_vector_dataset(bbox_name, DatasetType.REGION, adjust_name=True)

            type_class = FieldInfo("class_type", FieldType.TEXT, max_length=50)
            type_score = FieldInfo("scores", FieldType.DOUBLE, max_length=50)

            #############each vector dataset create field
            bbox_dataset.create_field(type_class)
            bbox_dataset.create_field(type_score)

            bbox_li = []
            if len(scores) != 0:
                for ie in range(len(scores)):
                    _rois = rois[ie]
                    _score = scores[ie]
                    _class = class_name[class_ids[ie]]

                    xmin, ymin = rio_transform.xy(transform, _rois[1], _rois[0])
                    xmax, ymax = rio_transform.xy(transform, _rois[3], _rois[2])

                    # so 1,3 is x, 0,2 is y
                    point = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax), (xmin, ymin)]
                    region = GeoRegion(point)
                    feature_roi = Feature(region, {"class_type": _class, "scores": _score},
                                          field_infos=[type_class, type_score])
                    bbox_li.append(feature_roi)
                bbox_dataset.append(bbox_li)
                bbox_dataset.close()

    return out_name


def mask2rle(img):
    '''
    Convert mask to rle.
    img: numpy array,
    1 - mask,
    0 - background

    Returns run length as string formated
    '''
    rle = {'counts': [], 'size': list(img.shape)}
    pixels = img.T.flatten()
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    rle['counts'] = runs.tolist()
    return rle


def rle2mask(rle):
    '''
    Decode rle encoded mask.

    :param mask_rle: run-length as string formatted (start length)
    :param shape: (height, width) of array to return
    Returns numpy array, 1 - mask, 0 - background
    '''
    starts, lengths = [np.asarray(x, dtype=int) for x in (rle['counts'][0:][::2], rle['counts'][1:][::2])]
    starts -= 1
    ends = starts + lengths
    img_new = np.zeros(rle['size'][0] * rle['size'][1], dtype=np.uint8)
    for lo, hi in zip(starts, ends):  # 进行恢复
        img_new[lo:hi] = 1
    return img_new.reshape((rle['size'][0], rle['size'][1]), order='F')
