import json

import cv2
import matplotlib.pyplot as plt
import numpy as np
import shapely
import torch
import tqdm
from shapely.geometry import Polygon
from torchvision.ops.boxes import batched_nms

from iobjectspy import topology_validate, FieldInfo, FieldType, Feature, Geometry
from .amg import (
    BatchMaskData,
    batch_calculate_stability_score,
    batchedimage_mask_to_box,
    mask_to_rle_pytorch,
    rle_to_mask,
    batch_is_box_near_tile_edge,
)


def uniform_sampling_by_interval(array, num_samples):
    # 如果数组中的点少于或等于所需样本数，直接返回原数组
    if len(array) <= num_samples:
        return array

    # 计算步长
    step = len(array) // num_samples

    # 使用步长进行均匀采样
    sampled_array = array[::step][:num_samples]
    return sampled_array


def farthest_point_sampling_with_constraints(points, constraints, k, min_dist_to_grad):
    farthest_pts = np.zeros((k, points.shape[1]))
    # farthest_pts[0] = points[np.random.randint(len(points))]
    # 不使用随机初始化的点，使用索引中位数点
    median_index = len(points) // 2
    farthest_pts[0] = points[median_index]
    distances = np.linalg.norm(points - farthest_pts[0], axis=1)
    if len(points) == 0 or len(constraints) == 0:
        print("----------------")
    constraint_distances = np.min(np.linalg.norm(points[:, None] - constraints[None, :], axis=2), axis=1)
    # 确保距离 coords_grad 的距离大于阈值
    valid_indices = constraint_distances > min_dist_to_grad
    distances = distances * valid_indices

    for i in range(1, k):
        farthest_index = np.argmax(distances)
        farthest_pts[i] = points[farthest_index]
        distances[farthest_index] = np.inf
        distances = np.minimum(distances, np.linalg.norm(points - farthest_pts[i], axis=1))

    return farthest_pts


# def show_result(image, mask_list=None, box_tuple=None, points_np=None, labels=None, save=None, index=None, visual=None):
#     """
#     测试时的可视化代码，可以观察中间的输出
#     image：ndarrary格式
#     box_tuple：tuple格式，如(xmin, ymin, xmax, ymax)
#     mask_list：list列表，里面是tensor张量
#     sampled_points_np：ndarray格式
#     labels：ndarray格式，多少个点对应多少个label
#     """
#
#     plt.figure(figsize=(10, 10))
#     plt.imshow(image)
#     if save:
#         plt.savefig(f"D:/temp/0110/img_show.jpg")
#     if mask_list is not None:
#         mask_list_temp = mask_list[0].cpu()
#         for mask_ in mask_list:
#             mask_list_temp += mask_
#
#         def show_mask(mask, ax, random_color=False):
#             if random_color:
#                 color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
#             else:
#                 color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])
#             h, w = mask.shape[-2:]
#             mask_image = mask[0, :, :].reshape(h, w, 1) * color.reshape(1, 1, -1)
#             ax.imshow(mask_image)
#
#         show_mask(mask_list_temp, plt.gca())
#
#     if points_np is not None:
#         def show_points(coords, labels, ax, marker_size=75):
#             pos_points = coords[labels == 1]
#             neg_points = coords[labels == 0]
#             ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=1, edgecolor='white',
#                        linewidth=1.25)
#             ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=1, edgecolor='white',
#                        linewidth=1.25)
#
#         show_points(points_np, labels, plt.gca())
#
#     if box_tuple is not None:
#         box_np = np.array(box_tuple, dtype=float)
#
#         def show_box(box, ax):
#             x0, y0 = box[0], box[1]
#             w, h = box[2] - box[0], box[3] - box[1]
#             ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))
#
#         show_box(box_np, plt.gca())
#
#     plt.axis('on')
#     if save:
#         plt.savefig(f"D:/temp/0110/{index}_show.jpg")
#     if visual:
#         plt.show()
#         plt.clf()
#         plt.close('all')


# @count_time
def sam_postprocess(crop_box, masks, iou_preds, pred_iou_thresh, mask_threshold, filter_edge, bbox_nms_thresh):
    if len(masks) == 0:
        return None
    # Serialize predictions and store in MaskData
    data = BatchMaskData(
        masks=masks.flatten(1, 2),
        iou_preds=iou_preds.flatten(1, 2),
    )

    # Filter by predicted IoU
    if pred_iou_thresh > 0.0:
        keep_mask = {key: (value >= pred_iou_thresh) for key, value in data["iou_preds"].items()}
        # keep_mask = data["iou_preds"] > self.pred_iou_thresh
        data.filter(keep_mask)
        del keep_mask
    # Calculate stability score
    # 通过定义一个较高的阈值得到intersections, 定义一个较低的阈值得到unions, 计算score
    stability_score_offset = 1
    stability_score_thresh = 0.90
    data["stability_score"] = batch_calculate_stability_score(
        data["masks"], mask_threshold, stability_score_offset
    )
    if stability_score_thresh > 0.0:
        keep_mask = {key: (value >= stability_score_thresh) for key, value in data["stability_score"].items()}
        data.filter(keep_mask)
    del data["stability_score"], keep_mask

    # Threshold masks and calculate boxes
    data["masks"] = {k: data["masks"][k] > mask_threshold for k in data["masks"].keys()}
    data["boxes"] = batchedimage_mask_to_box(data["masks"])

    # Filter boxes that touch crop boundaries
    if filter_edge:
        keep_mask = batch_is_box_near_tile_edge(data["boxes"], crop_box)  # 全图分割
        data.filter(keep_mask)
        del keep_mask

    # Compress to RLE
    data["rles"] = mask_to_rle_pytorch(data["masks"])
    del data["masks"]

    keep_by_nms = dict()
    for b in range(len(data["boxes"])):
        keep_by_nms[b] = batched_nms(
            data["boxes"][b].float(),
            data["iou_preds"][b],
            torch.zeros_like(data["boxes"][b][:, 0]),  # categories
            iou_threshold=bbox_nms_thresh,
        )
    data.filter(keep_by_nms)
    del data["boxes"], data["iou_preds"]
    data["segmentations"] = {k: [rle_to_mask(rel) for rel in rle_list] for k, rle_list in data["rles"].items()}
    del data["rles"]

    return data


def sam_filt_masks(crop_box, ori_box, masks, iou_preds, pred_iou_thresh, mask_threshold, filter_edge):
    if len(masks) == 0:
        return None
    # Serialize predictions and store in MaskData
    data = BatchMaskData(
        masks=masks.flatten(1, 2),
        iou_preds=iou_preds.flatten(1, 2),
    )

    # Filter by predicted IoU
    if pred_iou_thresh > 0.0:
        keep_mask = {key: (value >= pred_iou_thresh) for key, value in data["iou_preds"].items()}
        # keep_mask = data["iou_preds"] > self.pred_iou_thresh
        data.filter(keep_mask)
        del keep_mask

    # Calculate stability score
    # 通过定义一个较高的阈值得到intersections, 定义一个较低的阈值得到unions, 计算score
    stability_score_offset = 1
    stability_score_thresh = 0.90
    data["stability_score"] = batch_calculate_stability_score(
        data["masks"], mask_threshold, stability_score_offset
    )
    if stability_score_thresh > 0.0:
        keep_mask = {key: (value >= stability_score_thresh) for key, value in data["stability_score"].items()}
        data.filter(keep_mask)
    del data["stability_score"], keep_mask

    # Threshold masks and calculate boxes
    data["masks"] = {k: data["masks"][k] > mask_threshold for k in data["masks"].keys()}
    data["boxes"] = batchedimage_mask_to_box(data["masks"])

    # Filter boxes that touch crop boundaries
    if filter_edge:
        keep_mask = batch_is_box_near_tile_edge(data["boxes"], crop_box, ori_box)  # 全图分割
        data.filter(keep_mask)
        del keep_mask

    return data


def mask2bbox_nms(data, bbox_nms_thresh):
    # Compress to RLE
    data["rles"] = mask_to_rle_pytorch(data["masks"])
    del data["masks"]

    keep_by_nms = dict()
    for b in range(len(data["boxes"])):
        keep_by_nms[b] = batched_nms(
            data["boxes"][b].float(),
            data["iou_preds"][b],
            torch.zeros_like(data["boxes"][b][:, 0]),  # categories
            iou_threshold=bbox_nms_thresh,
        )
    data.filter(keep_by_nms)
    del data["boxes"], data["iou_preds"]
    data["segmentations"] = {k: [rle_to_mask(rel) for rel in rle_list] for k, rle_list in data["rles"].items()}
    del data["rles"]

    return data


# @count_time
def mask_seg_mask(mask_data, mask_areas_thresh, msm_kernel, bias, mam_open_iterations=10):
    """
    先筛选面积，再将被包含在大块区域里的小区域通过xor操作分割出来，并腐蚀边缘
    """
    mask_list_result = []
    if isinstance(mask_data["segmentations"], list):
        sorted_masks = mask_area_filtering(mask_data["segmentations"], mask_areas_thresh)
        mask_list_result = mask_to_segmentation_mask(sorted_masks, msm_kernel, mam_open_iterations=mam_open_iterations)
    elif isinstance(mask_data["segmentations"], dict):
        for key, masks_value in mask_data["segmentations"].items():
            sorted_masks = mask_area_filtering(masks_value, mask_areas_thresh)
            if bias == "large":
                mask_list_ = mask_to_segmentation_mask(sorted_masks, msm_kernel, mam_open_iterations=mam_open_iterations)
            else:
                mask_list_ = sorted_masks
            mask_list_result.append(mask_list_)
    else:
        print("Only supports list and dict!")

    return mask_list_result


def mask_area_filtering(masks_list, mask_areas_thresh):
    # 计算每个掩码的面积并构建带有索引的列表
    mask_areas = [np.sum(mask) for mask in masks_list]
    indexed_masks = list(enumerate(masks_list))  # 添加索引
    # 过滤掉面积小于阈值的掩码
    filtered_masks = [(index, mask) for index, mask in indexed_masks if
                      mask_areas[index] >= mask_areas_thresh]
    # 根据掩码的面积大小对掩码进行排序
    sorted_masks = [mask for _, mask in sorted(filtered_masks, key=lambda x_: mask_areas[x_[0]], reverse=True)]

    return sorted_masks


def mask_to_segmentation_mask(sorted_masks, msm_kernel, mam_open_iterations=10):
    num_masks = len(sorted_masks)
    if num_masks == 0:
        return []

    # 提前计算所有掩码对的交集和并集
    intersection_matrix = np.zeros((num_masks, num_masks), dtype=int)
    for i in range(len(sorted_masks)):
        for j in range(i + 1, len(sorted_masks)):
            intersection_matrix[i, j] = np.sum(np.logical_and(sorted_masks[i], sorted_masks[j]))

    index_ = np.where(intersection_matrix > 0)
    coordinates = np.transpose(index_)
    # 定义腐蚀核
    if msm_kernel == 3:
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    else:
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    for index in coordinates:
        i, j = index
        mask = sorted_masks[i]
        comper_mask = sorted_masks[j]
        mask_xor = np.logical_xor(mask, comper_mask).astype(np.uint8)  # 一个位置上只有一个为True
        opening = cv2.morphologyEx(mask_xor, cv2.MORPH_OPEN, kernel, iterations=mam_open_iterations)
        opening_bool = opening.astype(np.bool_)
        sorted_masks[i] = opening_bool

    return sorted_masks


def sam_bbox_postprocess(masks, iou_preds, pred_iou_thresh):
    if len(masks) == 0:
        return None
    # Serialize predictions and store in MaskData
    data = BatchMaskData(
        masks=masks.flatten(1, 2),
        iou_preds=iou_preds.flatten(1, 2),
    )
    del masks, iou_preds
    if pred_iou_thresh > 0.0:
        keep_mask = {key: (value >= pred_iou_thresh) for key, value in data["iou_preds"].items()}
        data.filter(keep_mask)
        del keep_mask
    # Compress to RLE
    data["rles"] = mask_to_rle_pytorch(data["masks"])
    del data["masks"]

    data["segmentations"] = {k: [rle_to_mask(rel) for rel in rle_list] for k, rle_list in data["rles"].items()}
    del data["rles"]

    return data


def geo2polygons(geo):
    points_list = geo.get_parts()
    points = max(points_list, key=len)
    polygon = Polygon(points)
    return polygon


def resolve_topo_overlaperror(ds_, analyzed_data, resolution):
    is_success = analyzed_data.update_field_express(item="SmUserID", express="SMID")
    topo_result = topology_validate(analyzed_data, validating_data=None,
                                    rule='REGION_NO_OVERLAP', tolerance=resolution,
                                    out_data=ds_, out_dataset_name='temp_topo')
    topolist_features = topo_result.get_features()
    error_list = []
    for feature in tqdm.tqdm(topolist_features, desc='Topology checking...', leave=False):
        value = feature.get_values()
        smid1, smid2 = value[2], value[3]
        if smid1 in error_list or smid2 in error_list:
            continue
        try:
            geo1 = analyzed_data.get_geometries(attr_filter=f"SmUserID={smid1}")[0]
            geo2 = analyzed_data.get_geometries(attr_filter=f"SmUserID={smid2}")[0]
        except IndexError:
            raise IndexError(f"smid1:{smid1}, smid2:{smid2}")
        polygon1 = geo2polygons(geo1)
        polygon2 = geo2polygons(geo2)
        bigger_polygon, smaller_polygon, modifysmid, deleteid = (
            (polygon1, polygon2, smid2, geo2.id) if polygon1.area > polygon2.area else
            (polygon2, polygon1, smid1, geo1.id))
        result_polygon = smaller_polygon.difference(bigger_polygon)
        if result_polygon.area == 0:
            analyzed_data.delete_records(ids=deleteid)
            error_list.append(deleteid)
            continue

        region_geojson = json.dumps(shapely.geometry.mapping(result_polygon))
        region_shp = Geometry.from_geojson(region_geojson)  # GeoRegion

        user_iD = FieldInfo("modifysmid", FieldType.INT32, default_value=modifysmid)
        feature_mask = Feature(region_shp, field_infos=[user_iD])
        field_dict_ = {'modifysmid': 'SmUserID'}
        is_write = analyzed_data.append(feature_mask, fields=field_dict_)
        analyzed_data.delete_records(ids=deleteid)
    return topo_result
