import sys

import torch
import torch.distributed as dist
from tqdm import tqdm as tqdm

from .seg_epoch import SegEpoch
from .._torch_models.util.meter import AverageMeter, intersectionAndUnionGPU


class SiameCDEpoch(SegEpoch):
    def __init__(self, model, loss, metrics, stage_name,
                 device='cpu', verbose=True, **kwargs):
        self.model = model
        self.loss = loss
        self.metrics = metrics
        self.stage_name = stage_name
        self.verbose = verbose
        self.device = device
        self._to_device()
        self.model_name = kwargs.get('model_name')

    def _to_device(self):
        self.model.to(self.device)
        self.loss.to(self.device)
        for metric in self.metrics:
            metric.to(self.device)

    def run(self, epoch_info, dataloader, multiprocessing_distributed, Rank):
        self.multiprocessing_distributed = multiprocessing_distributed
        self.Rank = Rank
        self.on_epoch_start()

        logs = {}
        meters = {
            'loss': AverageMeter(),
            'intersection': AverageMeter(),
            'union': AverageMeter(),
            'target': AverageMeter(),
            **{metric.__name__: AverageMeter() for metric in self.metrics}
        }

        metric_class = 2

        stage_name = epoch_info + ' ' + self.stage_name
        # 总步数
        total_steps = len(dataloader)
        local_progress = 0  # 当前进程的进度
        world_size = dist.get_world_size() if self.multiprocessing_distributed else 1
        # 主进程初始化合并进度条
        if Rank == 0:
            pbar = tqdm(total=total_steps * world_size, desc=stage_name, dynamic_ncols=True, file=sys.stdout)

        for x1, x2, y in dataloader:
            if self.device == 'cuda':
                x1, x2, y = x1.cuda(non_blocking=True), x2.cuda(non_blocking=True), y.cuda(non_blocking=True)
            else:
                x1, x2, y = x1.to(self.device), x2.to(self.device), y.to(self.device)

            loss, y_pred = self._batch_update(x1, x2, y)
            if hasattr(self, 'optimizer'):
                self._update_lr_logs(logs)

            self._update_loss_logs(loss, x1.size(0), meters, logs)

            self._update_metrics_logs(y_pred, y, metric_class, meters, logs)

            local_progress += 1  # 当前进程的进度加 1

            # 进度同步（确保所有 GPU 进度一致）
            if self.multiprocessing_distributed:
                progress_tensor = torch.tensor([local_progress], dtype=torch.float32).to(self.device)
                dist.all_reduce(progress_tensor, op=dist.ReduceOp.SUM)  # 求所有进程的进度和
                global_progress = int(progress_tensor.item())
            else:
                global_progress = local_progress  # 单 GPU 直接使用本地进度

            if Rank == 0:
                # 主进程更新进度条
                pbar.update(global_progress - pbar.n)
                if self.verbose:
                    pbar.set_postfix_str(self._format_logs(logs))

        if Rank == 0:
            pbar.close()

        return logs, y_pred

    def _update_metrics_logs(self, y_pred, y, metric_class, meters, logs):
        intersection, union, target = intersectionAndUnionGPU(y_pred, y, metric_class, self.device)

        if self.multiprocessing_distributed:
            dist.all_reduce(intersection)
            dist.all_reduce(union)
            dist.all_reduce(target)

        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()
        meters['intersection'].update(intersection)
        meters['union'].update(union)
        meters['target'].update(target)

        iou_class = meters['intersection'].sum / (meters['union'].sum + 1e-10)
        accuracy_class = meters['intersection'].sum / (meters['target'].sum + 1e-10)
        recall_class = meters['intersection'].sum / (
                meters['union'].sum - meters['target'].sum + meters['intersection'].sum + 1e-10)
        f1_class = 2 * accuracy_class * recall_class / (accuracy_class + recall_class + 1e-10)

        F1 = f1_class[1]
        IoU = iou_class[1]

        for metric_fn in self.metrics:
            value = IoU if metric_fn.__name__ == 'iou_score' else F1
            meters[metric_fn.__name__].update(value)
            logs[metric_fn.__name__] = meters[metric_fn.__name__].val


class TrainEpoch_SiameCD(SiameCDEpoch):

    def __init__(self, model, loss, metrics, optimizer, device='cpu', verbose=True, **kwargs):
        super().__init__(
            model=model,
            loss=loss,
            metrics=metrics,
            stage_name='train',
            device=device,
            verbose=verbose,
            **kwargs
        )
        self.optimizer = optimizer
        self.init_lr = self.optimizer.param_groups[0]['lr']
        self.global_step = kwargs.get('start_epoch')[0] * kwargs.get('start_epoch')[1]
        self.lr_sh = kwargs.get('lr_sh')
        self.scheduler = kwargs.get('scheduler')

    def on_epoch_start(self):
        self.model.train()

    def _batch_update(self, x1, x2, y):
        self.global_step += 1
        self.scheduler(self.optimizer, self.global_step)
        self.optimizer.zero_grad()
        prediction = self.model([x1, x2])
        if self.model_name == 'dsamnet':
            loss = self.loss(prediction, y.float())
        else:
            loss = self.loss(prediction.squeeze(dim=1), y.float())
        if not self.multiprocessing_distributed:
            loss = torch.mean(loss)
        loss.backward()
        self.optimizer.step()
        if self.model_name == 'dsamnet':
            return loss, (prediction > 1).float()[:, 0, :, :]
        else:
            prediction_out = torch.cat((1 - prediction, prediction), 1)
            return loss, prediction_out.max(1)[1]


class ValidEpoch_SiameCD(SiameCDEpoch):
    def __init__(self, model, loss, metrics, device='cpu', verbose=True, **kwargs):
        super().__init__(
            model=model,
            loss=loss,
            metrics=metrics,
            stage_name='valid',
            device=device,
            verbose=verbose,
            **kwargs
        )

    def on_epoch_start(self):
        self.model.eval()

    def _batch_update(self, x1, x2, y):
        if self.model_name == 'dsamnet':
            with torch.no_grad():
                prediction = self.model([x1, x2])
                loss = self.loss(prediction, y.float())
            return loss, (prediction > 1).float()[:, 0, :, :]
        else:
            with torch.no_grad():
                prediction = self.model([x1, x2])
                loss = self.loss(prediction.squeeze(dim=1), y.float())
                prediction_out = torch.cat((1 - prediction, prediction), 1)
            return loss, prediction_out.max(1)[1]
