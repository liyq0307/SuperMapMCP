import glob
import importlib
import os
import platform
import urllib.request
import warnings

import torch
from PIL import Image
from mmengine.config import ConfigDict

from .._mmdet import instance_segmentation_config, object_detection_config
from .._mmdet._tools import (get_classname_from_traindata, get_mmdet_argument_parser,
                             saving_mmdet_model, replace_num_classes, load_image_set_number,
                             creat_output_model_folder, update_dataset_config, saving_latest_model_sdl)
from ...device_utils import import_device_plugins
from ...object_detection._mmdet.configs._base_ import custom_imports


def import_modules(custom_imports):
    imported_modules = {}
    for module_name in custom_imports["imports"]:
        try:
            imported_modules[module_name] = importlib.import_module(module_name)
        except ImportError as e:
            if custom_imports["allow_failed_imports"]:
                print(f"Warning: Failed to import {module_name} - {e}")
            else:
                raise ImportError(f"Error: Failed to import {module_name}") from e
    return imported_modules


imported_modules = import_modules(custom_imports)


class MMDetTrainer:
    def __init__(self, train_data_path, sdt_config, epoch, batch_size, lr, output_model_path,
                 output_model_name, log_path, backbone_name, backbone_weight_path,
                 reload_model, pretrained_model_path, gpus, sdt_config_path=None, **kwargs):
        """初始化训练器"""
        self.cfg = None
        self.args = None
        self.set_gpu(gpus)
        import_device_plugins()

        class_names, training_data_format, input_bandnum = get_classname_from_traindata(sdt_config, train_data_path)

        # 创建日志目录
        if not os.path.exists(log_path):
            os.makedirs(log_path)
        # 构建参数

        self.args = get_mmdet_argument_parser(
            train_data_path=train_data_path,
            config=sdt_config,
            epoch=epoch,
            batch_size=batch_size,
            lr=lr,
            log_path=log_path,
            reload_model=reload_model,
            backbone_name=backbone_name,
            backbone_weight_path=backbone_weight_path,
            num_machines=1,
            class_names=class_names,
            gpus=gpus,
            pretrained_model_path=pretrained_model_path,
            training_data_format=training_data_format,
            sdt_config_path=sdt_config_path,
            output_model_name=output_model_name,
            output_model_path=output_model_path,
            input_bandnum=input_bandnum
        ).parse_args()

    def set_gpu(self, gpus):
        """设置GPU"""
        if (gpus == -1) or (gpus == []):
            os.environ["CUDA_VISIBLE_DEVICES"] = '-1'
        else:
            gpu_id = ','.join(str(x) for x in gpus)
            os.environ["CUDA_VISIBLE_DEVICES"] = gpu_id

    def update_mmdet_config_from_sdt(self, cfg, args):
        """
        从sdt文件更新cfg
        """
        # 设置warmup方式
        if str(self.args.config.lr_method.warmup) != 'default':
            if str(self.args.config.lr_method.warmup) == 'Linear' or str(self.args.config.lr_method.warmup) == 'linear':
                warnings.warn('The current default learning rate warm-up method is linear!', UserWarning)
            # todo:Exp预热
            # elif str(args.config.lr_method.warmup) == 'Exp' or str(args.config.lr_method.warmup) == 'exp':
            #     exp_warmup_dict = ConfigDict(type='ExponentialLR', gamma=1.001, begin=0, end=0, by_epoch=False)
            #     if isinstance(cfg.param_scheduler, list):
            #         cfg.param_scheduler[0] = exp_warmup_dict
            # 关闭warmup
            elif str(self.args.config.lr_method.warmup) == 'Close' or str(self.args.config.lr_method.warmup) == 'close':
                if isinstance(cfg.param_scheduler, list):
                    # 直接清除warmup的字典
                    lr_decay_dict = cfg.param_scheduler[1]
                    cfg.param_scheduler = lr_decay_dict
            else:
                raise NotImplementedError('This learning rate warm-up method is not currently supported!')
        # 设置warmup轮次,默认为1个epoch
        img_number = load_image_set_number(args.train_data_path)
        if isinstance(cfg.param_scheduler, list):

            if img_number % args.batch_size == 0:
                end = (img_number // args.batch_size)
            else:
                end = (img_number // args.batch_size) + 1

            if cfg.train_dataloader.dataset.type == 'RepeatDataset':
                times = cfg.train_dataloader.dataset.times
            else:
                times = 1
            if str(args.config.lr_method.warmup_epoch) == 'default':
                if cfg.model.backbone.type == 'ResNeXt':
                    # 默认在3个epoch完成warmup
                    times = 3
                # 默认在1个epoch完成warmup
                cfg.param_scheduler[0].end = end * times
            else:
                # 否则用户可自定义在n个epoch完成warmup
                if int(args.config.lr_method.warmup_epoch) >= args.epoch:
                    raise ValueError("The number of warmup epochs must not exceed the total number of training epochs!")
                else:
                    cfg.param_scheduler[0].end = int(args.config.lr_method.warmup_epoch) * end * times
        # 设置lr衰减方式
        lr_decay_close = False
        if str(args.config.lr_method.decay) != 'default':
            # Linear衰减
            if str(args.config.lr_method.decay) == 'Linear' or str(args.config.lr_method.decay) == 'linear':
                # 如果是'MaskRCNN' 或者 'CascadeRCNN'抛出警告
                if cfg.model.type == 'MaskRCNN' or cfg.model.type == 'CascadeRCNN':
                    warnings.warn('The current default learning rate updater is Linear!', UserWarning)
                else:
                    # RTMDet lr衰减设置为Linear
                    linear_dict = ConfigDict(type='LinearLR', start_factor=1, end_factor=0.01, begin=0, end=args.epoch,
                                             by_epoch=True)
                    if isinstance(cfg.param_scheduler, list):
                        cfg.param_scheduler[1] = linear_dict
                    else:
                        cfg.param_scheduler = linear_dict
            # exp衰减
            elif str(args.config.lr_method.decay) == 'Exp' or str(args.config.lr_method.decay) == 'exp':
                exp_dict = ConfigDict(type='ExponentialLR', gamma=0.9, begin=0, end=args.epoch, by_epoch=True)
                if cfg.model.type == 'RTMDet':
                    exp_dict = ConfigDict(type='ExponentialLR', gamma=0.99, begin=0, end=args.epoch, by_epoch=True)
                if isinstance(cfg.param_scheduler, list):
                    cfg.param_scheduler[1] = exp_dict
                else:
                    cfg.param_scheduler = exp_dict
            # CosineAnnealing衰减
            elif str(args.config.lr_method.decay) == 'CosineAnnealing' or str(args.config.lr_method.decay) == 'Cos':
                if cfg.model.type == 'RTMDet':
                    warnings.warn('The current default learning rate updater is CosineAnnealing!', UserWarning)
                else:
                    cos_dict = ConfigDict(type='CosineAnnealingLR', T_max=args.epoch, begin=args.epoch // 2,
                                          end=args.epoch, by_epoch=True)
                    if isinstance(cfg.param_scheduler, list):
                        cfg.param_scheduler[1] = cos_dict
                    else:
                        cfg.param_scheduler = cos_dict
            # 设置关闭lr衰减
            elif str(args.config.lr_method.decay) == 'close' or str(args.config.lr_method.decay) == 'Close':
                lr_decay_close = True
                close_dict = ConfigDict(type='ConstantLR', factor=1)
                if isinstance(cfg.param_scheduler, list):
                    cfg.param_scheduler[1] = close_dict
                else:
                    cfg.param_scheduler = close_dict
            else:
                raise NotImplementedError('This learning rate updater is not currently supported!')
        # # 设置lr衰减begin和end, 如果lr_decay_close==False，也就是开启lr衰减，才会生效
        if not lr_decay_close:
            # 有预热
            if isinstance(cfg.param_scheduler, list):
                # 单阶段的RTMDet算法，lr衰减是从epoch // 2开始
                if cfg.model.type == 'RTMDet':
                    cfg.param_scheduler[1].begin = args.epoch // 2
                else:
                    # 其余算法：衰减的开始=预热的结束
                    cfg.param_scheduler[1].begin = cfg.param_scheduler[0].end // img_number
                cfg.param_scheduler[1].end = args.epoch
            # 无预热
            else:
                if cfg.model.type != 'RTMDet':
                    cfg.param_scheduler.begin = args.epoch // 2
                else:
                    cfg.param_scheduler.begin = 0
                cfg.param_scheduler.end = args.epoch

        # 设置是否开启模型编译（torch 2.0）
        # if str(args.config.trainer.compile_optionsr) == 'True' or str(args.config.trainer.compile_optionsr) == 'true':
        #     if torch.__version__ >= '2.0.0':
        #         cfg = dict(compile=True)
        #     cfg = dict(compile=True)
        # 设置优化器
        # if str(args.config.trainer.optimizer) != 'default':
        #     cfg.optim_wrapper.optimizer.type = str(args.config.trainer.optimizer)
        # 设置是否开启自动混合精度训练
        if str(args.config.trainer.amp) == "True" or str(args.config.trainer.amp) == "true":
            cfg.optim_wrapper.type = 'AmpOptimWrapper'
        if str(args.config.trainer.accumulative_counts) != 'default':
            if int(args.config.trainer.accumulative_counts) > 1:
                cfg.optim_wrapper['accumulative_counts'] = int(args.config.trainer.accumulative_counts)
            else:
                raise ValueError("The accumulative counts must be greater than 1!")
        if str(args.config.trainer.RepeatDataset) != 'default':
            if isinstance(args.config.trainer.RepeatDataset, int):
                if args.config.trainer.RepeatDataset >= 1:
                    cfg.train_dataloader.dataset.times = int(args.config.trainer.RepeatDataset)
                else:
                    raise ValueError("The RepeatDataset must be equal or greater than 1!")
            else:
                raise ValueError("The RepeatDataset must be an integer!")
        # 设置每块gpu上的进程数（数据读取的总进程数 = num_workers * num_gpus）
        cfg.train_dataloader.num_workers = int(args.config.trainer.num_workers)
        cfg.val_dataloader.num_workers = int(args.config.trainer.num_workers)
        cfg.test_dataloader.num_workers = int(args.config.trainer.num_workers)
        # 设置打印信息间隔
        cfg.default_hooks.logger.interval = int(args.config.trainer.logger_interval)
        # 设置间隔验证参数
        cfg.train_cfg.val_interval = int(args.config.trainer.val_interval)
        # 设置间隔保存模型参数（保存中间模型）
        cfg.default_hooks.checkpoint.interval = int(args.config.trainer.checkpoint_interval)
        cfg.default_hooks.checkpoint.max_keep_ckpts = 3
        # 设置自动保存最优模型
        # 实例分割无中文情况，使用COCO数据集和COCO验证指标
        if cfg.dataset_type == 'SMCocoDataset':
            cfg.default_hooks.checkpoint['save_best'] = 'coco/segm_mAP'
        # 旋转框检测，使用Dota验证指标
        elif cfg.model.type == "mmdet.CascadeRCNN":
            cfg.default_hooks.checkpoint['save_best'] = 'dota/mAP'
        # 实例分割有中文情况，和正框检测均使用VOC验证指标
        else:
            cfg.default_hooks.checkpoint['save_best'] = 'pascal_voc/mAP'
        cfg.default_hooks.checkpoint['rule'] = 'greater'

        return cfg

    def download_bb_from_url(self, args, cfg, assign=False):
        """下载backbone"""
        print("Specified checkpoint file not found, downloading from the network!")

        if not assign:
            backbone_file_path = os.path.join(os.getcwd(), '..', 'backbone')
            if not os.path.exists(backbone_file_path):
                os.makedirs(backbone_file_path, exist_ok=True)
        else:
            if not os.path.exists(os.path.dirname(cfg.checkpoint)):
                os.makedirs(os.path.dirname(cfg.checkpoint), exist_ok=True)

        if args.config.model.backbone_name == "r-18":
            download_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'resnet18-5c106cde.pth')
        elif args.config.model.backbone_name == "r-50":
            download_url = 'https://download.pytorch.org/models/resnet50-19c8e357.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'resnet50-19c8e357.pth')
        elif args.config.model.backbone_name == "r-101":
            download_url = 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'resnet101-5d3b4d8f.pth')
        elif args.config.model.backbone_name == "x101-32x8d":
            download_url = 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'resnext101_32x8d-8ba56ff5.pth')
        elif args.config.model.backbone_name == "re-r50":
            # download_url = 'https://drive.google.com/uc?id=1FshfREfLZaNl5FcaKrH0lxFyZt50Uyu2&export=download'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 're_resnet50_c8_batch256-25b16846.pth')
            else:
                raise FileNotFoundError(
                    'Please download the “SuperMap iObjects Python Machine Learning Resources” and install it in the correct path.'
                )
        elif args.config.model.backbone_name == "re-r101":
            # download_url = 'https://drive.google.com/uc?id=1GmJzzHRgp5SvmGa6uj6n4GpCuYRT5RE9&export=download'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 're_resnet101_v2_c8_batch256-f248cc41.pth')
            else:
                raise FileNotFoundError(
                    'Please download the “SuperMap iObjects Python Machine Learning Resources” and install it in the correct path.'
                )
        elif args.config.model.backbone_name == "cspnext-tiny":
            download_url = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/cspnext_rsb_pretrain/cspnext-tiny_imagenet_600e.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'cspnext-tiny_imagenet_600e.pth')
        elif args.config.model.backbone_name == "cspnext-s":
            download_url = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/cspnext_rsb_pretrain/cspnext-s_imagenet_600e.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'cspnext-s_imagenet_600e.pth')
        elif args.config.model.backbone_name == "cspnext-m":
            download_url = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/cspnext_rsb_pretrain/cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'cspnext-m_8xb256-rsb-a1-600e_in1k-ecb3bbd9.pth')
        elif args.config.model.backbone_name == "cspnext-l":
            download_url = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/cspnext_rsb_pretrain/cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'cspnext-l_8xb256-rsb-a1-600e_in1k-6a760974.pth')
        elif args.config.model.backbone_name == "cspnext-x":
            download_url = 'https://download.openmmlab.com/mmdetection/v3.0/rtmdet/cspnext_rsb_pretrain/cspnext-x_8xb256-rsb-a1-600e_in1k-b3f78edd.pth'
            if not assign:
                cfg.checkpoint = os.path.join(backbone_file_path, 'cspnext-x_8xb256-rsb-a1-600e_in1k-b3f78edd.pth')

        urllib.request.urlretrieve(download_url, cfg.checkpoint)

    def update_mmdet_config_from_args(self, cfg, args):
        """
        从接口参数更新cfg
        """
        # 修改数据集类型以及路径
        cfg = update_dataset_config(cfg, args)

        # 设置学习率初始化，如果上层接口（用户）设置了lr，则关闭
        if args.lr is not None:
            cfg.auto_scale_lr.enable = False
            if isinstance(cfg.param_scheduler, list):
                if 'eta_min' in cfg.param_scheduler[1]:
                    if args.lr < cfg.param_scheduler[1].eta_min:
                        warnings.warn("Detected that the specified lr is lower than the algorithm's minimum lr. "
                                      "Initializing to the algorithm's minimum lr!", UserWarning)
                    else:
                        cfg.optim_wrapper.optimizer.lr = args.lr
                else:
                    cfg.optim_wrapper.optimizer.lr = args.lr
            else:
                if 'eta_min' in cfg.param_scheduler:
                    if args.lr < cfg.param_scheduler.eta_min:
                        warnings.warn("Detected that the specified lr is lower than the algorithm's minimum lr. "
                                      "Initializing to the algorithm's minimum lr!", UserWarning)
                    else:
                        cfg.optim_wrapper.optimizer.lr = args.lr
                else:
                    cfg.optim_wrapper.optimizer.lr = args.lr
        else:
            cfg.auto_scale_lr.enable = True

        # 加载pth文件(优先级pretrained_model_path > backbone_weight_path)
        if args.config.model.backbone_file is not None:
            cfg.checkpoint = os.path.join(os.path.abspath(os.path.dirname(os.getcwd())), 'backbone',
                                          str(args.config.model.backbone_file))
            if cfg.model.type == 'RTMDet':
                init_cfg = ConfigDict(type='Pretrained', prefix='backbone.', checkpoint=cfg.checkpoint)
            else:
                init_cfg = ConfigDict(type='Pretrained', checkpoint=cfg.checkpoint)
            cfg.model.backbone.init_cfg = init_cfg
        if args.backbone_weight_path is not None:
            cfg.checkpoint = args.backbone_weight_path
            if cfg.model.type == 'RTMDet':
                init_cfg = ConfigDict(type='Pretrained', prefix='backbone.', checkpoint=cfg.checkpoint)
            else:
                init_cfg = ConfigDict(type='Pretrained', checkpoint=cfg.checkpoint)
            cfg.model.backbone.init_cfg = init_cfg
        if args.pretrained_model_path is not None:
            file_name, file_extension = os.path.splitext(args.pretrained_model_path)
            if file_extension == '.pth':
                cfg.load_from = args.pretrained_model_path
            elif file_extension == '.sdm':
                pretrained_model_path = file_name + '.pth'
                cfg.load_from = pretrained_model_path
            else:
                raise FileNotFoundError("Failed to load the pre-trained model")
        else:
            if 'checkpoint' in cfg:
                # 如果用户指定了一个骨干网络，它却不存在，就去下载
                if not os.path.exists(cfg.checkpoint):
                    self.download_bb_from_url(args, cfg, assign=True)
            else:
                # 如果用户在接口和sdt文件都没有指定骨干网络，就去下载到backbone文件夹里
                self.download_bb_from_url(args, cfg, assign=False)
                if cfg.model.type == 'RTMDet':
                    init_cfg = ConfigDict(type='Pretrained', prefix='backbone.', checkpoint=cfg.checkpoint)
                else:
                    init_cfg = ConfigDict(type='Pretrained', checkpoint=cfg.checkpoint)
                cfg.model.backbone.init_cfg = init_cfg

        # 上层传入的bs数目为总bs，需要除以gpus
        if args.batch_size is not None:
            if args.gpus != [] and args.gpus != [-1]:
                if len(args.gpus) > args.batch_size:
                    raise ValueError("The number if gpus must be smaller than batch size")
                else:
                    cfg.train_dataloader.batch_size = args.batch_size // len(args.gpus)
                    cfg.val_dataloader.batch_size = cfg.train_dataloader.batch_size
                    cfg.test_dataloader.batch_size = cfg.train_dataloader.batch_size
            else:
                cfg.train_dataloader.batch_size = args.batch_size
                cfg.val_dataloader.batch_size = args.batch_size
                cfg.test_dataloader.batch_size = args.batch_size
                cfg.optim_wrapper.type = 'OptimWrapper'

        cfg.work_dir = args.log_path
        if args.epoch is not None:
            cfg.max_epochs = args.epoch
            cfg.train_cfg.max_epochs = args.epoch

            if isinstance(cfg.param_scheduler, list):
                if cfg.param_scheduler[1].type != 'ConstantLR':
                    cfg.param_scheduler[1].end = args.epoch
            else:
                if cfg.param_scheduler.type != 'ConstantLR':
                    cfg.param_scheduler.end = args.epoch
        cfg = replace_num_classes(cfg, len(args.class_names))

        cfg.custom_imports = dict(
            imports=['iobjectspy.ml.vision._models.object_detection._mmdet.data.od',
                     'iobjectspy.ml.vision._models.object_detection._mmdet.data.transformers.loading'],
            allow_failed_imports=False)
        # 随机种子
        # cfg.device = args.device
        # args.seed = 42
        # seed = init_random_seed(args.seed, device=cfg.device)
        # args.diff_seed = None
        # seed = seed + dist.get_rank() if args.diff_seed else seed
        # # logger.info(f'Set random seed to {seed}, '
        # #             f'deterministic: {args.deterministic}')
        # args.deterministic = False
        # set_random_seed(seed, deterministic=args.deterministic)
        # cfg.seed = seed
        return cfg

    def _get_model_type(self, model_name):
        """
        确定模型类型和标准化模型名称
        """
        INSTANCE_SEGMENTATION_MODELS = {'mask_rcnn', 'rtmdet-ins'}
        OBJECT_DETECTION_MODELS = {'cascade_rcnn', 'rtmdet', 'redet'}

        if model_name in INSTANCE_SEGMENTATION_MODELS:
            return model_name, 'instance_segmentation'
        elif model_name in OBJECT_DETECTION_MODELS:
            return model_name, 'object_detection'
        else:
            raise NotImplementedError("The current version does not support the specified algorithm!")

    def _get_model_config(self, model_name, model_type):
        """
        根据模型名称和类型获取对应的配置
        """
        if model_type == 'instance_segmentation':
            return instance_segmentation_config.get(model_name)
        else:
            return object_detection_config.get(model_name)

    def _validate_backbone(self, model_name, backbone_name, model_configs, task_type):
        """
        验证backbone是否支持
        """
        if backbone_name not in model_configs:
            raise NotImplementedError(
                f"The {model_name} ({task_type}) does not support the selected backbone '{backbone_name}'. "
                "Please check your config file!")

    def _handle_multiscale_warning(self, model_name, backbone_name, is_multiscale):
        """
        处理多尺度训练的警告
        """
        if model_name == 'mask_rcnn' and backbone_name == 'r-18' and is_multiscale:
            warnings.warn('Detected the backbone network as ResNet-18, '
                          'which does not support multi-scale training!', UserWarning)

    def _get_config_file(self, model_name, backbone_name, model_configs, is_multiscale):
        """
        获取具体的配置文件
        """
        if model_name == 'mask_rcnn':
            return model_configs[backbone_name][is_multiscale] if isinstance(
                model_configs[backbone_name], dict) else model_configs[backbone_name]
        return model_configs[backbone_name]

    def find_config_with_bb(self, args, model_config_path, inference=False):
        """
        主函数：查找并返回模型配置文件路径
        """
        # 获取基本参数
        model_name = str(args.config.model.name)
        backbone_name = str(args.config.model.backbone_name)
        is_multiscale = str(args.config.trainer.multiscale).lower() == 'true'

        # 确定模型类型
        model_name, model_type = self._get_model_type(model_name)

        # 获取模型配置
        model_configs = self._get_model_config(model_name, model_type)

        # 验证backbone
        self._validate_backbone(model_name, backbone_name, model_configs, model_type)

        # 处理多尺度警告
        self._handle_multiscale_warning(model_name, backbone_name, is_multiscale)

        # 获取配置文件
        config_file = self._get_config_file(model_name, backbone_name, model_configs, is_multiscale)

        # 返回完整路径
        return os.path.join(model_config_path, config_file)

    def update_mmdet_config(self, args):
        """更新MMDet配置"""
        from mmengine.config import Config

        # 继续训练：读取log下面的py文件
        if args.reload_model:
            resume_config_name = glob.glob(os.path.join(args.log_path, '**', '*.py'), recursive=True)
            cfg = Config.fromfile(resume_config_name[0])
            cfg.resume = True
            cfg.load_from = None
        # 初次训练：根据sdt读取组件内的config
        else:
            total_config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), '../_mmdet', "configs")
            model_config_path = os.path.join(total_config_path, str(args.config.model.name))

            if os.path.exists(model_config_path):
                # 根据backbone定位算法config并解析，然后通过sdt和args更新
                model_config = self.find_config_with_bb(args, model_config_path)
                cfg = Config.fromfile(model_config)
                cfg.train_dataloader.dataset['model_type'] = cfg.model.type
                cfg.val_dataloader.dataset['model_type'] = cfg.model.type
                if args.input_bandnum != 3:
                    if cfg.model.type == 'RTMDet':
                        cfg.train_dataloader.dataset.pipeline[6].pad_val.img = tuple(
                            114 for _ in range(args.input_bandnum))
                        # cfg.train_dataloader.dataset.pipeline[8].pad_val = tuple(114 for _ in range(args.input_bandnum))
                        cfg.val_dataloader.dataset.pipeline[2].pad_val.img = tuple(
                            114 for _ in range(args.input_bandnum))
                cfg = self.update_mmdet_config_from_sdt(cfg, args)
                cfg = self.update_mmdet_config_from_args(cfg, args)
            else:
                raise NotImplementedError(
                    "The current version does not support the specified algorithm!")

        return cfg

    def _train_process(self, args):
        """训练流程"""
        # 原main_train的代码
        cfg = self.update_mmdet_config(args)
        saving_latest_model_sdl(args, cfg)

        from mmdet.utils.setup_env import setup_cache_size_limit_of_dynamo
        setup_cache_size_limit_of_dynamo()

        if cfg.model.type == 'mmdet.CascadeRCNN':
            from mmrotate.utils import register_all_modules
            register_all_modules(init_default_scope=False)

        if len(args.gpus) > 1:
            cfg.launcher = 'pytorch'
            if platform.system() == 'Windows':
                cfg.env_cfg.dist_cfg.backend = 'gloo'

        from mmengine.registry import RUNNERS
        from mmengine.runner import Runner

        runner = Runner.from_cfg(cfg) if 'runner_type' not in cfg else RUNNERS.build(cfg)
        runner.train()

    def _launch_distributed(self, rank, world_size, args):
        """启动分布式训练"""
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '12355'
        os.environ['RANK'] = str(rank)
        os.environ['LOCAL_RANK'] = str(rank)
        os.environ['WORLD_SIZE'] = str(world_size)
        self._train_process(args)

    def train(self):
        """训练入口函数"""
        # 训练过程
        if len(self.args.gpus) == 1 or (self.args.gpus == [-1]) or (self.args.gpus == []):
            self._train_process(self.args)
        else:
            world_size = len(self.args.gpus)
            torch.multiprocessing.spawn(
                self._launch_distributed,
                args=(world_size, self.args),
                nprocs=world_size,
            )

        # 保存模型
        model_path = creat_output_model_folder(self.args.output_model_path, self.args.output_model_name)

        pic_names = os.listdir(os.path.join(self.args.train_data_path, 'Images'))
        im = Image.open(os.path.join(self.args.train_data_path, 'Images', pic_names[0]))
        blocksize = im.size[0]

        saving_mmdet_model(self.args.log_path, model_path, blocksize, self.args.class_names, self.args.config)
        print('model saved in dir : {}'.format(model_path))
        torch.cuda.empty_cache()
