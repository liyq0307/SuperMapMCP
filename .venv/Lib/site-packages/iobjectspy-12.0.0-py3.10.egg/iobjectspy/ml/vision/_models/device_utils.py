import torch
from typing import Union
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from multiprocessing import Process, Value

def check_cuda(flag):
    flag.value = torch.cuda.is_available()

def is_cuda_available() -> bool:
    flag = Value('b', False)  # 'b' 表示布尔类型
    p = Process(target=check_cuda, args=(flag,))
    p.start()
    p.join()
    return flag.value

def is_npu_available() -> bool:
    """Return True if npu devices exist."""
    try:
        import torch
        import torch_npu
        return (hasattr(torch, 'npu') and torch.npu.is_available())
    except Exception:
        return False

def is_musa_available() -> bool:
    """Return True if musa devices exist."""
    try:
        import torch
        import torch_musa
        return (hasattr(torch, 'musa') and torch.musa.is_available())
    except Exception:
        return False

def import_device_plugins():
    if is_npu_available():
        import torch_npu
    elif is_musa_available():
        import torch_musa

def _npu_before_train():
    import torch_npu
    torch_npu.npu.set_compile_mode(jit_compile=False)
    option = {"ACL_DEBUG_DIR": "/tmp"}
    torch.npu.set_option(option)

def _musa_before_train():
    import torch_musa
    try:
        torch.backends.mudnn.allow_tf32 = True
    except:
        pass

class Device_type:
    _device = None
    _set_device = None
    _backend = None
    _empty_cache = None
    _count = None
    _dataparallel = None
    _before_train = None

    @staticmethod
    def setup(device=None):
        if (device is None or device == 'npu') and is_npu_available():
            import torch_npu
            Device_type._device = 'npu'
            Device_type._set_device = torch.npu.set_device
            Device_type._backend = 'hccl'
            Device_type._empty_cache = torch.npu.empty_cache
            Device_type._count = torch.npu.device_count
            Device_type._dataparallel = lambda model, device_ids: model.to(f'npu:{device_ids[0]}')
            Device_type._before_train = _npu_before_train
            Device_type.env_str = 'ASCEND_RT_VISIBLE_DEVICES'
        elif (device is None or device == 'musa') and is_musa_available():
            import torch_musa
            Device_type._device = 'musa'
            Device_type._set_device = torch.musa.set_device
            Device_type._backend = 'mccl'
            Device_type._empty_cache = torch.musa.empty_cache
            Device_type._count = torch.musa.device_count
            Device_type._dataparallel = torch.nn.DataParallel
            Device_type._before_train = _musa_before_train
            Device_type.env_str = 'MUSA_VISIBLE_DEVICES'
        elif (device is None or device == 'cuda') and is_cuda_available():
            import platform
            if platform.system() == 'Windows':
                Device_type._backend = 'gloo'
            else:
                Device_type._backend = 'nccl'
            Device_type._device = 'cuda'
            Device_type._set_device = torch.cuda.set_device
            Device_type._empty_cache = torch.cuda.empty_cache
            Device_type._count = torch.cuda.device_count
            Device_type._dataparallel = torch.nn.DataParallel
            Device_type._before_train = lambda: None
            Device_type.env_str = 'CUDA_VISIBLE_DEVICES'
        else:
            Device_type._device = 'cpu'
            Device_type._set_device = lambda device_id: None
            Device_type._count = lambda: 0
            Device_type._dataparallel = lambda model, device_ids: model
            Device_type._before_train = lambda: None
            Device_type.env_str = 'CUDA_VISIBLE_DEVICES'

    @staticmethod
    def get_device() -> torch.device:
        return torch.device(Device_type._device)

    @staticmethod
    def init_dist(ip='tcp://127.0.0.1:6789', world_size=1, rank=0) -> None:
        Device_type._set_device(rank)
        dist.init_process_group(backend=Device_type._backend, init_method=ip, world_size=world_size,
                                rank=rank)

    @staticmethod
    def empty_cache() -> None:
        Device_type._empty_cache()

    @staticmethod
    def device_count() -> int:
        return Device_type._count()

    @staticmethod
    def dataparallel(model, device_ids) -> torch.nn.Module:
        if Device_type._device == 'cuda':
            return Device_type._dataparallel(Device_type.to_device(model))
        else:
            return Device_type._dataparallel(Device_type.to_device(model), device_ids=device_ids)

    @staticmethod
    def distributed(model, device_ids, find_unused_parameters=True) -> torch.nn.Module:
        if Device_type._device != 'musa':
            model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
        Device_type._set_device(device_ids)
        return DDP(Device_type.to_device(model), device_ids=[device_ids], find_unused_parameters=find_unused_parameters)

    @staticmethod
    def to_device(model, device_id=None) -> Union[torch.nn.Module, torch.Tensor]:
        if device_id is not None and Device_type._device != 'cuda':
            Device_type._set_device(device_id)
        return model.to(f'{Device_type._device}')

    @staticmethod
    def before_train() -> None:
        Device_type._before_train()
