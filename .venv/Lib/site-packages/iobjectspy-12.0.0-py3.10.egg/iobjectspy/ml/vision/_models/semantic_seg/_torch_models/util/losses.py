import torch

import torch.nn as nn

from . import base
from . import functional as F
from .base import Activation
from torch import Tensor


class JaccardLoss(base.Loss):

    def __init__(self, eps=1., activation=None, ignore_channels=None, **kwargs):
        super().__init__(**kwargs)
        self.eps = eps
        self.activation = Activation(activation)
        self.ignore_channels = ignore_channels

    def forward(self, y_pr, y_gt):
        y_pr = self.activation(y_pr)
        return 1 - F.jaccard(
            y_pr, y_gt,
            eps=self.eps,
            threshold=None,
            ignore_channels=self.ignore_channels,
        )


# class DiceLoss(base.Loss):
#
#     def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):
#         super().__init__(**kwargs)
#         self.eps = eps
#         self.beta = beta
#         self.activation = Activation(activation)
#         self.ignore_channels = ignore_channels
#
#     def forward(self, y_pr, y_gt):
#         # y_pr = self.activation(y_pr)
#         # ph, pw = y_pr.size(2), y_pr.size(3)
#         # h, w = y_gt.size(1), y_gt.size(2)
#         # if ph != h or pw != w:
#         #     y_pr = nn.functional.upsample(
#         #         input=y_pr, size=(h, w), mode='bilinear')
#         return 1 - F.f_score(
#             y_pr, y_gt,
#             beta=self.beta,
#             eps=self.eps,
#             threshold=None,
#             ignore_channels=self.ignore_channels,
#         )

# class NLLLoss(base.Loss):
#     def __init__(self, weight=None, size_average=True):
#         super(NLLLoss, self).__init__()
#         self.nll_loss = nn.CrossEntropyLoss(weight, size_average)
#
#     def forward(self, inputs, targets):
#         logits_2d = inputs.view(inputs.size()[0], inputs.size()[1], -1)
#         targets=torch.argmax(targets, dim=1)
#         labels_2d = targets.view(targets.size()[0], -1)
#         return self.nll_loss(logits_2d, labels_2d.long())

class CrossEntropyLoss(base.Loss):
    def __init__(self, weight=None, ignore_index=255):
        # ignore_index，忽略标签为255的像素，不参与loss和梯度计算
        super(CrossEntropyLoss, self).__init__()
        self.crossentropyloss = nn.CrossEntropyLoss(weight, ignore_index=ignore_index)

    def forward(self, inputs, targets):
        # logits_2d = inputs.view(inputs.size()[0], inputs.size()[1], -1)
        # targets = torch.argmax(targets, dim=1)
        # labels_2d = targets.view(targets.size()[0], -1)
        # return self.crossentropyloss(logits_2d, labels_2d.long())
        return self.crossentropyloss(inputs, targets)

    pass


class L1Loss(nn.L1Loss, base.Loss):
    pass


class MSELoss(nn.MSELoss, base.Loss):
    pass


# class CrossEntropyLoss(nn.CrossEntropyLoss, base.Loss):
#     pass


# class NLLLoss(nn.NLLLoss, base.Loss):
#     pass

class BCELoss(nn.BCELoss, base.Loss):
    pass


# class BCELoss(base.Loss):
#     def __init__(self):
#         super(BCELoss, self).__init__()
#         self.loss = nn.BCELoss()
#     def forward(self, y_pr, y_gt):
#
#         return self.loss(y_pr, y_gt)


class BCEWithLogitsLoss(nn.BCEWithLogitsLoss, base.Loss):
    pass


class HrnetCrossEntropy(base.Loss):
    def __init__(self, ignore_label=-1, weight=None):
        super(HrnetCrossEntropy, self).__init__()
        self.ignore_label = ignore_label
        self.criterion = nn.CrossEntropyLoss(weight=weight,
                                             ignore_index=ignore_label)

    def forward(self, score, target):
        ph, pw = score.size(2), score.size(3)
        h, w = target.size(1), target.size(2)
        if ph != h or pw != w:
            score = nn.functional.upsample(
                input=score, size=(h, w), mode='bilinear')

        loss = self.criterion(score, target)

        return loss


class HrnetOhemCrossEntropy(base.Loss):
    def __init__(self, ignore_label=-1, thres=0.7,
                 min_kept=100000, weight=None):
        super(HrnetOhemCrossEntropy, self).__init__()
        self.thresh = thres
        self.min_kept = max(1, min_kept)
        self.ignore_label = ignore_label
        self.criterion = nn.CrossEntropyLoss(weight=weight,
                                             ignore_index=ignore_label,
                                             reduction='none')

    def forward(self, score, target, **kwargs):
        ph, pw = score.size(2), score.size(3)
        h, w = target.size(1), target.size(2)
        if ph != h or pw != w:
            score = nn.functional.upsample(input=score, size=(h, w), mode='bilinear')
        pred = F.softmax(score, dim=1)
        pixel_losses = self.criterion(score, target).contiguous().view(-1)
        mask = target.contiguous().view(-1) != self.ignore_label

        tmp_target = target.clone()
        tmp_target[tmp_target == self.ignore_label] = 0
        pred = pred.gather(1, tmp_target.unsqueeze(1))
        pred, ind = pred.contiguous().view(-1, )[mask].contiguous().sort()
        min_value = pred[min(self.min_kept, pred.numel() - 1)]
        threshold = max(min_value, self.thresh)

        pixel_losses = pixel_losses[mask][ind]
        pixel_losses = pixel_losses[pred < threshold]
        return pixel_losses.mean()


class DiceJaccardLoss(base.Loss):

    def __init__(self, eps=1., beta=1., activation=None, ignore_channels=None, **kwargs):
        super().__init__(**kwargs)
        self.eps = eps
        self.beta = beta
        self.activation = Activation(activation)
        self.ignore_channels = ignore_channels

    def forward(self, y_pr, y_gt):
        # y_pr = self.activation(y_pr)
        # ph, pw = y_pr.size(2), y_pr.size(3)
        # h, w = y_gt.size(1), y_gt.size(2)
        # if ph != h or pw != w:
        #     y_pr = nn.functional.upsample(
        #         input=y_pr, size=(h, w), mode='bilinear')
        return 1 - F.f_score(
            y_pr, y_gt,
            beta=self.beta,
            eps=self.eps,
            threshold=None,
            ignore_channels=self.ignore_channels,
        ) + 1 - F.jaccard(
            y_pr, y_gt,
            eps=self.eps,
            threshold=None,
            ignore_channels=self.ignore_channels,
        )


def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):
    # Average of Dice coefficient for all batches, or for a single mask
    assert input.size() == target.size()
    if input.dim() == 2 and reduce_batch_first:
        raise ValueError(f'Dice: asked to reduce batch but got tensor without batch dimension (shape {input.shape})')

    if input.dim() == 2 or reduce_batch_first:
        inter = torch.dot(input.reshape(-1), target.reshape(-1))
        sets_sum = torch.sum(input) + torch.sum(target)
        if sets_sum.item() == 0:
            sets_sum = 2 * inter

        return (2 * inter + epsilon) / (sets_sum + epsilon)
    else:
        # compute and average metric for each batch element
        dice = 0
        for i in range(input.shape[0]):
            dice += dice_coeff(input[i, ...], target[i, ...])
        return dice / input.shape[0]


def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon=1e-6):
    # Average of Dice coefficient for all classes
    assert input.size() == target.size()
    dice = 0
    for channel in range(input.shape[1]):
        dice += dice_coeff(input[:, channel, ...], target[:, channel, ...], reduce_batch_first, epsilon)

    return dice / input.shape[1]


class DiceLoss(base.Loss):
    def __init__(self, multiclass=False, **kwargs):
        super().__init__(**kwargs)
        # super(dice_loss, self).__init__()
        self.multiclass = multiclass
        # self.bce = nn.BCELoss()

    def forward(self, input: Tensor, target: Tensor):
        # Dice loss (objective to minimize) between 0 and 1
        assert input.size() == target.size()
        fn = multiclass_dice_coeff if self.multiclass else dice_coeff
        return 1 - fn(input, target, reduce_batch_first=True)


class BCLLoss(base.Loss):
    """
    batch-balanced contrastive loss
    no-change 1
    change -1
    """

    def __init__(self, margin=2.0):
        super(BCLLoss, self).__init__()
        self.margin = margin

    def forward(self, distance: Tensor, label: Tensor):
        label[label == 1] = -1  # change
        label[label == 0] = 1  # no change
        label = label.unsqueeze(1)
        label[label == 255] = 1
        mask = (label != 255).float()
        distance = distance * mask
        pos_num = torch.sum((label == 1).float()) + 0.0001
        neg_num = torch.sum((label == -1).float()) + 0.0001

        loss_1 = torch.sum((1 + label) / 2 * torch.pow(distance, 2)) / pos_num
        loss_2 = torch.sum(
            (1 - label) / 2 * mask * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2)) / neg_num
        loss = loss_1 + loss_2
        return loss


class MaskClsLoss(base.Loss):
    def __init__(self):
        super(MaskClsLoss, self).__init__()
        self.loss = nn.Identity()

    def forward(self, input: Tensor, target: Tensor):
        return self.loss(input)


def get_loss(loss_type, **kwargs):
    loss_list = loss_type.split('+')
    loss_list = [l.lower().replace('_', '') for l in loss_list]
    loss_fs = []
    for l in loss_list:
        if l == 'diceloss':
            loss_fs.append(DiceLoss)
        elif l == 'jaccardloss':
            loss_fs.append(JaccardLoss)
        elif l == 'l1loss':
            loss_fs.append(L1Loss)
        elif l == 'bceloss':
            loss_fs.append(BCELoss)
        elif l == 'crossentropyloss':
            loss_fs.append(CrossEntropyLoss)
        elif l == 'bcewithlogitsloss':
            loss_fs.append(BCEWithLogitsLoss)
        elif l == 'dicejaccardloss':
            loss_fs.append(DiceJaccardLoss)
        elif l == 'bclloss':
            loss_fs.append(BCLLoss)
        elif l == 'maskclsloss':
            loss_fs.append(MaskClsLoss)
    all_loss = None
    for l in loss_fs:
        if all_loss is not None:
            all_loss = all_loss + l(**kwargs)
        else:
            all_loss = l(**kwargs)
    return all_loss
