import os
import shutil
import tempfile
import time
from collections import OrderedDict
from typing import Union
import torch

from iobjectspy.ml.vision._models.semantic_seg.tools.config import ConfigLoader
from iobjectspy.ml.toolkit._toolkit import save_config_to_yaml
from iobjectspy._logger import log_error, log_info, log_warning


class BaseTrainer:
    def __init__(self, **kwargs):
        self.callbacks = []
        self.loss = []
        self.acc = []
        self.val_loss = []
        self.val_acc = []
        self.model_architecture = None

    def _main_process(self):
        """
        判断是否是分布式训练，或是否是主进程
        :return: True or None
        """
        return not self.multiprocessing_distributed or (self.multiprocessing_distributed and self.Rank == 0)

    def init_attribute(self, **kwargs):
        """
        初始化训练过程所需的属性。

        该方法根据传入的参数配置初始化多个属性，包括训练配置、模型结构、数据路径等。它会从 `kwargs` 中获取配置信息，
        并使用这些信息初始化模型的参数、训练配置、数据路径等。若未提供某些配置参数，则使用默认值或从文件中加载配置。

        :param kwargs: 配置参数，包含路径、模型设置、训练配置等。
        :return: None
        """
        output_model_path = kwargs.get('output_model_path', '../api/')
        self.output_model_path_base = output_model_path

        self.trainer_config_path = kwargs.get('config_path')
        self.sdt_trainer_config = ConfigLoader(self.trainer_config_path)
        if kwargs.get("backbone_name", None):
            self.sdt_trainer_config.model.backbone_name = kwargs.get("backbone_name")
        if kwargs.get("backbone_weight_path", None):
            self.sdt_trainer_config.model.backbone_file = kwargs.get("backbone_weight_path")
        if kwargs.get("epoch", None):
            self.sdt_trainer_config.trainer.num_epochs = kwargs.get("epoch")
        if kwargs.get("batch_size", None):
            self.sdt_trainer_config.trainer.batch_size = kwargs.get("batch_size")
        if kwargs.get("lr", None):
            self.sdt_trainer_config.trainer.learning_rate = kwargs.get("lr")

        self.model_architecture = self.sdt_trainer_config.model.name
        self.train_data_path = kwargs.get('train_data_path')
        self.writers = None

        self.sda_path = os.path.join(self.train_data_path, os.path.basename(self.train_data_path) + '.sda')
        self.sda_data_config = ConfigLoader(self.sda_path)
        if self.sda_data_config.dataset.data_type.strip() == 'multi_classification':
            self.output_bands = len(self.sda_data_config.dataset.class_type)
            log_info("Read category output dimension from sda file")
        elif self.sda_data_config.dataset.data_type.strip() == 'general_change_detection':
            self.output_bands = 1
        elif self.sda_data_config.dataset.data_type.strip() == 'binary_classification':
            self.output_bands = 1
        else:
            log_error('data_type should be multi_classification or binary_classification or general_change_detection')
            raise Exception(
                'data_type should be multi_classification or binary_classification or general_change_detection')
        self.activation = 'sigmoid' if self.output_bands == 1 else None

        self.model_base_name = os.path.basename(output_model_path.rstrip(os.path.sep))

        self.encoder_weights = kwargs.get('encoder_weights', 'imagenet')

        self.log_path = kwargs.get('log_path')
        self.sdl_path = os.path.join(self.log_path, 'latest.sdl')  # 父类调用

        self.torch_model_path = None

        self.start_epoch = 0

        self.sdt_trainer_config.trainer.optimizer = kwargs.get('optimizer', self.sdt_trainer_config.trainer.optimizer)

    def train(self):
        pass

    def init_callbacks(self, log_path=None):
        """
        创建训练日志和模型检查点保存路径。

        该方法根据传入的 `log_path` 配置训练日志和模型检查点的保存路径。如果提供了 `log_path`，则使用该路径创建
        训练日志和检查点保存路径。如果未提供 `log_path`，则默认使用 `'experiments'` 目录，并基于当前日期和应用名称
        创建相应的子目录。此方法还会确保创建这些目录（如果它们不存在）。

        :param log_path: 日志保存路径，如果提供，则使用该路径创建子目录；否则，使用默认路径。
        :return: None
        """
        if log_path:
            self.sdt_trainer_config.trainer.callbacks.tensorboard_log_dir = os.path.join(log_path, "logs")
            self.sdt_trainer_config.trainer.callbacks.checkpoint_dir = os.path.join(log_path, "checkpoints")
        else:
            self.log_path = 'experiments'
            self.sdt_trainer_config.trainer.callbacks.tensorboard_log_dir = (
                os.path.join("experiments", time.strftime("%Y-%m-%d", time.localtime()),
                             self.sdt_trainer_config.application.name, "logs"))
            self.sdt_trainer_config.trainer.callbacks.checkpoint_dir = (
                os.path.join("experiments", time.strftime("%Y-%m-%d", time.localtime()),
                             self.sdt_trainer_config.application.name, "checkpoints"))

        if self._main_process():
            if os.path.exists(self.sdt_trainer_config.trainer.callbacks.tensorboard_log_dir) is not True:
                os.makedirs(self.sdt_trainer_config.trainer.callbacks.tensorboard_log_dir, exist_ok=True)
            else:
                log_info('tensorboard log directory already existed')
            if os.path.exists(self.sdt_trainer_config.trainer.callbacks.checkpoint_dir) is not True:
                os.makedirs(self.sdt_trainer_config.trainer.callbacks.checkpoint_dir, exist_ok=True)
            else:
                log_info('checkpoint directory already existed')

    def create_output_model_path(self, output_model_folder, output_model_name):
        """
        创建用于保存训练输出的 `.pth` 和 `.sdm` 文件的文件夹。

        该方法根据传入的文件夹路径（`output_model_folder`）和文件名（`output_model_name`）生成一个用于保存模型的
        路径。如果指定的路径已存在，它会通过附加数字后缀来避免覆盖现有目录。最后，返回用于保存模型的最终路径。

        :param output_model_folder: 存储模型的文件夹路径。
        :param output_model_name: 模型的文件名（不包含扩展名）。
        :return: 用于保存模型的完整路径。
        """
        output_model_path = os.path.join(output_model_folder, output_model_name)
        base_output_model_path = output_model_path
        file_num = 1
        while (os.path.exists(output_model_path) and
               os.path.exists(os.path.join(output_model_path,
                                           os.path.basename(output_model_path.rstrip(os.path.sep)) + '.sdm'))):
            output_model_path = base_output_model_path + ('_' + str(file_num))
            file_num += 1
        if self._main_process():
            if not os.path.exists(output_model_path):
                os.makedirs(output_model_path)

        return output_model_path

    def _save_model(self, i, score, **kwargs):
        """
        保存模型权重文件，支持普通保存和加密保存两种模式。

        该方法用于保存训练过程中模型的权重。根据配置，模型可以保存为普通的 `.pth` 文件，或者加密后保存为 `.pim` 文件。
        它会首先保存一个临时文件，然后根据保存模式将文件复制到目标目录，并清理临时文件。

        :param i: 当前训练的轮次（epoch）。
        :param score: 当前模型的评分，通常是验证阶段的得分（如 IoU 或其他评估指标）。
        :param kwargs: 额外的参数，可能包含模型副本、加密模式等。
        :return: None
        """
        file_name = '{}-{:0=4}-{:.4f}{}'.format(
            self.sdt_trainer_config.application.name,
            i,
            score,
            self.file_ext)

        temp_dir = tempfile.mkdtemp()
        temp_checkpoint = os.path.join(temp_dir, file_name)

        try:
            # 更新模型状态
            self.model_clone = self.load_state_dict(self.model_clone, self.get_state_dict(self.model))

            # 保存临时文件
            torch.save(self.model_clone, temp_checkpoint)

            # 设置目标路径
            checkpoint_dir = self.sdt_trainer_config.trainer.callbacks.checkpoint_dir
            check_save_name = os.path.join(checkpoint_dir, file_name)
            save_path = self.torch_model_path

            if self.pim:
                check_save_name = check_save_name.replace('.pth', '.pim')
                self._save_encrypted_model(temp_checkpoint, save_path, check_save_name)
            else:
                shutil.copy(temp_checkpoint, save_path)
                shutil.copy(save_path, check_save_name)

            log_info(f'Model save path: {save_path} and {check_save_name}')
        except Exception as e:
            log_error(f"Failed to save model: {e}")
        finally:
            # 安全清理临时文件和目录
            try:
                if os.path.exists(temp_checkpoint):
                    os.remove(temp_checkpoint)
                if os.path.exists(temp_dir):
                    try:
                        os.rmdir(temp_dir)
                    except OSError:
                        shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception as e:
                log_warning(f"Failed to clean up temporary files: {str(e)}")

    def _save_encrypted_model(self, temp_path, save_path, check_save_name=None):
        """
        加密保存模型

        该方法用于将模型加密并保存为 `.pim` 文件。它使用 `CryptoOps` 类对模型文件进行加密，并将加密后的模型
        保存到指定的路径。如果提供了 `check_save_name`，则还会将加密后的模型文件复制到该位置。

        :param temp_path: 临时保存的模型文件路径。
        :param save_path: 最终保存加密模型的路径，文件扩展名将被更改为 `.pim`。
        :param check_save_name: （可选）如果提供，则将加密后的模型文件复制到该路径。
        :return: None
        """
        from ...crypto import CryptoOps

        # 更新文件扩展名
        save_path = save_path.replace('.pth', '.pim')

        # 加密并保存
        crypto = CryptoOps()
        crypto.encrypt_pth_file(
            temp_path,
            save_path,
            model_id=self.model_id
        )
        if check_save_name:
            # 复制到指定位置
            shutil.copy(save_path, check_save_name)

    def _save_latest_model(self, epoch, model, model_clone, optimizer, **kwargs):
        """
        保存训练最后轮次的权重文件。

        该方法用于保存训练的最新模型权重和优化器状态。它会将当前模型的状态字典（`state_dict`）和优化器的状态字典一起
        保存到临时文件中，并根据配置决定是加密保存（`.pim` 文件）还是普通保存（`.pth` 文件）。它还会保存当前训练
        轮次、学习率、批次大小等信息。最终保存的文件用于恢复训练过程或做模型备份。

        :param epoch: 当前训练轮次（epoch）。
        :param model: 当前模型。
        :param model_clone: 模型副本，用于保存。
        :param optimizer: 优化器，用于保存优化器状态。
        :param kwargs: 额外的参数（如保存路径等）。
        :return: None
        """
        model_sd = self.get_state_dict(model)
        model_clone = self.load_state_dict(model_clone, model_sd)
        temp_checkpoint = os.path.join(tempfile.mkdtemp(), f'latest{self.file_ext}')
        save_path = os.path.join(self.sdt_trainer_config.trainer.callbacks.checkpoint_dir, f'latest{self.file_ext}')
        torch.save({'current_epoch': epoch,
                    'num_epochs': self.sdt_trainer_config.trainer.num_epochs,
                    'lr': self.sdt_trainer_config.trainer.learning_rate,
                    'lr_decay': self.sdt_trainer_config.trainer.lr_method.decay,
                    'model': model_clone,
                    'batch_size': self.sdt_trainer_config.trainer.batch_size,
                    'optimizer': self.get_state_dict(optimizer)}, temp_checkpoint)

        if self.pim:
            self._save_encrypted_model(temp_checkpoint, save_path)
        else:
            shutil.copy(temp_checkpoint, save_path)

        os.remove(temp_checkpoint)

    def _saving_latest_model_sdl(self, current_epoch, max_score):
        """
        保存训练的参数信息和最后轮次的验证精度信息。

        该方法将训练过程中重要的配置信息（如数据路径、模型类型、架构、训练配置、当前轮次、学习率等）保存为
        `.sdl` 文件。它还会将最后一轮验证精度（如 IoU）记录到该文件中，以便后续查看训练的相关参数和模型性能。

        :param current_epoch: 当前训练的 epoch，用于记录当前训练轮次。
        :param max_score: 当前训练轮次的验证精度（通常是 IoU 或其他评估指标）。
        :return: 保存的配置文件路径（`sdl_path`）。
        """
        config = OrderedDict({
            'train_data_path': self.train_data_path,
            'model_path': self.output_model_path_base,
            'train_config': self.trainer_config_path,
            'model_type': self.sda_data_config.dataset.data_type,
            'framework': 'pytorch',
            'model_architecture': self.model_architecture,
            'model_name': self.model_base_name,
            'batch_size': self.sdt_trainer_config.trainer.batch_size,
            'encoder': self.sdt_trainer_config.model.backbone_name,
            'num_epochs': self.sdt_trainer_config.trainer.num_epochs,
            'current_epoch': current_epoch,
            'learning_rate': self.sdt_trainer_config.trainer.learning_rate,
            'lr_method': [{'warmup': self.sdt_trainer_config.trainer.lr_method.warmup},
                          {'decay': self.sdt_trainer_config.trainer.lr_method.decay},
                          {'warmup_epoch': self.sdt_trainer_config.trainer.lr_method.warmup_epoch}],
            'metrics': [{'iou_score': round(float(max_score), 4)}, ]
        })
        save_config_to_yaml(config, os.path.join(
            self.sdt_trainer_config.trainer.callbacks.checkpoint_dir, 'latest.sdl'))
        return save_config_to_yaml(config, self.sdl_path)

    def _saving_model_sdm(self):
        """
        保存模型为 SDM 文件。

        该方法用于将训练模型保存为 SDM 格式的文件，它会保存模型的架构、输入输出形状、类别信息等，并
        以 `.sdm` 格式存储。这通常用于将模型导出为可在其他应用中使用的标准化文件格式。

        :return: 返回保存的 SDM 文件路径。
        """
        model_input = [(self.sda_data_config.dataset.x_bandnum,
                        self.sda_data_config.dataset.tile_size, self.sda_data_config.dataset.tile_size)]

        model_output = [(len(self.sda_data_config.dataset.class_type) if
                         len(self.sda_data_config.dataset.class_type) > 2 else 1,
                         self.sda_data_config.dataset.tile_size, self.sda_data_config.dataset.tile_size)]
        # save_sdm
        config = OrderedDict({
            'model_type': self.sda_data_config.dataset.data_type,
            'framework': self.framework,
            'framework_version': str(torch.__version__),
            'model_architecture': self.model_architecture,
            'model_categorys': self.model_base_name,
            'tile_size': self.sda_data_config.dataset.tile_size,
            'model_tag': 'standard',
            'signature_name': 'predict',
            'model_input': [{'shape': input,
                             'type': 'float',
                             'inputs': 'images' + str(i)} for i, input in enumerate(model_input)],
            'model_output': [{'shape': output,
                              'type': 'float',
                              'outputs': 'scores' + str(i), } for i, output in enumerate(model_output)],
            'class_type': [OrderedDict(l) for l in list(self.sda_data_config.dataset.class_type)],
            'is_stretch': 0,
            'batch_size': 1,
            'encoder': self.sdt_trainer_config.model.backbone_name,
            'encoder_weights': self.encoder_weights,
            'scales': [1.0]
        })
        save_config_to_yaml(config, self.sdm_path)
        log_info('model saved in dir : {}'.format(self.sdm_path))
        print('model saved in dir : {}'.format(self.sdm_path))
        return self.sdm_path

    def get_model(self):
        raise NotImplementedError

    def get_state_dict(self):
        raise NotImplementedError

    def load_state_dict(self):
        raise NotImplementedError


class BaseTorchTrainer(BaseTrainer):
    def __init__(self, **kwargs):
        """
        初始化函数，初始化框架对应框架名、权重文件后缀名。
        
        Args:
            config (dict, optional): 配置文件，默认为None。
        
        Returns:
            None
        
        """
        self.framework = 'pytorch'
        self.file_ext = '.pth'

    def get_state_dict(self, model: Union[torch.nn.Module, torch.optim.Optimizer]):
        """
        获取模型或优化器的状态字典。
        
        Args:
            model (Union[torch.nn.Module, torch.optim.Optimizer]): 模型或优化器实例。
        
        Returns:
            dict: 包含模型或优化器状态信息的字典。
        
        """
        try:
            model_sd = model.module.state_dict()
        except:
            model_sd = model.state_dict()
        return model_sd

    def load_state_dict(self, model, state_dict):
        """
        加载模型的权重。

        Args:
            model (torch.nn.Module): 需要加载权重的模型。
            state_dict (dict): 权重字典，包含模型的所有权重和偏置。
        
        Returns:
            torch.nn.Module: 加载权重后的模型。
        
        """
        model.load_state_dict(state_dict)
        return model


class BaseMindSporeTrainer(BaseTrainer):
    def __init__(self, **kwargs):
        """
        初始化函数，初始化框架对应权重文件后缀名。
        
        Args:
            config (dict, optional): 配置文件，默认为None。
        
        Returns:
            None
        
        """
        self.file_ext = '.ckpt'

    def get_state_dict(self, model):
        pass

    def load_state_dict(self, model, state_dict):
        pass
