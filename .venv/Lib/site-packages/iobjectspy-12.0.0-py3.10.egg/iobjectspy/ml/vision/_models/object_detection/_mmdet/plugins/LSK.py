import torch
import torch.nn as nn
from mmengine.registry import MODELS


@MODELS.register_module()
class LSKblock(nn.Module):
    def __init__(self, in_channels):
        super(LSKblock, self).__init__()
        self.conv0 = nn.Conv2d(in_channels, in_channels, kernel_size=5, padding=2, groups=in_channels)
        self.conv_spatial = nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=2, padding=9, groups=in_channels, dilation=3)
        self.conv1 = nn.Conv2d(in_channels, in_channels//2, kernel_size=1)
        self.conv2 = nn.Conv2d(in_channels, in_channels//2, kernel_size=1)
        self.conv_squeeze = nn.Conv2d(2, 2, kernel_size=7, padding=3)
        self.conv = nn.Conv2d(in_channels // 2, in_channels, kernel_size=1)

    def forward(self, x):
        attn1 = self.conv0(x)
        attn2 = self.conv_spatial(attn1)
        attn1 = self.conv1(attn1)
        attn2 = self.conv2(attn2)
        attn = torch.cat([attn1, attn2], dim=1)
        attn_mean = torch.mean(attn, dim=1, keepdim=True)
        attn_max, _ = torch.max(attn, dim=1, keepdim=True)
        agg = torch.cat([attn_mean, attn_max], dim=1)
        sig = self.conv_squeeze(agg).sigmoid()
        attn = attn1 * sig[:,0,:,:].unsqueeze(1) + attn1 * sig[:,1,:,:].unsqueeze(1)
        attn = self.conv(attn)
        return (x * attn)


@MODELS.register_module()
class LSKAttention(nn.Module):
    def __init__(self, in_channels):
        super(LSKAttention, self).__init__()

        self.proj_1 = nn.Conv2d(in_channels, in_channels, 1)
        self.activation = nn.GELU()
        # self.spatial_gating_unit = LSKblock(in_channels)
        self.proj_2 = nn.Conv2d(in_channels, in_channels, 1)

    def forward(self, x):
        shorcut = x.clone()
        x = self.proj_1(x)
        x = self.activation(x)
        # x = self.spatial_gating_unit(x)
        x = self.proj_2(x)
        x = x + shorcut
        return x
