from typing import Optional
from ..base import SegmentationModel, SegmentationHead
from ..encoders import get_encoder
from ....prompt_segmentation.segment_anything import sam_model_registry
import os
from pathlib import Path
import torch.nn as nn
import torch
from torch.nn import functional as F
from typing import Any, Dict, List, Tuple
from .decoder import SamDecoder
from ..encoders import load_weighs_url
from ..encoders.sam_vit import sam_vit_encoders
import re

encoder_to_folder = {
    "vit-b": "sam_b",
    "vit-l": "sam_l",
    "vit-h": "sam_h",
    "vit-t": "sam_t"
}


def load_from(model, ori_mask_decoder_state_dict, encoder_global_attn_indexes, image_size=1024, vit_patch_size=16):
    model_state_dict = model.state_dict()
    except_keys = ['mask_tokens', 'output_hypernetworks_mlps', 'iou_prediction_head']

    new_state_dict = {}
    for k, v in ori_mask_decoder_state_dict.items():
        if k.startswith('image_encoder') or k.startswith('prompt_encoder'):
            new_key = 'encoder.' + k
        elif k.startswith('mask_decoder'):
            new_key = 'decoder' + k.replace('mask_decoder', '', 1)
        else:
            new_key = k
        if new_key in model_state_dict and not any(ex_key in new_key for ex_key in except_keys):
            new_state_dict[new_key] = v
    pos_embed = new_state_dict['encoder.image_encoder.pos_embed']
    token_size = int(image_size // vit_patch_size)
    if pos_embed.shape[1] != token_size:
        # resize pos embedding, which may sacrifice the performance, but I have no better idea
        pos_embed = pos_embed.permute(0, 3, 1, 2)  # [b, c, h, w]
        pos_embed = F.interpolate(pos_embed, (token_size, token_size), mode='bilinear', align_corners=False)
        pos_embed = pos_embed.permute(0, 2, 3, 1)  # [b, h, w, c]
        new_state_dict['encoder.image_encoder.pos_embed'] = pos_embed
        rel_pos_keys = [k for k in model_state_dict if 'rel_pos' in k]
        indices = {str(x) for x in encoder_global_attn_indexes}
        # indices = {'2', '5', '8', '11'}
        # global_rel_pos_keys = [k for k in rel_pos_keys if any(indice in k for indice in indices)]
        global_rel_pos_keys = [
            k for k in rel_pos_keys if
            any(re.search(r'\.{}\.' .format(indice), k) for indice in indices)
        ]
        for k in global_rel_pos_keys:
            rel_pos_params = new_state_dict[k]
            h, w = rel_pos_params.shape
            rel_pos_params = rel_pos_params.unsqueeze(0).unsqueeze(0)
            rel_pos_params = F.interpolate(rel_pos_params, (token_size * 2 - 1, w), mode='bilinear',
                                           align_corners=False)
            new_state_dict[k] = rel_pos_params[0, 0, ...]
    model_state_dict.update(new_state_dict)
    return model_state_dict


class Sam(SegmentationModel):
    def __init__(
            self,
            encoder_name: str = "sam-vit-b",
            encoder_weights: str = "sa-1b",
            encoder_depth: int = 12,
            in_channels: int = 3,
            classes: int = 1,
            activation: Optional[str] = None,
            image_size: int = 1024,
    ):
        super().__init__()
        self.image_size = image_size

        self.encoder = get_encoder(
            encoder_name,
            in_channels=in_channels,
            depth=encoder_depth,
            weights=encoder_weights,
        )
        self.decoder = SamDecoder["vit-b", classes]
        self.encoder_global_attn_indexes = sam_vit_encoders[encoder_name]["params"]["encoder_global_attn_indexes"]

        if encoder_weights:
            self.initialize_decoder(encoder_name, encoder_weights)

        if classes > 2:
            self.activate = nn.Identity()
            self.multimask_output = True
        else:
            self.activate = nn.Sigmoid()
            self.multimask_output = False

    def base_forward(self, x, y):
        """Sequentially pass `x` trough model`s encoder, decoder and heads"""
        image_size = x.shape[2]
        image_embeddings, sparse_embeddings, dense_embeddings = self.encoder(x)

        low_res_masks, iou_predictions = self.decoder(
            image_embeddings=image_embeddings,
            image_pe=self.encoder.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=self.multimask_output
        )

        out = self.postprocess_masks(
            low_res_masks,
            input_size=(image_size, image_size),
            original_size=(image_size, image_size)
        )

        out = self.activate(out)
        return out

    def postprocess_masks(
        self,
        masks: torch.Tensor,
        input_size: Tuple[int, ...],
        original_size: Tuple[int, ...],
    ) -> torch.Tensor:
        """
        Remove padding and upscale masks to the original image size.

        Arguments:
          masks (torch.Tensor): Batched masks from the mask_decoder,
            in BxCxHxW format.
          input_size (tuple(int, int)): The size of the image input to the
            model, in (H, W) format. Used to remove padding.
          original_size (tuple(int, int)): The original size of the image
            before resizing for input to the model, in (H, W) format.

        Returns:
          (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)
            is given by original_size.
        """
        masks = F.interpolate(
            masks,
            (self.encoder.image_encoder.img_size, self.encoder.image_encoder.img_size),
            mode="bilinear",
            align_corners=False,
        )
        masks = masks[..., : input_size[0], : input_size[1]]
        masks = F.interpolate(masks, original_size, mode="bilinear", align_corners=False)
        return masks

    def initialize_decoder(self, encoder_name, encoder_weights):
        checkpoint_url = sam_vit_encoders[encoder_name]["pretrained_settings"][encoder_weights]["url"]
        state_dict = load_weighs_url(checkpoint_url)
        mask_decoder_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('mask_decoder'):
                new_key = key.replace('mask_decoder.', '')
                mask_decoder_state_dict[new_key] = value
        self.decoder.load_state_dict(mask_decoder_state_dict)

    def get_new_state_dict(self, state_dict):
        new_state_dict = load_from(self, state_dict, self.encoder_global_attn_indexes, image_size=512)
        return new_state_dict
