from typing import Optional, Tuple, List

import numpy as np
import torch
import torch.nn as nn

from ..utils.transforms import ResizeLongestSide, BatchResizeLongestSide
from ...segment_anything.modeling import Sam


class SamInference(nn.Module):
    def __init__(
            self,
            sam_model: Sam,
            batch_size: int,
    ) -> None:
        super(SamInference, self).__init__()
        """
        Uses SAM to calculate the image embedding for an image, and then
        allow repeated, efficient mask prediction given prompts.

        Arguments:
          sam_model (Sam): The model to use for mask prediction.
        """
        self.model = sam_model
        self.input_size = (sam_model.image_encoder.img_size,) * 2
        self.original_size = None
        self.features = None
        self.tile_corner = torch.zeros(batch_size, 4)

    @torch.no_grad()
    def forward(self, input_image, image_size, tile_corner,
                transformed_boxes=None,
                transf_points=None,
                labels=None,
                return_logits=True) -> Tuple[torch.Tensor, torch.Tensor]:
        input_image = input_image.to(self.device)
        image_size = image_size.to(self.device)
        tile_corner = tile_corner.to(self.device)

        self.original_size = tuple(image_size[0, :].tolist())
        device = torch.device(f'{tile_corner.device}')
        self.tile_corner = self.tile_corner.to(device)
        if not torch.all(torch.eq(self.tile_corner, tile_corner)) or self.features is None:
            self.features = self.model.image_encoder(input_image)
            self.tile_corner = tile_corner

        if transf_points is not None:
            transf_points = transf_points.to(self.device)
            labels = labels.to(self.device)
            in_points = transf_points.view(-1, *transf_points.shape[2:])
            in_labels = labels.view(-1, *labels.shape[2:])

            in_points = in_points[:, None, :]
            in_labels = in_labels[:, None]

            points = (in_points, in_labels)
            multimask_output = True
        else:
            points = None

        if transformed_boxes is not None:
            transformed_boxes = transformed_boxes.to(self.device)
            # bbox 功能不支持图像多batch_size
            transformed_boxes = torch.squeeze(transformed_boxes, dim=0)
            multimask_output = False

        # Embed prompts
        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
            points=points,
            boxes=transformed_boxes,
            masks=None,
        )

        # Predict masks
        masks, iou_predictions = self.model.mask_decoder(
            image_embeddings=self.features,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
        )

        # Upscale the masks to the original image resolution
        masks = self.model.postprocess_masks(masks, self.input_size, self.original_size)
        if not return_logits:
            masks = masks > self.model.mask_threshold

        masks = masks.reshape(input_image.shape[0], masks.shape[0] // input_image.shape[0], *masks.shape[1:])
        iou_predictions = iou_predictions.reshape(input_image.shape[0],
                                                  iou_predictions.shape[0] // input_image.shape[0],
                                                  *iou_predictions.shape[1:])
        return masks, iou_predictions

    @property
    def device(self) -> torch.device:
        return self.model.device

    def reset_image(self):
        self.features = None
