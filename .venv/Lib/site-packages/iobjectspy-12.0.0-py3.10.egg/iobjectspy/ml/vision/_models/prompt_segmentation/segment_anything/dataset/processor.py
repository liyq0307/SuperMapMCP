from ...segment_anything.utils import ResizeLongestSide, BatchResizeLongestSide
import numpy as np
import torch
from torch.nn import functional as F
from typing import List, Tuple


class Samprocessor:
    """
    Processor that transform the image and bounding box prompt with ResizeLongestSide and then pre process both data
        Arguments:
            sam_model: Model of SAM with LoRA weights initialised

        Return:
            inputs (list(dict)): list of dict in the input format of SAM containing (prompt key is a personal addition)
                image: Image preprocessed
                boxes: bounding box preprocessed
                prompt: bounding box of the original image

    """

    def __init__(self, input_image_size, sample_func=None):
        super().__init__()

        self.transform = ResizeLongestSide(input_image_size)
        self.batchtransform = BatchResizeLongestSide(input_image_size)
        pixel_mean = [123.675, 116.28, 103.53]
        pixel_std = [58.395, 57.12, 57.375]
        self.pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)
        self.pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)
        self.sample_func = sample_func

    def __call__(self,
                 image: np.ndarray,
                 original_size: Tuple[int, int],
                 tile_corner: List = None,
                 prompt: np.ndarray = None,
                 point_num=None,
                 bbox=None) -> dict:
        # Processing of the image
        image_torch = self.process_image(image)

        # Transform input prompts
        if point_num is not None:
            points_torch, labels_torch, sample_points = self.process_points_prompt(image, prompt,
                                                                                   point_num, tile_corner)
            original_size_np = np.array(original_size)

            inputs = {"tile_rgb": image_torch,
                      "tile_size": original_size_np,
                      "sample_pos_points": points_torch,
                      'ori_sample_points': sample_points,
                      "sample_pos_labels": labels_torch}

        elif bbox is not None:
            bboxes_torch = self.process_bboxes_prompt(bbox, original_size)
            original_size_np = np.array(original_size)

            inputs = {"tile_rgb": image_torch,
                      "tile_size": original_size_np,
                      "bbox": bboxes_torch}
        else:
            print("异常抛出")

        return inputs

    def process_image(self, image: np.ndarray) -> torch.tensor:
        """
        Preprocess the image to make it to the input format of SAM

        Arguments:
            image: Image loaded in PIL
            original_size: tuple of the original image size (H,W)

        Return:
            (tensor): Tensor of the image preprocessed
        """

        # Transform the image to the form expected by the model
        if len(image.shape) > 3:
            input_image = self.batchtransform.apply_batchimage(image)
            input_image_torch = torch.as_tensor(input_image)
            input_image_torch = input_image_torch.permute(0, 3, 1, 2).contiguous()
            input_image = self.normalize_pixel(input_image_torch)
        else:
            input_image = self.transform.apply_image(image)
            input_image_torch = torch.as_tensor(input_image)
            input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()
            input_image = self.normalize_pixel(input_image_torch)

        return input_image

    def process_points_prompt(self, image, prompts, point_num, tile_corner) -> torch.tensor:
        """
        Preprocess the prompt (bounding box) to make it to the input format of SAM

        Arguments:
            box: Bounding bounding box coordinates in [XYXY]
            original_size: tuple of the original image size (H,W)

        Return:
            (tensor): Tensor of the prompt preprocessed
        """
        ori_pos_points, ori_pos_labels = self.sample_func.sample(image, prompts, point_num)

        if len(ori_pos_points) != 0:
            # 判断 pos_points 和 pos_labels 的形状是否为 (num, 2),如果不是,则复制到指定数量
            if ori_pos_points.shape != (point_num, 2):
                nums = ori_pos_points.shape[0]
                num_copies = (point_num + nums - 1) // nums  # 计算需要复制的次数
                pos_points = np.tile(ori_pos_points, (num_copies, 1))[:point_num]
                ori_pos_points = np.tile(ori_pos_points, (num_copies, 1))[:point_num]
                pos_labels = np.tile(ori_pos_labels, num_copies)[:point_num]
            else:
                pos_points = ori_pos_points
                pos_labels = ori_pos_labels

            offset = np.array([[tile_corner[0], tile_corner[1]]]).reshape(-1, 2)
            offsets = offset.repeat(ori_pos_points.shape[0], axis=0)
            sample_points = ori_pos_points + offsets
            pos_points = self.batchtransform.apply_batchcoords(pos_points, image.shape[:2])
        else:
            pos_points, pos_labels, sample_points = ori_pos_points, ori_pos_points, ori_pos_labels
        return pos_points, pos_labels, sample_points

    def process_bboxes_prompt(self, boxes: torch.Tensor, original_size: Tuple[int, ...]
                              ) -> torch.Tensor:
        """
        Expects a torch tensor with shape Bx4. Requires the original image
        size in (H, W) format.
        """
        boxes = self.transform.apply_boxes_torch(boxes, original_size)
        return boxes

    def normalize_pixel(self, x: torch.Tensor) -> torch.Tensor:
        """Normalize pixel values and pad to a square input."""
        # Normalize colors
        x = (x - self.pixel_mean) / self.pixel_std

        # Pad
        h, w = x.shape[-2:]
        padh = 1024 - h
        padw = 1024 - w
        x = F.pad(x, (0, padw, 0, padh))
        return x
