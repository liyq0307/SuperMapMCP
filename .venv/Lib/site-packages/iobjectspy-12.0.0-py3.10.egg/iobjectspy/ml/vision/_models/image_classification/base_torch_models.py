# !/usr/bin/env python3
# coding=utf-8
import os
import time
import math
from collections import OrderedDict
import shutil
import torch
import torch.nn.functional as F
import numpy as np
import torch.distributed as dist
import os.path as osp
import warnings
import tempfile
from ....toolkit._toolkit import save_config_to_yaml
from ....._logger import log_info, log_warning
from ..device_utils import Device_type


class Estimation():
    def __init__(self):
        pass
    def estimate_img(self):
        pass

    def estimate_tile(self):
        pass

    def load_model(self, model_path):
        self.model = torch.load(model_path, map_location=self.device)
        if Device_type.device_count() >= 1:  # Returns the number of GPUs available
            self.model = Device_type.dataparallel(self.model, self.gpus)

        if self.model.training:
            self.model.eval()
        pass

    def close_model(self):
        # todo close model
        try:
            del self.model
            Device_type.empty_cache()
        except Exception as e:
            log_warning('Close model error : {}'.format(e))
        pass

    def _predict_tile_local(self, predict_tile, out_shape):
        """
        利用给定的模型使用torch推断得到模型预测结果
        :param predict_tile:  ndarray 需要预测的数组片 形状为 （tile_nums,:） 即第一列为图片的数量
        :param out_shape: tuple 输出结果的形状  如（100,1,320,320）
        :return:  ndarray 返回预测的结果
        """

        predict_tile = Device_type.to_device(torch.from_numpy(predict_tile.transpose((0, 3, 1, 2)).astype(np.float32)))
        batch_size = 1
        total_batch = int(predict_tile.shape[0] / batch_size)

        for i in range(total_batch):
            out = self.model.forward(predict_tile[i * batch_size:(i + 1) * batch_size, :])
            out = F.softmax(out, dim=-1).cpu().detach().numpy()
            if i == 0:
                y_all = out
            else:
                y_all = np.concatenate((y_all, out), 0)
        y_out = np.expand_dims(y_all, axis=0)
        y_out.resize(out_shape)

        return y_out


class Trainer():
    def __init__(self):
        self.callbacks = []
        self.loss = []
        self.acc = []
        self.val_loss = []
        self.val_acc = []
        self.model_type = ''
        self.model_architecture = ''

    def init_callbacks(self, log_path=None, epoch_total=None):
        if log_path:
            self.config.trainer.callbacks.tensorboard_log_dir = os.path.join(log_path, "logs")
            self.config.trainer.callbacks.checkpoint_dir = os.path.join(log_path, "checkpoints")
        else:
            self.log_path = 'experiments'
            self.config.trainer.callbacks.tensorboard_log_dir = os.path.join("experiments",
                                                                             time.strftime("%Y-%m-%d",
                                                                                           time.localtime()),
                                                                             self.config.application.name, "logs")
            self.config.trainer.callbacks.checkpoint_dir = os.path.join("experiments",
                                                                        time.strftime("%Y-%m-%d", time.localtime()),
                                                                        self.config.application.name, "checkpoints")

        if self.main_process(self.multiprocessing_distributed, self.Rank):
            if os.path.exists(self.config.trainer.callbacks.tensorboard_log_dir) is not True:
                os.makedirs(self.config.trainer.callbacks.tensorboard_log_dir)
            else:
                log_info('tensorboard log directory already existed')
            if os.path.exists(self.config.trainer.callbacks.checkpoint_dir) is not True:
                os.makedirs(self.config.trainer.callbacks.checkpoint_dir)
            else:
                log_info('checkpoint directory already existed')

    def train(self):
        pass

    def get_world_size(self):
        if not self.multiprocessing_distributed:
            return 1
        return dist.get_world_size()

    def find_free_port(self):
        import socket
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        # Binding to port 0 will cause the OS to find an available port for us
        sock.bind(("", 0))
        port = sock.getsockname()[1]
        sock.close()
        # NOTE: there is still a chance the port could be taken by other processes.
        return port

    def _save_model_pth(self, i, model, model_clone, score, **kwargs):
        try:
            model_sd = model.module.state_dict()
        except:
            model_sd = model.state_dict()
        model_clone.load_state_dict(model_sd)
        temp_pth = os.path.join(tempfile.mkdtemp(), '{}-{:0=4}-{:.4f}.pth'.format(self.config.application.name, i, score))
        check_save_name = os.path.join(self.config.trainer.callbacks.checkpoint_dir,
                                       '{}-{:0=4}-{:.4f}.pth'.format(self.config.application.name, i, score))

        torch.save(model_clone, temp_pth)
        shutil.copy(temp_pth, self.torch_model_path)
        shutil.copy(temp_pth, check_save_name)
        os.remove(temp_pth)

    def _save_latest_model_pth(self, epoch, model, model_clone, optimizer, **kwargs):
        try:
            model_sd = model.module.state_dict()
        except:
            model_sd = model.state_dict()
        model_clone.load_state_dict(model_sd)
        temp_pth = os.path.join(tempfile.mkdtemp(), 'latest.pth')
        save_path = os.path.join(self.config.trainer.callbacks.checkpoint_dir, 'latest.pth')
        torch.save({'current_epoch': epoch, 'num_epochs': self.config.trainer.num_epochs,
                    'lr': self.lr, 'lr_decay': self.lr_decay, 'model': model_clone,
                    'batch_size': self.batch_size,
                    'optimizer': optimizer.state_dict()}, temp_pth)
        shutil.copy(temp_pth, save_path)
        os.remove(temp_pth)

    def _saving_latest_model_sdl(self, current_epoch, max_score):

        config = OrderedDict({
            'train_data_path': self.train_data_path,
            'model_path': self.output_model_path_base,
            'train_config': self.config_path,
            'model_type': self.model_type,
            'framework': 'pytorch',
            'model_architecture': self.model_architecture,
            'model_name': self.model_base_name,
            'batch_size': self.batch_size,
            'encoder': self.backbone_name,
            'num_epochs': self.config.trainer.num_epochs,
            'current_epoch': current_epoch,
            'learning_rate': self.lr,
            'lr_method': [{'warmup': self.lr_warmup},
                          {'decay': self.lr_decay},
                          {'warmup_epoch': self.warmup_epochs}],
            'metrics': [{'acc_score': round(float(max_score), 4)}, ]
        })
        save_config_to_yaml(config, osp.join(self.config.trainer.callbacks.checkpoint_dir, 'latest.sdl'))
        return save_config_to_yaml(config, self.sdl_path)

    def main_process(self, multiprocessing_distributed, Rank):
        return not multiprocessing_distributed or (multiprocessing_distributed and Rank == 0)

    def _saving_model(self):
        # save_sdm and pth
        # very important to do this as a first thing
        config = OrderedDict({
            'model_type': self.model_type,
            'framework': 'pytorch',
            'model_architecture': self.model_architecture,
            'model_categorys': self.model_base_name,
            'tile_size': self.tile_size,
            'model_tag': 'standard',
            'signature_name': 'predict',
            'model_input': [{'shape': input, 'type': 'float',
                             'inputs': 'images'} for i, input in enumerate(self.model_input)],
            'model_output': [{'shape': output, 'type': 'float',
                              'outputs': 'scores', } for i, output in enumerate(self.model_output)],
            'class_type': [OrderedDict(l.toDict()) for l in list(self.class_type)],
            'is_stretch': 0,
            'batch_size': 1,
        })
        save_config_to_yaml(config, self.sdm_path)
        log_info('model saved in dir : {}'.format(self.sdm_path))
        print('model saved in dir : {}'.format(self.sdm_path))
        return self.sdm_path

    def cosine_scheduler(self, base_value, final_value, epochs, niter_per_ep, warmup_epochs=0,
                         start_warmup_value=0, warmup_steps=-1):
        warmup_schedule = np.array([])
        warmup_iters = warmup_epochs * niter_per_ep
        if warmup_steps > 0:
            warmup_iters = warmup_steps
        print("Set warmup steps = %d" % warmup_iters)
        if warmup_epochs > 0:
            warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)

        iters = np.arange(epochs * niter_per_ep - warmup_iters)
        schedule = np.array(
            [final_value + 0.5 * (base_value - final_value) * (1 + math.cos(math.pi * i / (len(iters)))) for i in
             iters])

        schedule = np.concatenate((warmup_schedule, schedule))

        assert len(schedule) == epochs * niter_per_ep
        return schedule