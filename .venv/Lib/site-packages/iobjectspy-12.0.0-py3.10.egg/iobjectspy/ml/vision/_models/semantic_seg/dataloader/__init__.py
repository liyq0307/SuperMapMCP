import torch

from .fast_dataloader import FastDataLoader


class DataLoaderBuilder:
    def __init__(self, syst, sdt_config, batch_size,
                 train_dataset, valid_dataset,
                 data_works, ngpus_per_node, multiprocessing_distributed):

        if multiprocessing_distributed:
            batch_size_train = int(batch_size / ngpus_per_node)
            batch_size_val = int(batch_size / ngpus_per_node)
            workers = int(data_works / ngpus_per_node)
            self._train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
            self._val_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset)
        else:
            batch_size_train = batch_size
            batch_size_val = batch_size
            workers = data_works
            self._train_sampler = None
            self._val_sampler = None

        if syst == 'Windows':
            workers = sdt_config.trainer.workers

        self._train_loader = FastDataLoader(train_dataset, batch_size=batch_size_train,
                                            shuffle=(self._train_sampler is None),
                                            num_workers=workers, sampler=self._train_sampler, drop_last=True)

        self._valid_loader = FastDataLoader(valid_dataset, batch_size=batch_size_val, shuffle=False,
                                            num_workers=workers, sampler=self._val_sampler)

    def get_train_loader(self):
        return self._train_loader

    def get_valid_loader(self):
        return self._valid_loader

    def get_train_sampler(self):
        return self._train_sampler
